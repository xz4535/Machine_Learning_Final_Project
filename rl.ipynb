{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "Python Environment: Use an environment with Python and necessary libraries installed (e.g., numpy, pandas, matplotlib for data manipulation and visualization; TensorFlow or PyTorch for neural network modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'stock_data' is a DataFrame containing the stock data\n",
    "file_path = 'TSLA_stock_data_2023.csv'\n",
    "stock_data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using min-max scaling\n",
    "\n",
    "# Convert date column to datetime if it exists\n",
    "if 'Date' in stock_data.columns:\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data['Year'] = stock_data['Date'].dt.year\n",
    "    stock_data['Month'] = stock_data['Date'].dt.month\n",
    "    stock_data['Day'] = stock_data['Date'].dt.day\n",
    "    # Optionally, drop the original date column if no longer needed\n",
    "    stock_data.drop('Date', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns for normalization\n",
    "numeric_cols = stock_data.select_dtypes(include=['number']).columns\n",
    "stock_data[numeric_cols] = (stock_data[numeric_cols] - stock_data[numeric_cols].min()) / (stock_data[numeric_cols].max() - stock_data[numeric_cols].min())\n",
    "\n",
    "stock_data.fillna(method='ffill', inplace=True)  # forward fill to propagate last valid observation forward\n",
    "\n",
    "# Define the neural network for the agent\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(3, activation='linear')  # Assuming three actions: buy, hold, sell\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define State and Reward\n",
    "State Definition: Define the state as a vector of features like the day's opening price, high, low, close, and volume.\n",
    "Reward Calculation: Calculate rewards based on the change in stock price, as described in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for the agent\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(3, activation='linear')  # Assuming three actions: buy, hold, sell\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, model):\n",
    "    if isinstance(state, pd.Series):\n",
    "        state = state.values \n",
    "\n",
    "    # Reshape the state to fit the model's input requirements (1, number of features)\n",
    "    state = state.reshape(1, -1)\n",
    "    # Use the model to predict the action from the current state\n",
    "    q_values = model.predict(state)\n",
    "    return np.argmax(q_values[0])  # Choosing the action with the highest Q-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(current_state, next_state):\n",
    "    # Assuming 'close_prices' is a list or array of closing prices\n",
    "    # rewards = np.diff(close_prices) / close_prices[:-1]  # Percentage change between consecutive days\n",
    "    # return rewards\n",
    "    current_price = current_state['Close']\n",
    "    next_price = next_state['Close']\n",
    "    return (next_price - current_price) / current_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Model\n",
    "Model Initialization: Initialize the parameters for the TD(0) algorithm, including the discount factor (γ) and learning rate (α).\n",
    "Network Setup: Set up a neural network for function approximation. A simple multi-layer perceptron (MLP) can be used initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_model(model, state, action, reward, next_state):\n",
    "    state = state.values.astype('float32').reshape(1, -1)\n",
    "    next_state = next_state.values.astype('float32').reshape(1, -1)\n",
    "    # Perform a TD update on the model\n",
    "    target = reward + 0.95 * np.amax(model.predict(np.array([next_state]))[0])  # Discount factor gamma = 0.95\n",
    "    target_vec = model.predict(np.array([state]))[0]\n",
    "    target_vec[action] = target\n",
    "    # model.fit(np.array([state]), np.array([target_vec]), epochs=1, verbose=0)\n",
    "    model.fit(state, target_vec.reshape(-1, 1), epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate each trading period as an episode. For each episode, reset the environment to an initial state\n",
    "def run_episode(data, model):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(len(data) - 1):\n",
    "        current_state = data[numeric_cols].iloc[t]\n",
    "        next_state = data[numeric_cols].iloc[t + 1]\n",
    "        \n",
    "        action = choose_action(current_state, model)\n",
    "        reward = calculate_reward(current_state, next_state)\n",
    "        \n",
    "        update_model(model, current_state, action, reward, next_state)\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Algorithm: Implement the TD(0) learning algorithm to update the value function based on the state and reward observed from the data.\n",
    "Iteration: Iterate over episodes (each episode can be a sequence of stock price data), updating the model with each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple episodes to train the model effectively.\n",
    "def train_model(data, model, episodes):\n",
    "    for e in range(episodes):\n",
    "        total_reward = run_episode(data, model)\n",
    "        print(f'Episode {e+1}/{episodes}, Total Reward: {total_reward}')\n",
    "\n",
    "# Initialize the model\n",
    "# print(model.input_shape) \n",
    "# Initialize the model\n",
    "num_features = len(stock_data[numeric_cols])  # Update to match the number of input features after preprocessing\n",
    "model = create_model(num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 1742, but received input with shape (1, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 7), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(stock_data, model, \u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(data, model, episodes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(data, model, episodes):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m----> 4\u001b[0m         total_reward \u001b[38;5;241m=\u001b[39m run_episode(data, model)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(data, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m current_state \u001b[38;5;241m=\u001b[39m data[numeric_cols]\u001b[38;5;241m.\u001b[39miloc[t]\n\u001b[0;32m      7\u001b[0m next_state \u001b[38;5;241m=\u001b[39m data[numeric_cols]\u001b[38;5;241m.\u001b[39miloc[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m action \u001b[38;5;241m=\u001b[39m choose_action(current_state, model)\n\u001b[0;32m     10\u001b[0m reward \u001b[38;5;241m=\u001b[39m calculate_reward(current_state, next_state)\n\u001b[0;32m     12\u001b[0m update_model(model, current_state, action, reward, next_state)\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mchoose_action\u001b[1;34m(state, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Use the model to predict the action from the current state\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m q_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 1742, but received input with shape (1, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 7), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(stock_data, model, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stock_data.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Testing: After training, test the model on unseen data to assess its predictive accuracy.\n",
    "Performance Metrics: Use metrics like RMSE or predictive accuracy grades as used in the paper to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Adjustment\n",
    "Continuous Monitoring: Set up scripts to monitor the model’s performance over time.\n",
    "Adjustment: Tune parameters and refine the model as needed based on performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
