{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sliding windows\n",
    "def slide_windows(features):\n",
    "\n",
    "    # We need to ensure we have data for t-1, t, t+1 without index errors\n",
    "    windowed_data = []\n",
    "    predict_prices = []  # List to store the target 'Close' prices\n",
    "    cycle_data=[]\n",
    "\n",
    "    # Handling daily cycles\n",
    "    cycle_length = 7\n",
    "    for cycle in range(len(features) - cycle_length + 1):\n",
    "        start_index = cycle\n",
    "        end_index = start_index + cycle_length\n",
    "        cycle_datas = features.iloc[start_index:end_index,1:6].values\n",
    "        cycle_data.append(cycle_datas)\n",
    "        \n",
    "    # Creating sliding windows within the cycle\n",
    "    for i in range(14,len(cycle_data)-7):  # Avoiding index error by stopping before the last day\n",
    "        pre_previous = cycle_data[i - 14]\n",
    "        previous = cycle_data[i- 7]\n",
    "        current_state = cycle_data[i]\n",
    "\n",
    "        combined_features = np.concatenate([pre_previous, previous, current_state]).reshape(1,-1).squeeze()\n",
    "        windowed_data.append(combined_features)\n",
    "        predict_prices.append(cycle_data[i+7][-1][3])\n",
    "    \n",
    "    return windowed_data, predict_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "row_data = pd.read_csv('TSLA_stock_data_2023.csv')\n",
    "\n",
    "windowed_data, predict_prices = slide_windows(row_data)\n",
    "# Convert to PyTorch tensors\n",
    "state = torch.tensor(windowed_data, dtype=torch.float32).to(device)\n",
    "predict_price = torch.tensor(predict_prices, dtype=torch.float32).to(device)\n",
    "# state[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into training part and test part\n",
    "state_train, state_test, predict_price_train,predict_price_test= train_test_split(state, predict_price,test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(TensorDataset(state_train, predict_price_train), batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(state_test, predict_price_test), batch_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct NN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimazation function, it do one step of gredient decent, \n",
    "def optimize_model():\n",
    "    for state, target in train_loader:\n",
    "        current_value=value_net(state).squeeze()              \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss=criterion(current_value,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for state, target in test_loader:\n",
    "        with torch.no_grad():\n",
    "            test_valur=value_net(state).squeeze()                 \n",
    "            l_test=criterion(test_valur,target)\n",
    "    return loss.item(),l_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with LR = 1e-10 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 205730.203125, L_test: 258230.234375\n",
      "Epoch [101/2000], L_train: 236961.34375, L_test: 258228.34375\n",
      "Epoch [201/2000], L_train: 203048.046875, L_test: 258226.484375\n",
      "Epoch [301/2000], L_train: 214734.578125, L_test: 258224.671875\n",
      "Epoch [401/2000], L_train: 240570.296875, L_test: 258222.796875\n",
      "Epoch [501/2000], L_train: 210176.984375, L_test: 258220.984375\n",
      "Epoch [601/2000], L_train: 235544.90625, L_test: 258219.09375\n",
      "Epoch [701/2000], L_train: 225890.140625, L_test: 258217.203125\n",
      "Epoch [801/2000], L_train: 223120.859375, L_test: 258215.390625\n",
      "Epoch [901/2000], L_train: 230204.515625, L_test: 258213.53125\n",
      "Epoch [1001/2000], L_train: 227778.609375, L_test: 258211.75\n",
      "Epoch [1101/2000], L_train: 193577.9375, L_test: 258209.890625\n",
      "Epoch [1201/2000], L_train: 220896.53125, L_test: 258208.0625\n",
      "Epoch [1301/2000], L_train: 230291.53125, L_test: 258206.234375\n",
      "Epoch [1401/2000], L_train: 220352.09375, L_test: 258204.421875\n",
      "Epoch [1501/2000], L_train: 223532.71875, L_test: 258202.578125\n",
      "Epoch [1601/2000], L_train: 241679.765625, L_test: 258200.75\n",
      "Epoch [1701/2000], L_train: 231787.5625, L_test: 258198.953125\n",
      "Epoch [1801/2000], L_train: 214896.34375, L_test: 258197.09375\n",
      "Epoch [1901/2000], L_train: 216224.890625, L_test: 258195.296875\n",
      "Testing with LR = 1e-10 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 212237.859375, L_test: 258193.484375\n",
      "Epoch [101/2000], L_train: 225976.140625, L_test: 258191.65625\n",
      "Epoch [201/2000], L_train: 209110.984375, L_test: 258189.8125\n",
      "Epoch [301/2000], L_train: 217348.84375, L_test: 258188.0\n",
      "Epoch [401/2000], L_train: 227304.25, L_test: 258186.09375\n",
      "Epoch [501/2000], L_train: 216061.046875, L_test: 258184.28125\n",
      "Epoch [601/2000], L_train: 246165.4375, L_test: 258182.515625\n",
      "Epoch [701/2000], L_train: 246635.90625, L_test: 258180.71875\n",
      "Epoch [801/2000], L_train: 228588.765625, L_test: 258178.84375\n",
      "Epoch [901/2000], L_train: 235853.75, L_test: 258177.015625\n",
      "Epoch [1001/2000], L_train: 213452.625, L_test: 258175.234375\n",
      "Epoch [1101/2000], L_train: 240760.65625, L_test: 258173.34375\n",
      "Epoch [1201/2000], L_train: 219065.0625, L_test: 258171.53125\n",
      "Epoch [1301/2000], L_train: 231226.203125, L_test: 258169.703125\n",
      "Epoch [1401/2000], L_train: 225515.9375, L_test: 258167.890625\n",
      "Epoch [1501/2000], L_train: 228885.375, L_test: 258166.0625\n",
      "Epoch [1601/2000], L_train: 207040.765625, L_test: 258164.234375\n",
      "Epoch [1701/2000], L_train: 225845.34375, L_test: 258162.421875\n",
      "Epoch [1801/2000], L_train: 234436.1875, L_test: 258160.578125\n",
      "Epoch [1901/2000], L_train: 232372.09375, L_test: 258158.71875\n",
      "Testing with LR = 1e-10 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 216183.0, L_test: 258156.9375\n",
      "Epoch [101/2000], L_train: 237192.609375, L_test: 258155.09375\n",
      "Epoch [201/2000], L_train: 230866.890625, L_test: 258153.28125\n",
      "Epoch [301/2000], L_train: 240029.703125, L_test: 258151.4375\n",
      "Epoch [401/2000], L_train: 205906.71875, L_test: 258149.625\n",
      "Epoch [501/2000], L_train: 213746.46875, L_test: 258147.765625\n",
      "Epoch [601/2000], L_train: 243919.5625, L_test: 258145.9375\n",
      "Epoch [701/2000], L_train: 215209.109375, L_test: 258144.140625\n",
      "Epoch [801/2000], L_train: 206697.984375, L_test: 258142.328125\n",
      "Epoch [901/2000], L_train: 225320.71875, L_test: 258140.515625\n",
      "Epoch [1001/2000], L_train: 224485.796875, L_test: 258138.671875\n",
      "Epoch [1101/2000], L_train: 223478.53125, L_test: 258136.84375\n",
      "Epoch [1201/2000], L_train: 223413.0625, L_test: 258134.984375\n",
      "Epoch [1301/2000], L_train: 238048.234375, L_test: 258133.1875\n",
      "Epoch [1401/2000], L_train: 221224.71875, L_test: 258131.34375\n",
      "Epoch [1501/2000], L_train: 225219.609375, L_test: 258129.515625\n",
      "Epoch [1601/2000], L_train: 216786.390625, L_test: 258127.671875\n",
      "Epoch [1701/2000], L_train: 232950.90625, L_test: 258125.859375\n",
      "Epoch [1801/2000], L_train: 210568.796875, L_test: 258124.0625\n",
      "Epoch [1901/2000], L_train: 222312.390625, L_test: 258122.203125\n",
      "Testing with LR = 1e-10 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 237973.90625, L_test: 258120.390625\n",
      "Epoch [101/2000], L_train: 219845.234375, L_test: 258118.53125\n",
      "Epoch [201/2000], L_train: 227060.984375, L_test: 258116.703125\n",
      "Epoch [301/2000], L_train: 227359.203125, L_test: 258114.9375\n",
      "Epoch [401/2000], L_train: 224685.765625, L_test: 258113.109375\n",
      "Epoch [501/2000], L_train: 220878.859375, L_test: 258111.25\n",
      "Epoch [601/2000], L_train: 249465.90625, L_test: 258109.421875\n",
      "Epoch [701/2000], L_train: 228149.46875, L_test: 258107.609375\n",
      "Epoch [801/2000], L_train: 235515.984375, L_test: 258105.765625\n",
      "Epoch [901/2000], L_train: 256068.140625, L_test: 258103.9375\n",
      "Epoch [1001/2000], L_train: 234258.1875, L_test: 258102.09375\n",
      "Epoch [1101/2000], L_train: 204138.328125, L_test: 258100.28125\n",
      "Epoch [1201/2000], L_train: 238807.0625, L_test: 258098.421875\n",
      "Epoch [1301/2000], L_train: 219697.328125, L_test: 258096.625\n",
      "Epoch [1401/2000], L_train: 230040.703125, L_test: 258094.796875\n",
      "Epoch [1501/2000], L_train: 218056.46875, L_test: 258092.953125\n",
      "Epoch [1601/2000], L_train: 222374.9375, L_test: 258091.15625\n",
      "Epoch [1701/2000], L_train: 218133.328125, L_test: 258089.328125\n",
      "Epoch [1801/2000], L_train: 216298.53125, L_test: 258087.53125\n",
      "Epoch [1901/2000], L_train: 224107.796875, L_test: 258085.65625\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 189898.40625, L_test: 201426.5625\n",
      "Epoch [101/2000], L_train: 37283.6640625, L_test: 41255.6796875\n",
      "Epoch [201/2000], L_train: 20469.30078125, L_test: 35422.57421875\n",
      "Epoch [301/2000], L_train: 15801.8720703125, L_test: 32526.013671875\n",
      "Epoch [401/2000], L_train: 15928.671875, L_test: 29795.14453125\n",
      "Epoch [501/2000], L_train: 12641.0634765625, L_test: 27955.041015625\n",
      "Epoch [601/2000], L_train: 11906.1474609375, L_test: 26563.654296875\n",
      "Epoch [701/2000], L_train: 9397.7978515625, L_test: 26523.41015625\n",
      "Epoch [801/2000], L_train: 9131.66015625, L_test: 25444.33984375\n",
      "Epoch [901/2000], L_train: 9403.9404296875, L_test: 25337.546875\n",
      "Epoch [1001/2000], L_train: 7591.3720703125, L_test: 24821.60546875\n",
      "Epoch [1101/2000], L_train: 7129.67431640625, L_test: 24244.611328125\n",
      "Epoch [1201/2000], L_train: 5828.1484375, L_test: 23825.353515625\n",
      "Epoch [1301/2000], L_train: 7393.04638671875, L_test: 24071.24609375\n",
      "Epoch [1401/2000], L_train: 5321.009765625, L_test: 23909.810546875\n",
      "Epoch [1501/2000], L_train: 5749.193359375, L_test: 23349.08203125\n",
      "Epoch [1601/2000], L_train: 6036.88818359375, L_test: 22983.673828125\n",
      "Epoch [1701/2000], L_train: 5346.849609375, L_test: 22849.11328125\n",
      "Epoch [1801/2000], L_train: 5982.5341796875, L_test: 22457.796875\n",
      "Epoch [1901/2000], L_train: 4516.7734375, L_test: 22509.65234375\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 4971.3486328125, L_test: 21894.890625\n",
      "Epoch [101/2000], L_train: 4920.02099609375, L_test: 22347.79296875\n",
      "Epoch [201/2000], L_train: 5754.1484375, L_test: 21554.322265625\n",
      "Epoch [301/2000], L_train: 4261.69189453125, L_test: 22282.34375\n",
      "Epoch [401/2000], L_train: 4722.220703125, L_test: 21719.8203125\n",
      "Epoch [501/2000], L_train: 5578.8544921875, L_test: 21781.81640625\n",
      "Epoch [601/2000], L_train: 4600.005859375, L_test: 21641.376953125\n",
      "Epoch [701/2000], L_train: 4105.048828125, L_test: 21439.091796875\n",
      "Epoch [801/2000], L_train: 3752.4521484375, L_test: 22057.076171875\n",
      "Epoch [901/2000], L_train: 3860.974853515625, L_test: 21435.025390625\n",
      "Epoch [1001/2000], L_train: 3747.9833984375, L_test: 21732.5546875\n",
      "Epoch [1101/2000], L_train: 4678.22998046875, L_test: 21375.310546875\n",
      "Epoch [1201/2000], L_train: 3571.7509765625, L_test: 20983.609375\n",
      "Epoch [1301/2000], L_train: 4393.1171875, L_test: 21280.40625\n",
      "Epoch [1401/2000], L_train: 3993.079833984375, L_test: 20960.236328125\n",
      "Epoch [1501/2000], L_train: 3096.521484375, L_test: 20974.4453125\n",
      "Epoch [1601/2000], L_train: 3714.767822265625, L_test: 21040.3359375\n",
      "Epoch [1701/2000], L_train: 3079.216552734375, L_test: 21197.65234375\n",
      "Epoch [1801/2000], L_train: 2803.65380859375, L_test: 20695.677734375\n",
      "Epoch [1901/2000], L_train: 2386.617431640625, L_test: 20639.078125\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 3359.01318359375, L_test: 19557.595703125\n",
      "Epoch [101/2000], L_train: 3060.596923828125, L_test: 20883.193359375\n",
      "Epoch [201/2000], L_train: 3149.482177734375, L_test: 20853.404296875\n",
      "Epoch [301/2000], L_train: 3139.9365234375, L_test: 20631.06640625\n",
      "Epoch [401/2000], L_train: 2743.10791015625, L_test: 20544.875\n",
      "Epoch [501/2000], L_train: 3310.838623046875, L_test: 20615.28125\n",
      "Epoch [601/2000], L_train: 2869.94775390625, L_test: 20695.18359375\n",
      "Epoch [701/2000], L_train: 2297.203125, L_test: 20444.533203125\n",
      "Epoch [801/2000], L_train: 2795.664794921875, L_test: 20512.73046875\n",
      "Epoch [901/2000], L_train: 3902.239501953125, L_test: 20352.310546875\n",
      "Epoch [1001/2000], L_train: 2676.57373046875, L_test: 20087.888671875\n",
      "Epoch [1101/2000], L_train: 2486.880859375, L_test: 20189.896484375\n",
      "Epoch [1201/2000], L_train: 3216.89892578125, L_test: 20763.228515625\n",
      "Epoch [1301/2000], L_train: 2947.243896484375, L_test: 20383.828125\n",
      "Epoch [1401/2000], L_train: 2330.951904296875, L_test: 20174.3671875\n",
      "Epoch [1501/2000], L_train: 2223.871826171875, L_test: 20246.7578125\n",
      "Epoch [1601/2000], L_train: 3203.324462890625, L_test: 19714.109375\n",
      "Epoch [1701/2000], L_train: 2288.95068359375, L_test: 19509.04296875\n",
      "Epoch [1801/2000], L_train: 2618.744140625, L_test: 19814.671875\n",
      "Epoch [1901/2000], L_train: 2269.080322265625, L_test: 19449.541015625\n",
      "Testing with LR = 1e-05 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 3344.009521484375, L_test: 21007.013671875\n",
      "Epoch [101/2000], L_train: 2209.74462890625, L_test: 19547.01171875\n",
      "Epoch [201/2000], L_train: 3096.078125, L_test: 19241.763671875\n",
      "Epoch [301/2000], L_train: 2616.42529296875, L_test: 19123.439453125\n",
      "Epoch [401/2000], L_train: 2345.3154296875, L_test: 19303.97265625\n",
      "Epoch [501/2000], L_train: 2204.316162109375, L_test: 19347.95703125\n",
      "Epoch [601/2000], L_train: 2178.98828125, L_test: 19154.19140625\n",
      "Epoch [701/2000], L_train: 2344.790771484375, L_test: 19107.640625\n",
      "Epoch [801/2000], L_train: 2565.16162109375, L_test: 19053.91796875\n",
      "Epoch [901/2000], L_train: 2577.239013671875, L_test: 19005.94140625\n",
      "Epoch [1001/2000], L_train: 2256.247802734375, L_test: 18910.89453125\n",
      "Epoch [1101/2000], L_train: 2268.457763671875, L_test: 18746.8203125\n",
      "Epoch [1201/2000], L_train: 2453.564208984375, L_test: 19149.724609375\n",
      "Epoch [1301/2000], L_train: 2652.3212890625, L_test: 19177.267578125\n",
      "Epoch [1401/2000], L_train: 1845.4013671875, L_test: 19589.298828125\n",
      "Epoch [1501/2000], L_train: 2009.7630615234375, L_test: 19150.171875\n",
      "Epoch [1601/2000], L_train: 2286.459716796875, L_test: 18967.5\n",
      "Epoch [1701/2000], L_train: 1878.93896484375, L_test: 19071.03125\n",
      "Epoch [1801/2000], L_train: 1936.0343017578125, L_test: 18843.130859375\n",
      "Epoch [1901/2000], L_train: 1779.914794921875, L_test: 19317.79296875\n",
      "Testing with LR = 0.001 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 118900.2109375, L_test: 215936.34375\n",
      "Epoch [101/2000], L_train: 10702.5498046875, L_test: 15338.609375\n",
      "Epoch [201/2000], L_train: 3358.07958984375, L_test: 6842.2412109375\n",
      "Epoch [301/2000], L_train: 2261.615478515625, L_test: 4976.63720703125\n",
      "Epoch [401/2000], L_train: 1672.4185791015625, L_test: 4728.8828125\n",
      "Epoch [501/2000], L_train: 1050.5225830078125, L_test: 3185.51025390625\n",
      "Epoch [601/2000], L_train: 743.2036743164062, L_test: 1649.685302734375\n",
      "Epoch [701/2000], L_train: 529.6587524414062, L_test: 843.5558471679688\n",
      "Epoch [801/2000], L_train: 327.9209289550781, L_test: 487.3287048339844\n",
      "Epoch [901/2000], L_train: 600.2797241210938, L_test: 1081.67138671875\n",
      "Epoch [1001/2000], L_train: 258.1845703125, L_test: 287.68109130859375\n",
      "Epoch [1101/2000], L_train: 105.91233825683594, L_test: 216.6214599609375\n",
      "Epoch [1201/2000], L_train: 64.62470245361328, L_test: 250.51976013183594\n",
      "Epoch [1301/2000], L_train: 70.1071548461914, L_test: 156.37022399902344\n",
      "Epoch [1401/2000], L_train: 70.07955169677734, L_test: 125.15560150146484\n",
      "Epoch [1501/2000], L_train: 55.040714263916016, L_test: 159.80966186523438\n",
      "Epoch [1601/2000], L_train: 218.8932647705078, L_test: 421.2852478027344\n",
      "Epoch [1701/2000], L_train: 205.74954223632812, L_test: 1762.191162109375\n",
      "Epoch [1801/2000], L_train: 198.4167938232422, L_test: 1805.23876953125\n",
      "Epoch [1901/2000], L_train: 195.8554229736328, L_test: 1995.6346435546875\n",
      "Testing with LR = 0.001 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 88.61756134033203, L_test: 764.1300048828125\n",
      "Epoch [101/2000], L_train: 57.40953063964844, L_test: 70.48419189453125\n",
      "Epoch [201/2000], L_train: 57.19854736328125, L_test: 66.00115966796875\n",
      "Epoch [301/2000], L_train: 52.72565460205078, L_test: 70.32042694091797\n",
      "Epoch [401/2000], L_train: 51.173744201660156, L_test: 64.3983383178711\n",
      "Epoch [501/2000], L_train: 52.443153381347656, L_test: 62.558536529541016\n",
      "Epoch [601/2000], L_train: 50.10080337524414, L_test: 60.97439956665039\n",
      "Epoch [701/2000], L_train: 33.0178108215332, L_test: 32.03891372680664\n",
      "Epoch [801/2000], L_train: 44.75664138793945, L_test: 63.34457778930664\n",
      "Epoch [901/2000], L_train: 58.60072326660156, L_test: 61.50766372680664\n",
      "Epoch [1001/2000], L_train: 23.543054580688477, L_test: 22.54850196838379\n",
      "Epoch [1101/2000], L_train: 176.2141571044922, L_test: 166.99546813964844\n",
      "Epoch [1201/2000], L_train: 160.3145751953125, L_test: 147.06320190429688\n",
      "Epoch [1301/2000], L_train: 132.6610870361328, L_test: 124.01386260986328\n",
      "Epoch [1401/2000], L_train: 114.92005920410156, L_test: 103.0440902709961\n",
      "Epoch [1501/2000], L_train: 98.31470489501953, L_test: 86.79369354248047\n",
      "Epoch [1601/2000], L_train: 77.02436065673828, L_test: 66.2472152709961\n",
      "Epoch [1701/2000], L_train: 56.708961486816406, L_test: 55.08339309692383\n",
      "Epoch [1801/2000], L_train: 54.81657791137695, L_test: 46.51695251464844\n",
      "Epoch [1901/2000], L_train: 41.175865173339844, L_test: 39.171104431152344\n",
      "Testing with LR = 0.001 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 2295.961181640625, L_test: 185.56692504882812\n",
      "Epoch [101/2000], L_train: 202.51864624023438, L_test: 194.47959899902344\n",
      "Epoch [201/2000], L_train: 35.222896575927734, L_test: 39.32614517211914\n",
      "Epoch [301/2000], L_train: 33.46157455444336, L_test: 40.647090911865234\n",
      "Epoch [401/2000], L_train: 35.645992279052734, L_test: 41.096588134765625\n",
      "Epoch [501/2000], L_train: 34.12982940673828, L_test: 40.97622299194336\n",
      "Epoch [601/2000], L_train: 34.01245880126953, L_test: 41.30275344848633\n",
      "Epoch [701/2000], L_train: 39.27461624145508, L_test: 41.43193817138672\n",
      "Epoch [801/2000], L_train: 35.48074722290039, L_test: 41.48459243774414\n",
      "Epoch [901/2000], L_train: 30.478851318359375, L_test: 41.516868591308594\n",
      "Epoch [1001/2000], L_train: 33.29741287231445, L_test: 41.539100646972656\n",
      "Epoch [1101/2000], L_train: 37.82497787475586, L_test: 41.54248046875\n",
      "Epoch [1201/2000], L_train: 33.97401809692383, L_test: 41.55681228637695\n",
      "Epoch [1301/2000], L_train: 34.05369567871094, L_test: 41.54808807373047\n",
      "Epoch [1401/2000], L_train: 32.055694580078125, L_test: 41.548587799072266\n",
      "Epoch [1501/2000], L_train: 37.13821792602539, L_test: 41.555885314941406\n",
      "Epoch [1601/2000], L_train: 32.655982971191406, L_test: 41.55083465576172\n",
      "Epoch [1701/2000], L_train: 33.72028732299805, L_test: 41.558006286621094\n",
      "Epoch [1801/2000], L_train: 31.801042556762695, L_test: 41.55988693237305\n",
      "Epoch [1901/2000], L_train: 33.95592498779297, L_test: 41.56438064575195\n",
      "Testing with LR = 0.001 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 58.67534255981445, L_test: 616.6845092773438\n",
      "Epoch [101/2000], L_train: 59.69256591796875, L_test: 66.15127563476562\n",
      "Epoch [201/2000], L_train: 58.71985626220703, L_test: 61.21315002441406\n",
      "Epoch [301/2000], L_train: 55.06311798095703, L_test: 67.77571105957031\n",
      "Epoch [401/2000], L_train: 61.885093688964844, L_test: 61.14285659790039\n",
      "Epoch [501/2000], L_train: 55.91800308227539, L_test: 74.35913848876953\n",
      "Epoch [601/2000], L_train: 46.420352935791016, L_test: 46.21867752075195\n",
      "Epoch [701/2000], L_train: 39.94890594482422, L_test: 48.09953689575195\n",
      "Epoch [801/2000], L_train: 51.747581481933594, L_test: 24.410137176513672\n",
      "Epoch [901/2000], L_train: 32.438255310058594, L_test: 41.56253433227539\n",
      "Epoch [1001/2000], L_train: 34.60214614868164, L_test: 41.54894256591797\n",
      "Epoch [1101/2000], L_train: 36.12575912475586, L_test: 41.56263732910156\n",
      "Epoch [1201/2000], L_train: 32.388343811035156, L_test: 41.579959869384766\n",
      "Epoch [1301/2000], L_train: 32.98045349121094, L_test: 41.57414627075195\n",
      "Epoch [1401/2000], L_train: 35.64787673950195, L_test: 41.61087417602539\n",
      "Epoch [1501/2000], L_train: 33.69546127319336, L_test: 41.54067611694336\n",
      "Epoch [1601/2000], L_train: 35.68449020385742, L_test: 41.57988357543945\n",
      "Epoch [1701/2000], L_train: 32.84487533569336, L_test: 41.55318832397461\n",
      "Epoch [1801/2000], L_train: 30.811525344848633, L_test: 41.56181716918945\n",
      "Epoch [1901/2000], L_train: 34.898170471191406, L_test: 41.62929153442383\n",
      "Best Learning Rate: 0.001, Best Weight Decay: 0.001, Lowest L_test: 19.116600036621094\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-10, 1e-5, 1e-3]  # Possible learning rates\n",
    "weight_decays = [1e-2, 1e-3, 1e-4, 1e-5]  # Possible weight decay values\n",
    "n_observations = len(state[0])\n",
    "epochs=2000\n",
    "\n",
    "best_lr = None\n",
    "best_weight_decay = None\n",
    "lowest_test_loss = float('inf')  # Initialize with infinity to ensure any first result is better\n",
    "\n",
    "# value_net is your prediction function, take state as input and output is the prediction price\n",
    "value_net = NN(n_observations).to(device)\n",
    "\n",
    "for LR in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        print(f\"Testing with LR = {LR} and Weight Decay = {wd}\")\n",
    "        # 'optimize'  is an easy package to do Gredient decent, 'Adam' is a method to let learing rate decay as step go.\n",
    "        optimizer = optim.Adam(value_net.parameters(), lr=LR,weight_decay=wd)\n",
    "    \n",
    "        # Run the training loop for this combination of parameters\n",
    "        for epoch in range(epochs):\n",
    "            l_train, l_test = optimize_model()  # Use the current lr and wd in your optimization\n",
    "            if epoch % 100 == 0:  # Report every 100 epochs\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], L_train: {l_train}, L_test: {l_test}')\n",
    "\n",
    "            # Save parameters if this is the best we've seen\n",
    "            if l_test < lowest_test_loss:\n",
    "                lowest_test_loss = l_test\n",
    "                best_lr = LR\n",
    "                best_weight_decay = wd\n",
    "\n",
    "# Output the best parameters and the test loss achieved with them\n",
    "print(f\"Best Learning Rate: {best_lr}, Best Weight Decay: {best_weight_decay}, Lowest L_test: {lowest_test_loss}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], L_train: 34.40766906738281,L_test: 41.59270477294922\n",
      "Epoch [101/10000], L_train: 36.63518524169922,L_test: 41.575870513916016\n",
      "Epoch [201/10000], L_train: 33.14952087402344,L_test: 41.59728240966797\n",
      "Epoch [301/10000], L_train: 35.45770263671875,L_test: 41.57453536987305\n",
      "Epoch [401/10000], L_train: 32.29743194580078,L_test: 40.998802185058594\n",
      "Epoch [501/10000], L_train: 34.25470733642578,L_test: 41.23904800415039\n",
      "Epoch [601/10000], L_train: 80.77875518798828,L_test: 72.77326965332031\n",
      "Epoch [701/10000], L_train: 44.58434295654297,L_test: 51.333740234375\n",
      "Epoch [801/10000], L_train: 43.650020599365234,L_test: 54.22959518432617\n",
      "Epoch [901/10000], L_train: 49.00927734375,L_test: 53.5388069152832\n",
      "Epoch [1001/10000], L_train: 40.38454055786133,L_test: 54.02301025390625\n",
      "Epoch [1101/10000], L_train: 41.03861618041992,L_test: 53.37639617919922\n",
      "Epoch [1201/10000], L_train: 41.92079162597656,L_test: 49.35393524169922\n",
      "Epoch [1301/10000], L_train: 44.359127044677734,L_test: 43.93617248535156\n",
      "Epoch [1401/10000], L_train: 43.92472839355469,L_test: 45.696250915527344\n",
      "Epoch [1501/10000], L_train: 33.105857849121094,L_test: 41.69044494628906\n",
      "Epoch [1601/10000], L_train: 31.717744827270508,L_test: 41.041507720947266\n",
      "Epoch [1701/10000], L_train: 33.343223571777344,L_test: 41.188743591308594\n",
      "Epoch [1801/10000], L_train: 33.010677337646484,L_test: 41.30295181274414\n",
      "Epoch [1901/10000], L_train: 80.9449234008789,L_test: 68.7904281616211\n",
      "Epoch [2001/10000], L_train: 41.2288932800293,L_test: 51.28843688964844\n",
      "Epoch [2101/10000], L_train: 42.01103210449219,L_test: 48.29543685913086\n",
      "Epoch [2201/10000], L_train: 40.72554016113281,L_test: 46.96848678588867\n",
      "Epoch [2301/10000], L_train: 35.234642028808594,L_test: 41.64808654785156\n",
      "Epoch [2401/10000], L_train: 79.82583618164062,L_test: 66.97453308105469\n",
      "Epoch [2501/10000], L_train: 77.6181411743164,L_test: 66.21756744384766\n",
      "Epoch [2601/10000], L_train: 73.64195251464844,L_test: 65.56514739990234\n",
      "Epoch [2701/10000], L_train: 77.85493469238281,L_test: 64.93502807617188\n",
      "Epoch [2801/10000], L_train: 74.08503723144531,L_test: 64.3243637084961\n",
      "Epoch [2901/10000], L_train: 42.2670783996582,L_test: 53.10932540893555\n",
      "Epoch [3001/10000], L_train: 39.9500732421875,L_test: 47.194984436035156\n",
      "Epoch [3101/10000], L_train: 34.42755889892578,L_test: 41.731449127197266\n",
      "Epoch [3201/10000], L_train: 35.605003356933594,L_test: 41.56578063964844\n",
      "Epoch [3301/10000], L_train: 32.787410736083984,L_test: 41.55950164794922\n",
      "Epoch [3401/10000], L_train: 34.82806396484375,L_test: 41.56595230102539\n",
      "Epoch [3501/10000], L_train: 30.712936401367188,L_test: 41.563148498535156\n",
      "Epoch [3601/10000], L_train: 33.17041778564453,L_test: 41.58119583129883\n",
      "Epoch [3701/10000], L_train: 32.85002899169922,L_test: 41.578487396240234\n",
      "Epoch [3801/10000], L_train: 43.43844985961914,L_test: 193.02052307128906\n",
      "Epoch [3901/10000], L_train: 69.70503234863281,L_test: 62.397010803222656\n",
      "Epoch [4001/10000], L_train: 74.01729583740234,L_test: 61.67288589477539\n",
      "Epoch [4101/10000], L_train: 65.98160552978516,L_test: 61.02433395385742\n",
      "Epoch [4201/10000], L_train: 69.8731918334961,L_test: 60.40302658081055\n",
      "Epoch [4301/10000], L_train: 39.09911346435547,L_test: 49.57275390625\n",
      "Epoch [4401/10000], L_train: 38.187992095947266,L_test: 51.200172424316406\n",
      "Epoch [4501/10000], L_train: 37.132572174072266,L_test: 41.87293243408203\n",
      "Epoch [4601/10000], L_train: 35.372032165527344,L_test: 41.584381103515625\n",
      "Epoch [4701/10000], L_train: 39.766441345214844,L_test: 52.79270935058594\n",
      "Epoch [4801/10000], L_train: 70.18144989013672,L_test: 58.967567443847656\n",
      "Epoch [4901/10000], L_train: 45.68508529663086,L_test: 47.35211181640625\n",
      "Epoch [5001/10000], L_train: 42.651737213134766,L_test: 48.3286018371582\n",
      "Epoch [5101/10000], L_train: 39.27114486694336,L_test: 49.33213806152344\n",
      "Epoch [5201/10000], L_train: 40.529685974121094,L_test: 48.25261306762695\n",
      "Epoch [5301/10000], L_train: 39.071842193603516,L_test: 46.21220779418945\n",
      "Epoch [5401/10000], L_train: 33.79003143310547,L_test: 41.590877532958984\n",
      "Epoch [5501/10000], L_train: 61.3982048034668,L_test: 56.63542938232422\n",
      "Epoch [5601/10000], L_train: 39.46766662597656,L_test: 48.90293502807617\n",
      "Epoch [5701/10000], L_train: 61.72443389892578,L_test: 56.188804626464844\n",
      "Epoch [5801/10000], L_train: 66.3236083984375,L_test: 55.305057525634766\n",
      "Epoch [5901/10000], L_train: 65.49620056152344,L_test: 54.61127853393555\n",
      "Epoch [6001/10000], L_train: 63.90936279296875,L_test: 53.984283447265625\n",
      "Epoch [6101/10000], L_train: 63.49654769897461,L_test: 53.39442443847656\n",
      "Epoch [6201/10000], L_train: 60.76072692871094,L_test: 52.84318923950195\n",
      "Epoch [6301/10000], L_train: 55.68707275390625,L_test: 52.31790542602539\n",
      "Epoch [6401/10000], L_train: 63.57750701904297,L_test: 51.8132438659668\n",
      "Epoch [6501/10000], L_train: 55.4775276184082,L_test: 51.339969635009766\n",
      "Epoch [6601/10000], L_train: 59.53578567504883,L_test: 50.91067886352539\n",
      "Epoch [6701/10000], L_train: 33.465423583984375,L_test: 41.60605239868164\n",
      "Epoch [6801/10000], L_train: 33.793128967285156,L_test: 41.45231246948242\n",
      "Epoch [6901/10000], L_train: 31.4865665435791,L_test: 41.53324508666992\n",
      "Epoch [7001/10000], L_train: 32.85784149169922,L_test: 41.5683708190918\n",
      "Epoch [7101/10000], L_train: 33.65939712524414,L_test: 41.47170639038086\n",
      "Epoch [7201/10000], L_train: 36.852962493896484,L_test: 41.228641510009766\n",
      "Epoch [7301/10000], L_train: 33.48371505737305,L_test: 41.33654022216797\n",
      "Epoch [7401/10000], L_train: 33.82614517211914,L_test: 41.39977264404297\n",
      "Epoch [7501/10000], L_train: 32.40524673461914,L_test: 41.17023849487305\n",
      "Epoch [7601/10000], L_train: 32.978919982910156,L_test: 41.037750244140625\n",
      "Epoch [7701/10000], L_train: 32.107784271240234,L_test: 41.19227981567383\n",
      "Epoch [7801/10000], L_train: 32.93272399902344,L_test: 41.302772521972656\n",
      "Epoch [7901/10000], L_train: 31.724306106567383,L_test: 41.01043701171875\n",
      "Epoch [8001/10000], L_train: 31.688879013061523,L_test: 41.17329788208008\n",
      "Epoch [8101/10000], L_train: 34.680458068847656,L_test: 41.28432846069336\n",
      "Epoch [8201/10000], L_train: 33.40995407104492,L_test: 41.14433288574219\n",
      "Epoch [8301/10000], L_train: 36.321937561035156,L_test: 41.09165954589844\n",
      "Epoch [8401/10000], L_train: 33.216346740722656,L_test: 41.22457504272461\n",
      "Epoch [8501/10000], L_train: 32.82059097290039,L_test: 41.3277473449707\n",
      "Epoch [8601/10000], L_train: 36.174556732177734,L_test: 41.418128967285156\n",
      "Epoch [8701/10000], L_train: 34.506187438964844,L_test: 41.093265533447266\n",
      "Epoch [8801/10000], L_train: 35.7606201171875,L_test: 41.19053268432617\n",
      "Epoch [8901/10000], L_train: 36.19093322753906,L_test: 41.318092346191406\n",
      "Epoch [9001/10000], L_train: 39.94808578491211,L_test: 44.151519775390625\n",
      "Epoch [9101/10000], L_train: 36.91587829589844,L_test: 41.513938903808594\n",
      "Epoch [9201/10000], L_train: 35.177818298339844,L_test: 41.52935028076172\n",
      "Epoch [9301/10000], L_train: 35.256587982177734,L_test: 41.60240173339844\n",
      "Epoch [9401/10000], L_train: 33.901607513427734,L_test: 41.07425308227539\n",
      "Epoch [9501/10000], L_train: 33.1031379699707,L_test: 41.188636779785156\n",
      "Epoch [9601/10000], L_train: 32.41729736328125,L_test: 41.305809020996094\n",
      "Epoch [9701/10000], L_train: 33.11619567871094,L_test: 41.378143310546875\n",
      "Epoch [9801/10000], L_train: 35.23534393310547,L_test: 41.49187469482422\n",
      "Epoch [9901/10000], L_train: 54.35523986816406,L_test: 48.313175201416016\n"
     ]
    }
   ],
   "source": [
    "# continue optimize\n",
    "# start from last training result\n",
    "epochs=10000\n",
    "LR = 1e-3\n",
    "Weight_decay=0.01\n",
    "value_net.train()\n",
    "for epoch in range(epochs):\n",
    "    l_train,l_test=optimize_model()\n",
    "    if epoch % 10 ==0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], L_train: {l_train},L_test: {l_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save last line results\n",
    "torch.save(value_net.state_dict(),'model_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "loaded_model = NN(n_observations).to(device)\n",
    "value_net.load_state_dict(torch.load('model_parameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[352], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#load the trained parameter to the model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m NN(n_observations)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_parameters.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[0;32m     11\u001b[0m value_net\u001b[38;5;241m=\u001b[39mloaded_model \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# predict your own price\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "windowed_data, predict_prices = slide_windows(pd.read_csv('TSLA_stock_data_2024.csv'))\n",
    "# Convert to PyTorch tensors\n",
    "state_new = torch.tensor(windowed_data, dtype=torch.float32).to(device)\n",
    "real_price = torch.tensor(predict_prices, dtype=torch.float32).to(device)\n",
    "\n",
    "#load the trained parameter to the model\n",
    "# loaded_model = NN(n_observations).to(device)\n",
    "# loaded_model.load_state_dict(torch.load('model_parameters.pth')) \n",
    "# value_net=loaded_model \n",
    "\n",
    "# predict your own price\n",
    "value_net.eval()  \n",
    "with torch.no_grad():\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    predict_price=loaded_model(state_new).squeeze() #predict price\n",
    "    l_test=criterion(real_price,predict_price) #calculate difference\n",
    "print(l_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
