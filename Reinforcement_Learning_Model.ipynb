{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "Python Environment: Use an environment with Python and necessary libraries installed (e.g., numpy, pandas, matplotlib for data manipulation and visualization; TensorFlow or PyTorch for neural network modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'stock_data' is a DataFrame containing the stock data\n",
    "file_path = 'TSLA_stock_data_2023.csv'\n",
    "stock_data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using min-max scaling\n",
    "\n",
    "# Convert date column to datetime if it exists\n",
    "if 'Date' in stock_data.columns:\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data['Year'] = stock_data['Date'].dt.year\n",
    "    stock_data['Month'] = stock_data['Date'].dt.month\n",
    "    stock_data['Day'] = stock_data['Date'].dt.day\n",
    "    # Optionally, drop the original date column if no longer needed\n",
    "    stock_data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# If there are categorical columns, consider converting them to a one-hot encoded format\n",
    "if 'CategoryColumn' in stock_data.columns:\n",
    "    # This is an example; replace 'CategoryColumn' with the name of your actual column\n",
    "    dummies = pd.get_dummies(stock_data['CategoryColumn'], prefix='Category')\n",
    "    stock_data = pd.concat([stock_data, dummies], axis=1)\n",
    "    stock_data.drop('CategoryColumn', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1444/3442148219.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  stock_data.fillna(method='ffill', inplace=True)  # forward fill to propagate last valid observation forward\n"
     ]
    }
   ],
   "source": [
    "# Select only the numeric columns for normalization\n",
    "numeric_cols = stock_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "stock_data[numeric_cols] = (stock_data[numeric_cols] - stock_data[numeric_cols].min()) / (stock_data[numeric_cols].max() - stock_data[numeric_cols].min())\n",
    "\n",
    "stock_data.fillna(method='ffill', inplace=True)  # forward fill to propagate last valid observation forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define State and Reward\n",
    "State Definition: Define the state as a vector of features like the day's opening price, high, low, close, and volume.\n",
    "Reward Calculation: Calculate rewards based on the change in stock price, as described in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_state(data):\n",
    "    # Assuming 'data' is a DataFrame with columns for open, high, low, close, volume\n",
    "    # Normalizing data\n",
    "    max_vals = data.max()\n",
    "    min_vals = data.min()\n",
    "    state_vector = (data - min_vals) / (max_vals - min_vals)\n",
    "    return state_vector.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(close_prices):\n",
    "    # Assuming 'close_prices' is a list or array of closing prices\n",
    "    rewards = np.diff(close_prices) / close_prices[:-1]  # Percentage change between consecutive days\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Model\n",
    "Model Initialization: Initialize the parameters for the TD(0) algorithm, including the discount factor (γ) and learning rate (α).\n",
    "Network Setup: Set up a neural network for function approximation. A simple multi-layer perceptron (MLP) can be used initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network for the agent\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(3, activation='linear')  # Assuming three actions: buy, hold, sell\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "num_features = 5  # e.g., open, high, low, close, volume\n",
    "model = create_model(num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each step within the episode, decides an action, executes it, and then observes the outcome.\n",
    "def take_action(state, action, data, t):\n",
    "    # This function should define how to take an action\n",
    "    # For simplicity, we're not really trading but simulating action effects\n",
    "    next_state = data.iloc[t + 1]\n",
    "    reward = calculate_reward(state['close'], next_state['close'])\n",
    "    return next_state, reward\n",
    "\n",
    "def calculate_reward(current_price, next_price):\n",
    "    return (next_price - current_price) / current_price  # Percentage change\n",
    "\n",
    "def update_model(model, state, action, reward, next_state):\n",
    "    # Perform a TD update on the model\n",
    "    target = reward + 0.95 * np.amax(model.predict(np.array([next_state]))[0])  # Discount factor gamma = 0.95\n",
    "    target_vec = model.predict(np.array([state]))[0]\n",
    "    target_vec[action] = target\n",
    "    model.fit(np.array([state]), np.array([target_vec]), epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate each trading period as an episode. For each episode, reset the environment to an initial state\n",
    "\n",
    "def run_episode(data, model):\n",
    "    total_reward = 0\n",
    "    state = get_initial_state(data)\n",
    "\n",
    "    for t in range(len(data) - 1):\n",
    "        action = choose_action(state, model)\n",
    "        next_state, reward = take_action(state, action, data, t)\n",
    "        update_model(model, state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def get_initial_state(data):\n",
    "    # Normalize data and prepare the initial state\n",
    "    return data.iloc[0]\n",
    "\n",
    "# def choose_action(state, model):\n",
    "#     if isinstance(state, pd.Series):\n",
    "#         state = state.values \n",
    "#     # state = state.astype('float32')\n",
    "#     # Use the model to predict the action from the current state\n",
    "#     state = np.reshape(state, (1, -1))\n",
    "#     q_values = model.predict(state)\n",
    "#     return np.argmax(q_values[0])  # Choosing the action with the highest Q-value\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, P, R, Q, discount_factor):\n",
    "    # Ensure 'state' is the index of your current state.\n",
    "    \n",
    "    # This will store the expected Q-values for each possible action from the current state.\n",
    "    action_values = np.zeros(number_of_actions)\n",
    "    \n",
    "    for action in range(number_of_actions):\n",
    "        # Initialize the Q-value for this state-action pair.\n",
    "        Q_value = 0\n",
    "        for next_state in range(number_of_states):\n",
    "            # Calculate the Q-value using the transition probability, the reward, and the discounted Q-value.\n",
    "            Q_value += P[state, action, next_state] * (R[state, action, next_state] + discount_factor * max(Q[next_state]))\n",
    "        \n",
    "        action_values[action] = Q_value\n",
    "    \n",
    "    # Choose the action with the highest Q-value.\n",
    "    return np.argmax(action_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Close' column does not exist in stock_data\n",
      "(None, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "choose_action() missing 3 required positional arguments: 'R', 'Q', and 'discount_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[268], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(stock_data[numeric_cols]\u001b[38;5;241m.\u001b[39mcolumns)  \u001b[38;5;66;03m# Update to match the number of input features after preprocessing\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(num_features)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(stock_data\u001b[38;5;241m.\u001b[39mcolumns) \n",
      "Cell \u001b[0;32mIn[268], line 4\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data, model, episodes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(data, model, episodes):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m----> 4\u001b[0m         total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[266], line 8\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m state \u001b[38;5;241m=\u001b[39m get_initial_state(data)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     next_state, reward \u001b[38;5;241m=\u001b[39m take_action(state, action, data, t)\n\u001b[1;32m     10\u001b[0m     update_model(model, state, action, reward, next_state)\n",
      "\u001b[0;31mTypeError\u001b[0m: choose_action() missing 3 required positional arguments: 'R', 'Q', and 'discount_factor'"
     ]
    }
   ],
   "source": [
    "# Run multiple episodes to train the model effectively.\n",
    "def train_model(data, model, episodes):\n",
    "    for e in range(episodes):\n",
    "        total_reward = run_episode(data, model)\n",
    "        print(f'Episode {e+1}/{episodes}, Total Reward: {total_reward}')\n",
    "# Initialize the model\n",
    "# Using .get() which returns None if the column doesn't exist\n",
    "close_prices = stock_data.get('Close')\n",
    "\n",
    "# Or you can check if 'close' is in columns before accessing it\n",
    "if 'close' in stock_data.columns:\n",
    "    close_prices = stock_data['Close']\n",
    "else:\n",
    "    print(\"'Close' column does not exist in stock_data\")\n",
    "print(model.input_shape) \n",
    "num_features = len(stock_data[numeric_cols].columns)  # Update to match the number of input features after preprocessing\n",
    "model = create_model(num_features)\n",
    "train_model(stock_data, model, 10000)\n",
    "print(stock_data.columns) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Algorithm: Implement the TD(0) learning algorithm to update the value function based on the state and reward observed from the data.\n",
    "Iteration: Iterate over episodes (each episode can be a sequence of stock price data), updating the model with each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Testing: After training, test the model on unseen data to assess its predictive accuracy.\n",
    "Performance Metrics: Use metrics like RMSE or predictive accuracy grades as used in the paper to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Adjustment\n",
    "Continuous Monitoring: Set up scripts to monitor the model’s performance over time.\n",
    "Adjustment: Tune parameters and refine the model as needed based on performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
