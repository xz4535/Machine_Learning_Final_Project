{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  False\n",
      "CUDA version:  None\n",
      "Number of CUDA devices:  0\n",
      "CUDA Device Name:  No CUDA device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "print(\"Number of CUDA devices: \", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name: \", torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else \"No CUDA device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data_file = 'TSLA_stock_data_2023.csv'\n",
    "predict_data_file = 'TSLA_stock_data_2024.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sliding windows\n",
    "def slide_windows(features):\n",
    "\n",
    "    # We need to ensure we have data for t-1, t, t+1 without index errors\n",
    "    windowed_data = []\n",
    "    predict_prices = []  # List to store the target 'Close' prices\n",
    "    cycle_data=[]\n",
    "\n",
    "    # Handling daily cycles\n",
    "    cycle_length = 7\n",
    "    for cycle in range(len(features) - cycle_length + 1):\n",
    "        start_index = cycle\n",
    "        end_index = start_index + cycle_length\n",
    "        cycle_datas = features.iloc[start_index:end_index,1:6].values\n",
    "        cycle_data.append(cycle_datas)\n",
    "        \n",
    "    # Creating sliding windows within the cycle\n",
    "    for i in range(14,len(cycle_data)-7):  # Avoiding index error by stopping before the last day\n",
    "        pre_previous = cycle_data[i - 14]\n",
    "        previous = cycle_data[i- 7]\n",
    "        current_state = cycle_data[i]\n",
    "\n",
    "        combined_features = np.concatenate([pre_previous, previous, current_state]).reshape(1,-1).squeeze()\n",
    "        windowed_data.append(combined_features)\n",
    "        predict_prices.append(cycle_data[i+7][-1][3])\n",
    "    \n",
    "    return windowed_data, predict_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26210\\AppData\\Local\\Temp\\ipykernel_22108\\181566533.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  state = torch.tensor(windowed_data, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "row_data = pd.read_csv(row_data_file)\n",
    "\n",
    "windowed_data, predict_prices = slide_windows(row_data)\n",
    "# Convert to PyTorch tensors\n",
    "state = torch.tensor(windowed_data, dtype=torch.float32).to(device)\n",
    "predict_price = torch.tensor(predict_prices, dtype=torch.float32).to(device)\n",
    "# state[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into training part and test part\n",
    "state_train, state_test, predict_price_train,predict_price_test= train_test_split(state, predict_price,test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(TensorDataset(state_train, predict_price_train), batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(state_test, predict_price_test), batch_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct NN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimazation function, it do one step of gredient decent, \n",
    "def optimize_model():\n",
    "    for state, target in train_loader:\n",
    "        current_value=value_net(state).squeeze()              \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss=criterion(current_value,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for state, target in test_loader:\n",
    "        with torch.no_grad():\n",
    "            test_valur=value_net(state).squeeze()                 \n",
    "            l_test=criterion(test_valur,target)\n",
    "    return loss.item(),l_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with LR = 1e-10 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 404807.625, L_test: 399929.78125\n",
      "Epoch [101/2000], L_train: 349372.5, L_test: 399929.21875\n",
      "Epoch [201/2000], L_train: 333433.46875, L_test: 399928.59375\n",
      "Epoch [301/2000], L_train: 367545.4375, L_test: 399928.0\n",
      "Epoch [401/2000], L_train: 365227.375, L_test: 399927.4375\n",
      "Epoch [501/2000], L_train: 388198.03125, L_test: 399926.875\n",
      "Epoch [601/2000], L_train: 342717.65625, L_test: 399926.3125\n",
      "Epoch [701/2000], L_train: 341645.78125, L_test: 399925.78125\n",
      "Epoch [801/2000], L_train: 363376.3125, L_test: 399925.15625\n",
      "Epoch [901/2000], L_train: 387314.9375, L_test: 399924.5625\n",
      "Epoch [1001/2000], L_train: 405618.21875, L_test: 399924.03125\n",
      "Epoch [1101/2000], L_train: 373733.4375, L_test: 399923.5\n",
      "Epoch [1201/2000], L_train: 357462.46875, L_test: 399922.84375\n",
      "Epoch [1301/2000], L_train: 351467.5, L_test: 399922.3125\n",
      "Epoch [1401/2000], L_train: 391673.8125, L_test: 399921.6875\n",
      "Epoch [1501/2000], L_train: 351041.96875, L_test: 399921.125\n",
      "Epoch [1601/2000], L_train: 343846.5625, L_test: 399920.5625\n",
      "Epoch [1701/2000], L_train: 402869.71875, L_test: 399920.0\n",
      "Epoch [1801/2000], L_train: 327630.9375, L_test: 399919.40625\n",
      "Epoch [1901/2000], L_train: 353114.6875, L_test: 399918.875\n",
      "Testing with LR = 1e-10 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 344603.3125, L_test: 399918.28125\n",
      "Epoch [101/2000], L_train: 396328.96875, L_test: 399917.6875\n",
      "Epoch [201/2000], L_train: 390215.4375, L_test: 399917.15625\n",
      "Epoch [301/2000], L_train: 346753.5, L_test: 399916.5625\n",
      "Epoch [401/2000], L_train: 385879.59375, L_test: 399915.96875\n",
      "Epoch [501/2000], L_train: 379108.75, L_test: 399915.40625\n",
      "Epoch [601/2000], L_train: 321757.5, L_test: 399914.84375\n",
      "Epoch [701/2000], L_train: 336518.0, L_test: 399914.28125\n",
      "Epoch [801/2000], L_train: 313270.21875, L_test: 399913.6875\n",
      "Epoch [901/2000], L_train: 357787.8125, L_test: 399913.125\n",
      "Epoch [1001/2000], L_train: 348699.96875, L_test: 399912.5625\n",
      "Epoch [1101/2000], L_train: 344310.15625, L_test: 399911.96875\n",
      "Epoch [1201/2000], L_train: 355768.75, L_test: 399911.34375\n",
      "Epoch [1301/2000], L_train: 354789.59375, L_test: 399910.78125\n",
      "Epoch [1401/2000], L_train: 386870.3125, L_test: 399910.28125\n",
      "Epoch [1501/2000], L_train: 382093.78125, L_test: 399909.71875\n",
      "Epoch [1601/2000], L_train: 365020.59375, L_test: 399909.125\n",
      "Epoch [1701/2000], L_train: 322457.875, L_test: 399908.5\n",
      "Epoch [1801/2000], L_train: 373321.5, L_test: 399907.96875\n",
      "Epoch [1901/2000], L_train: 344983.84375, L_test: 399907.34375\n",
      "Testing with LR = 1e-10 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 328534.5625, L_test: 399906.84375\n",
      "Epoch [101/2000], L_train: 354490.65625, L_test: 399906.21875\n",
      "Epoch [201/2000], L_train: 363522.15625, L_test: 399905.6875\n",
      "Epoch [301/2000], L_train: 380395.75, L_test: 399905.125\n",
      "Epoch [401/2000], L_train: 362835.125, L_test: 399904.46875\n",
      "Epoch [501/2000], L_train: 323827.4375, L_test: 399903.96875\n",
      "Epoch [601/2000], L_train: 360559.375, L_test: 399903.34375\n",
      "Epoch [701/2000], L_train: 338737.90625, L_test: 399902.75\n",
      "Epoch [801/2000], L_train: 291268.90625, L_test: 399902.1875\n",
      "Epoch [901/2000], L_train: 346347.75, L_test: 399901.625\n",
      "Epoch [1001/2000], L_train: 370955.53125, L_test: 399901.03125\n",
      "Epoch [1101/2000], L_train: 390410.75, L_test: 399900.5\n",
      "Epoch [1201/2000], L_train: 391119.0625, L_test: 399899.90625\n",
      "Epoch [1301/2000], L_train: 364777.03125, L_test: 399899.34375\n",
      "Epoch [1401/2000], L_train: 324758.5, L_test: 399898.75\n",
      "Epoch [1501/2000], L_train: 382829.6875, L_test: 399898.1875\n",
      "Epoch [1601/2000], L_train: 347915.21875, L_test: 399897.625\n",
      "Epoch [1701/2000], L_train: 313394.875, L_test: 399897.0625\n",
      "Epoch [1801/2000], L_train: 361686.71875, L_test: 399896.46875\n",
      "Epoch [1901/2000], L_train: 363792.03125, L_test: 399895.90625\n",
      "Testing with LR = 1e-10 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 356602.78125, L_test: 399895.3125\n",
      "Epoch [101/2000], L_train: 379786.6875, L_test: 399894.78125\n",
      "Epoch [201/2000], L_train: 329192.96875, L_test: 399894.125\n",
      "Epoch [301/2000], L_train: 328573.6875, L_test: 399893.625\n",
      "Epoch [401/2000], L_train: 323654.59375, L_test: 399893.03125\n",
      "Epoch [501/2000], L_train: 314528.3125, L_test: 399892.40625\n",
      "Epoch [601/2000], L_train: 344894.8125, L_test: 399891.90625\n",
      "Epoch [701/2000], L_train: 365431.0625, L_test: 399891.3125\n",
      "Epoch [801/2000], L_train: 339924.71875, L_test: 399890.75\n",
      "Epoch [901/2000], L_train: 384876.78125, L_test: 399890.125\n",
      "Epoch [1001/2000], L_train: 360826.59375, L_test: 399889.625\n",
      "Epoch [1101/2000], L_train: 349813.5625, L_test: 399889.03125\n",
      "Epoch [1201/2000], L_train: 337264.1875, L_test: 399888.46875\n",
      "Epoch [1301/2000], L_train: 372551.125, L_test: 399887.875\n",
      "Epoch [1401/2000], L_train: 357589.65625, L_test: 399887.3125\n",
      "Epoch [1501/2000], L_train: 341717.125, L_test: 399886.75\n",
      "Epoch [1601/2000], L_train: 363444.28125, L_test: 399886.125\n",
      "Epoch [1701/2000], L_train: 342748.15625, L_test: 399885.59375\n",
      "Epoch [1801/2000], L_train: 361099.25, L_test: 399885.03125\n",
      "Epoch [1901/2000], L_train: 345516.59375, L_test: 399884.46875\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 333396.40625, L_test: 375158.28125\n",
      "Epoch [101/2000], L_train: 35472.8359375, L_test: 48685.3359375\n",
      "Epoch [201/2000], L_train: 21709.154296875, L_test: 31581.775390625\n",
      "Epoch [301/2000], L_train: 16199.958984375, L_test: 26111.1953125\n",
      "Epoch [401/2000], L_train: 13521.3681640625, L_test: 24263.34375\n",
      "Epoch [501/2000], L_train: 13312.4228515625, L_test: 23054.30859375\n",
      "Epoch [601/2000], L_train: 11784.0185546875, L_test: 22297.72265625\n",
      "Epoch [701/2000], L_train: 8447.587890625, L_test: 21453.546875\n",
      "Epoch [801/2000], L_train: 8070.751953125, L_test: 20362.6875\n",
      "Epoch [901/2000], L_train: 7854.06201171875, L_test: 19535.560546875\n",
      "Epoch [1001/2000], L_train: 7854.6875, L_test: 19004.29296875\n",
      "Epoch [1101/2000], L_train: 6391.8525390625, L_test: 18295.9453125\n",
      "Epoch [1201/2000], L_train: 5669.15576171875, L_test: 18059.236328125\n",
      "Epoch [1301/2000], L_train: 5522.03125, L_test: 17829.35546875\n",
      "Epoch [1401/2000], L_train: 5188.64599609375, L_test: 17377.14453125\n",
      "Epoch [1501/2000], L_train: 5217.05810546875, L_test: 17304.810546875\n",
      "Epoch [1601/2000], L_train: 5049.40087890625, L_test: 16776.0703125\n",
      "Epoch [1701/2000], L_train: 4082.08203125, L_test: 16786.5859375\n",
      "Epoch [1801/2000], L_train: 4181.98974609375, L_test: 16398.103515625\n",
      "Epoch [1901/2000], L_train: 4823.8720703125, L_test: 16109.552734375\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 5455.15625, L_test: 16548.00390625\n",
      "Epoch [101/2000], L_train: 4218.83349609375, L_test: 16204.9384765625\n",
      "Epoch [201/2000], L_train: 3456.31787109375, L_test: 15758.5263671875\n",
      "Epoch [301/2000], L_train: 4689.55810546875, L_test: 15792.6904296875\n",
      "Epoch [401/2000], L_train: 4631.048828125, L_test: 15660.43359375\n",
      "Epoch [501/2000], L_train: 3650.895263671875, L_test: 15770.484375\n",
      "Epoch [601/2000], L_train: 3164.346923828125, L_test: 15522.384765625\n",
      "Epoch [701/2000], L_train: 3523.653076171875, L_test: 15449.5185546875\n",
      "Epoch [801/2000], L_train: 3127.478271484375, L_test: 15193.8984375\n",
      "Epoch [901/2000], L_train: 3298.025146484375, L_test: 15095.3837890625\n",
      "Epoch [1001/2000], L_train: 2996.024169921875, L_test: 15105.154296875\n",
      "Epoch [1101/2000], L_train: 2969.380615234375, L_test: 15231.6005859375\n",
      "Epoch [1201/2000], L_train: 2735.031494140625, L_test: 14879.9853515625\n",
      "Epoch [1301/2000], L_train: 2833.923828125, L_test: 14806.2080078125\n",
      "Epoch [1401/2000], L_train: 2639.090576171875, L_test: 14687.1455078125\n",
      "Epoch [1501/2000], L_train: 2942.642578125, L_test: 14566.3603515625\n",
      "Epoch [1601/2000], L_train: 2885.326904296875, L_test: 14529.5322265625\n",
      "Epoch [1701/2000], L_train: 2020.727294921875, L_test: 14599.34765625\n",
      "Epoch [1801/2000], L_train: 2675.74755859375, L_test: 14412.9638671875\n",
      "Epoch [1901/2000], L_train: 2667.343017578125, L_test: 14286.859375\n",
      "Testing with LR = 1e-05 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 2537.4033203125, L_test: 14183.908203125\n",
      "Epoch [101/2000], L_train: 2187.97119140625, L_test: 14300.9697265625\n",
      "Epoch [201/2000], L_train: 2383.492431640625, L_test: 14175.505859375\n",
      "Epoch [301/2000], L_train: 2013.449462890625, L_test: 14154.6220703125\n",
      "Epoch [401/2000], L_train: 2448.31201171875, L_test: 14228.021484375\n",
      "Epoch [501/2000], L_train: 2232.307861328125, L_test: 13957.046875\n",
      "Epoch [601/2000], L_train: 2479.068115234375, L_test: 13973.833984375\n",
      "Epoch [701/2000], L_train: 2174.677001953125, L_test: 14126.62890625\n",
      "Epoch [801/2000], L_train: 2089.925537109375, L_test: 14052.2822265625\n",
      "Epoch [901/2000], L_train: 2213.177734375, L_test: 13959.232421875\n",
      "Epoch [1001/2000], L_train: 1674.0406494140625, L_test: 13716.857421875\n",
      "Epoch [1101/2000], L_train: 2414.166748046875, L_test: 13860.7685546875\n",
      "Epoch [1201/2000], L_train: 2048.6201171875, L_test: 13899.30078125\n",
      "Epoch [1301/2000], L_train: 1864.0264892578125, L_test: 13832.642578125\n",
      "Epoch [1401/2000], L_train: 2174.484130859375, L_test: 13439.44921875\n",
      "Epoch [1501/2000], L_train: 1515.68798828125, L_test: 13704.59765625\n",
      "Epoch [1601/2000], L_train: 1269.2867431640625, L_test: 13763.005859375\n",
      "Epoch [1701/2000], L_train: 1433.00341796875, L_test: 13596.13671875\n",
      "Epoch [1801/2000], L_train: 1342.7562255859375, L_test: 13829.8896484375\n",
      "Epoch [1901/2000], L_train: 1248.1307373046875, L_test: 13583.3544921875\n",
      "Testing with LR = 1e-05 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 7740.21826171875, L_test: 14582.041015625\n",
      "Epoch [101/2000], L_train: 1463.0216064453125, L_test: 13532.83984375\n",
      "Epoch [201/2000], L_train: 1213.431640625, L_test: 13607.7822265625\n",
      "Epoch [301/2000], L_train: 1458.07861328125, L_test: 13576.1806640625\n",
      "Epoch [401/2000], L_train: 1442.322998046875, L_test: 13464.41015625\n",
      "Epoch [501/2000], L_train: 1589.66357421875, L_test: 13368.7822265625\n",
      "Epoch [601/2000], L_train: 1515.235595703125, L_test: 13204.1103515625\n",
      "Epoch [701/2000], L_train: 1002.1176147460938, L_test: 13250.8359375\n",
      "Epoch [801/2000], L_train: 1486.9815673828125, L_test: 13166.3515625\n",
      "Epoch [901/2000], L_train: 1291.6009521484375, L_test: 13179.478515625\n",
      "Epoch [1001/2000], L_train: 1093.8853759765625, L_test: 13147.9248046875\n",
      "Epoch [1101/2000], L_train: 1343.56298828125, L_test: 13294.4931640625\n",
      "Epoch [1201/2000], L_train: 1288.1014404296875, L_test: 13291.5126953125\n",
      "Epoch [1301/2000], L_train: 1342.7606201171875, L_test: 12918.1396484375\n",
      "Epoch [1401/2000], L_train: 1576.74560546875, L_test: 13147.3759765625\n",
      "Epoch [1501/2000], L_train: 1236.634765625, L_test: 13161.115234375\n",
      "Epoch [1601/2000], L_train: 1431.4337158203125, L_test: 12923.5625\n",
      "Epoch [1701/2000], L_train: 1731.7423095703125, L_test: 12840.857421875\n",
      "Epoch [1801/2000], L_train: 1636.983642578125, L_test: 12665.228515625\n",
      "Epoch [1901/2000], L_train: 1334.28857421875, L_test: 12998.755859375\n",
      "Testing with LR = 0.001 and Weight Decay = 0.01\n",
      "Epoch [1/2000], L_train: 457472.375, L_test: 201265.34375\n",
      "Epoch [101/2000], L_train: 15725.4306640625, L_test: 9778.14453125\n",
      "Epoch [201/2000], L_train: 3981.952392578125, L_test: 7656.05517578125\n",
      "Epoch [301/2000], L_train: 2247.584716796875, L_test: 4764.748046875\n",
      "Epoch [401/2000], L_train: 1990.022216796875, L_test: 2822.801025390625\n",
      "Epoch [501/2000], L_train: 1110.6990966796875, L_test: 2490.5576171875\n",
      "Epoch [601/2000], L_train: 1180.0938720703125, L_test: 1437.369140625\n",
      "Epoch [701/2000], L_train: 517.5357055664062, L_test: 1109.1494140625\n",
      "Epoch [801/2000], L_train: 432.8316345214844, L_test: 1013.3450927734375\n",
      "Epoch [901/2000], L_train: 304.1260681152344, L_test: 728.7701416015625\n",
      "Epoch [1001/2000], L_train: 286.4894714355469, L_test: 752.233154296875\n",
      "Epoch [1101/2000], L_train: 239.5912322998047, L_test: 723.1424560546875\n",
      "Epoch [1201/2000], L_train: 125.08354949951172, L_test: 628.9199829101562\n",
      "Epoch [1301/2000], L_train: 87.20877838134766, L_test: 427.186279296875\n",
      "Epoch [1401/2000], L_train: 70.71272277832031, L_test: 362.7901611328125\n",
      "Epoch [1501/2000], L_train: 52.63993835449219, L_test: 296.2069396972656\n",
      "Epoch [1601/2000], L_train: 53.270286560058594, L_test: 353.1624755859375\n",
      "Epoch [1701/2000], L_train: 50.14394760131836, L_test: 277.0804748535156\n",
      "Epoch [1801/2000], L_train: 42.10175704956055, L_test: 352.93450927734375\n",
      "Epoch [1901/2000], L_train: 70.72735595703125, L_test: 374.2790832519531\n",
      "Testing with LR = 0.001 and Weight Decay = 0.001\n",
      "Epoch [1/2000], L_train: 179.47317504882812, L_test: 613.8827514648438\n",
      "Epoch [101/2000], L_train: 62.07498550415039, L_test: 67.12995910644531\n",
      "Epoch [201/2000], L_train: 62.525184631347656, L_test: 65.97993469238281\n",
      "Epoch [301/2000], L_train: 57.61965560913086, L_test: 68.63278198242188\n",
      "Epoch [401/2000], L_train: 63.14406967163086, L_test: 61.24539566040039\n",
      "Epoch [501/2000], L_train: 55.46881866455078, L_test: 61.381858825683594\n",
      "Epoch [601/2000], L_train: 58.50602340698242, L_test: 76.62853240966797\n",
      "Epoch [701/2000], L_train: 39.7236328125, L_test: 74.03734588623047\n",
      "Epoch [801/2000], L_train: 43.779483795166016, L_test: 52.42856216430664\n",
      "Epoch [901/2000], L_train: 213.86029052734375, L_test: 203.3970947265625\n",
      "Epoch [1001/2000], L_train: 127.83040618896484, L_test: 118.30438995361328\n",
      "Epoch [1101/2000], L_train: 34.92032241821289, L_test: 39.33356857299805\n",
      "Epoch [1201/2000], L_train: 34.676963806152344, L_test: 41.14031982421875\n",
      "Epoch [1301/2000], L_train: 33.25489807128906, L_test: 41.410404205322266\n",
      "Epoch [1401/2000], L_train: 32.14744567871094, L_test: 40.931968688964844\n",
      "Epoch [1501/2000], L_train: 35.24592208862305, L_test: 41.29362869262695\n",
      "Epoch [1601/2000], L_train: 31.306896209716797, L_test: 40.92369842529297\n",
      "Epoch [1701/2000], L_train: 31.667747497558594, L_test: 41.0872917175293\n",
      "Epoch [1801/2000], L_train: 32.572471618652344, L_test: 41.27143478393555\n",
      "Epoch [1901/2000], L_train: 36.061622619628906, L_test: 40.69801712036133\n",
      "Testing with LR = 0.001 and Weight Decay = 0.0001\n",
      "Epoch [1/2000], L_train: 233.48243713378906, L_test: 3016.769287109375\n",
      "Epoch [101/2000], L_train: 61.535491943359375, L_test: 65.0128173828125\n",
      "Epoch [201/2000], L_train: 61.767906188964844, L_test: 64.90042114257812\n",
      "Epoch [301/2000], L_train: 59.30134963989258, L_test: 67.75723266601562\n",
      "Epoch [401/2000], L_train: 68.02918243408203, L_test: 64.75066375732422\n",
      "Epoch [501/2000], L_train: 65.3761978149414, L_test: 64.79666137695312\n",
      "Epoch [601/2000], L_train: 63.57061767578125, L_test: 65.88426971435547\n",
      "Epoch [701/2000], L_train: 60.81486892700195, L_test: 68.50220489501953\n",
      "Epoch [801/2000], L_train: 55.372615814208984, L_test: 77.04994201660156\n",
      "Epoch [901/2000], L_train: 59.201602935791016, L_test: 62.32212829589844\n",
      "Epoch [1001/2000], L_train: 55.09166717529297, L_test: 64.22542572021484\n",
      "Epoch [1101/2000], L_train: 42.5123176574707, L_test: 50.457801818847656\n",
      "Epoch [1201/2000], L_train: 30.447418212890625, L_test: 30.13910675048828\n",
      "Epoch [1301/2000], L_train: 50.7739143371582, L_test: 25.803956985473633\n",
      "Epoch [1401/2000], L_train: 34.29875564575195, L_test: 41.41167068481445\n",
      "Epoch [1501/2000], L_train: 35.0052604675293, L_test: 41.541900634765625\n",
      "Epoch [1601/2000], L_train: 32.98154067993164, L_test: 41.529903411865234\n",
      "Epoch [1701/2000], L_train: 36.41303634643555, L_test: 41.57213592529297\n",
      "Epoch [1801/2000], L_train: 36.190555572509766, L_test: 41.585357666015625\n",
      "Epoch [1901/2000], L_train: 33.807979583740234, L_test: 41.57867431640625\n",
      "Testing with LR = 0.001 and Weight Decay = 1e-05\n",
      "Epoch [1/2000], L_train: 43.56533432006836, L_test: 302.455322265625\n",
      "Epoch [101/2000], L_train: 66.1797866821289, L_test: 59.55242919921875\n",
      "Epoch [201/2000], L_train: 62.9653434753418, L_test: 67.35140991210938\n",
      "Epoch [301/2000], L_train: 58.79396057128906, L_test: 46.103885650634766\n",
      "Epoch [401/2000], L_train: 45.94810485839844, L_test: 50.615074157714844\n",
      "Epoch [501/2000], L_train: 31.099700927734375, L_test: 37.46171188354492\n",
      "Epoch [601/2000], L_train: 64.5111312866211, L_test: 71.99728393554688\n",
      "Epoch [701/2000], L_train: 21.705890655517578, L_test: 46.19410705566406\n",
      "Epoch [801/2000], L_train: 62.27082061767578, L_test: 73.98745727539062\n",
      "Epoch [901/2000], L_train: 57.70455551147461, L_test: 63.23054885864258\n",
      "Epoch [1001/2000], L_train: 60.62203598022461, L_test: 64.08617401123047\n",
      "Epoch [1101/2000], L_train: 60.3647575378418, L_test: 63.55173110961914\n",
      "Epoch [1201/2000], L_train: 60.62681198120117, L_test: 63.39664840698242\n",
      "Epoch [1301/2000], L_train: 56.586029052734375, L_test: 63.52760314941406\n",
      "Epoch [1401/2000], L_train: 60.839969635009766, L_test: 68.60011291503906\n",
      "Epoch [1501/2000], L_train: 57.942928314208984, L_test: 69.9685287475586\n",
      "Epoch [1601/2000], L_train: 62.853187561035156, L_test: 63.125999450683594\n",
      "Epoch [1701/2000], L_train: 57.30555725097656, L_test: 66.23240661621094\n",
      "Epoch [1801/2000], L_train: 58.539466857910156, L_test: 63.676883697509766\n",
      "Epoch [1901/2000], L_train: 60.40171813964844, L_test: 66.9404067993164\n",
      "Best Learning Rate: 0.001, Best Weight Decay: 0.0001, Lowest L_test: 23.691694259643555\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-5, 1e-3]  # Possible learning rates\n",
    "weight_decays = [1e-2, 1e-3, 1e-4, 1e-5]  # Possible weight decay values\n",
    "n_observations = len(state[0])\n",
    "epochs=2000\n",
    "\n",
    "best_lr = None\n",
    "best_weight_decay = None\n",
    "lowest_test_loss = float('inf')  # Initialize with infinity to ensure any first result is better\n",
    "\n",
    "# value_net is your prediction function, take state as input and output is the prediction price\n",
    "value_net = NN(n_observations).to(device)\n",
    "\n",
    "for LR in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        print(f\"Testing with LR = {LR} and Weight Decay = {wd}\")\n",
    "        # 'optimize'  is an easy package to do Gredient decent, 'Adam' is a method to let learing rate decay as step go.\n",
    "        optimizer = optim.Adam(value_net.parameters(), lr=LR,weight_decay=wd)\n",
    "    \n",
    "        # Run the training loop for this combination of parameters\n",
    "        for epoch in range(epochs):\n",
    "            l_train, l_test = optimize_model()  # Use the current lr and wd in your optimization\n",
    "            if epoch % 100 == 0:  # Report every 100 epochs\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], L_train: {l_train}, L_test: {l_test}')\n",
    "\n",
    "            # Save parameters if this is the best we've seen\n",
    "            if l_test < lowest_test_loss:\n",
    "                lowest_test_loss = l_test\n",
    "                best_lr = LR\n",
    "                best_weight_decay = wd\n",
    "\n",
    "# Output the best parameters and the test loss achieved with them\n",
    "print(f\"Best Learning Rate: {best_lr}, Best Weight Decay: {best_weight_decay}, Lowest L_test: {lowest_test_loss}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100000], L_train: 59.46142578125,L_test: 66.21602630615234\n",
      "Epoch [11/100000], L_train: 59.95954132080078,L_test: 72.85272216796875\n",
      "Epoch [21/100000], L_train: 52.18948745727539,L_test: 72.85000610351562\n",
      "Epoch [31/100000], L_train: 56.47296905517578,L_test: 66.0677719116211\n",
      "Epoch [41/100000], L_train: 50.293174743652344,L_test: 65.43447875976562\n",
      "Epoch [51/100000], L_train: 67.69593811035156,L_test: 66.10823822021484\n",
      "Epoch [61/100000], L_train: 59.5383415222168,L_test: 62.41083908081055\n",
      "Epoch [71/100000], L_train: 56.66339111328125,L_test: 63.117000579833984\n",
      "Epoch [81/100000], L_train: 57.51463317871094,L_test: 65.15682983398438\n",
      "Epoch [91/100000], L_train: 57.65017318725586,L_test: 62.998138427734375\n",
      "Epoch [101/100000], L_train: 55.419715881347656,L_test: 64.20887756347656\n",
      "Epoch [111/100000], L_train: 60.46889877319336,L_test: 67.38843536376953\n",
      "Epoch [121/100000], L_train: 57.39326477050781,L_test: 62.74835968017578\n",
      "Epoch [131/100000], L_train: 52.41047286987305,L_test: 62.822059631347656\n",
      "Epoch [141/100000], L_train: 50.320804595947266,L_test: 64.65823364257812\n",
      "Epoch [151/100000], L_train: 61.938602447509766,L_test: 65.25149536132812\n",
      "Epoch [161/100000], L_train: 54.33507537841797,L_test: 63.80451583862305\n",
      "Epoch [171/100000], L_train: 58.703739166259766,L_test: 72.12154388427734\n",
      "Epoch [181/100000], L_train: 59.709659576416016,L_test: 72.16248321533203\n",
      "Epoch [191/100000], L_train: 52.330299377441406,L_test: 64.65141296386719\n",
      "Epoch [201/100000], L_train: 57.36162185668945,L_test: 60.89948272705078\n",
      "Epoch [211/100000], L_train: 54.47834014892578,L_test: 65.7276382446289\n",
      "Epoch [221/100000], L_train: 50.63897705078125,L_test: 65.06531524658203\n",
      "Epoch [231/100000], L_train: 49.52609634399414,L_test: 63.614105224609375\n",
      "Epoch [241/100000], L_train: 58.31776809692383,L_test: 60.43940353393555\n",
      "Epoch [251/100000], L_train: 55.53841018676758,L_test: 61.33477783203125\n",
      "Epoch [261/100000], L_train: 49.83269119262695,L_test: 66.78257751464844\n",
      "Epoch [271/100000], L_train: 55.23089599609375,L_test: 65.81675720214844\n",
      "Epoch [281/100000], L_train: 52.61974334716797,L_test: 68.91112518310547\n",
      "Epoch [291/100000], L_train: 51.94811248779297,L_test: 56.093563079833984\n",
      "Epoch [301/100000], L_train: 58.3007698059082,L_test: 52.61241149902344\n",
      "Epoch [311/100000], L_train: 49.138511657714844,L_test: 66.90570831298828\n",
      "Epoch [321/100000], L_train: 50.943580627441406,L_test: 61.17578125\n",
      "Epoch [331/100000], L_train: 59.48529052734375,L_test: 52.86598205566406\n",
      "Epoch [341/100000], L_train: 40.11048889160156,L_test: 51.719783782958984\n",
      "Epoch [351/100000], L_train: 92.20487213134766,L_test: 51.20438003540039\n",
      "Epoch [361/100000], L_train: 48.86215591430664,L_test: 44.374183654785156\n",
      "Epoch [371/100000], L_train: 38.644744873046875,L_test: 49.06295394897461\n",
      "Epoch [381/100000], L_train: 37.4571418762207,L_test: 48.883358001708984\n",
      "Epoch [391/100000], L_train: 31.347436904907227,L_test: 37.48016357421875\n",
      "Epoch [401/100000], L_train: 88.591064453125,L_test: 75.0223388671875\n",
      "Epoch [411/100000], L_train: 46.554893493652344,L_test: 42.3097038269043\n",
      "Epoch [421/100000], L_train: 33.57656478881836,L_test: 39.15855407714844\n",
      "Epoch [431/100000], L_train: 34.134883880615234,L_test: 40.3795166015625\n",
      "Epoch [441/100000], L_train: 34.97462463378906,L_test: 40.85087203979492\n",
      "Epoch [451/100000], L_train: 33.235477447509766,L_test: 41.09135818481445\n",
      "Epoch [461/100000], L_train: 31.5439453125,L_test: 41.24140548706055\n",
      "Epoch [471/100000], L_train: 39.366207122802734,L_test: 41.3231315612793\n",
      "Epoch [481/100000], L_train: 33.37160873413086,L_test: 41.39287185668945\n",
      "Epoch [491/100000], L_train: 33.22310256958008,L_test: 41.42079162597656\n",
      "Epoch [501/100000], L_train: 32.91529846191406,L_test: 41.46492004394531\n",
      "Epoch [511/100000], L_train: 32.507598876953125,L_test: 41.491432189941406\n",
      "Epoch [521/100000], L_train: 35.14419937133789,L_test: 41.51925277709961\n",
      "Epoch [531/100000], L_train: 31.70622444152832,L_test: 41.51168441772461\n",
      "Epoch [541/100000], L_train: 36.11161804199219,L_test: 41.54848861694336\n",
      "Epoch [551/100000], L_train: 36.485145568847656,L_test: 41.540977478027344\n",
      "Epoch [561/100000], L_train: 34.71040725708008,L_test: 41.5599479675293\n",
      "Epoch [571/100000], L_train: 31.687034606933594,L_test: 41.548587799072266\n",
      "Epoch [581/100000], L_train: 36.88135528564453,L_test: 41.57170867919922\n",
      "Epoch [591/100000], L_train: 37.165245056152344,L_test: 41.56191635131836\n",
      "Epoch [601/100000], L_train: 34.16462707519531,L_test: 41.56694793701172\n",
      "Epoch [611/100000], L_train: 34.834102630615234,L_test: 41.5629997253418\n",
      "Epoch [621/100000], L_train: 30.652902603149414,L_test: 41.55278396606445\n",
      "Epoch [631/100000], L_train: 34.80110168457031,L_test: 41.56623458862305\n",
      "Epoch [641/100000], L_train: 37.47444534301758,L_test: 41.5796012878418\n",
      "Epoch [651/100000], L_train: 34.090675354003906,L_test: 41.56756591796875\n",
      "Epoch [661/100000], L_train: 36.01213073730469,L_test: 41.55927276611328\n",
      "Epoch [671/100000], L_train: 33.013671875,L_test: 41.555477142333984\n",
      "Epoch [681/100000], L_train: 37.09747314453125,L_test: 41.57597732543945\n",
      "Epoch [691/100000], L_train: 34.10778045654297,L_test: 41.5700798034668\n",
      "Epoch [701/100000], L_train: 35.45179748535156,L_test: 41.5859260559082\n",
      "Epoch [711/100000], L_train: 32.60122299194336,L_test: 41.57516860961914\n",
      "Epoch [721/100000], L_train: 33.75111389160156,L_test: 41.573787689208984\n",
      "Epoch [731/100000], L_train: 31.943634033203125,L_test: 41.569969177246094\n",
      "Epoch [741/100000], L_train: 36.42677307128906,L_test: 41.576961517333984\n",
      "Epoch [751/100000], L_train: 32.44361877441406,L_test: 41.57599639892578\n",
      "Epoch [761/100000], L_train: 34.54749298095703,L_test: 41.58045959472656\n",
      "Epoch [771/100000], L_train: 33.930057525634766,L_test: 41.56402587890625\n",
      "Epoch [781/100000], L_train: 36.47252655029297,L_test: 41.58812713623047\n",
      "Epoch [791/100000], L_train: 35.288414001464844,L_test: 41.56573486328125\n",
      "Epoch [801/100000], L_train: 34.24150466918945,L_test: 41.553871154785156\n",
      "Epoch [811/100000], L_train: 35.4278450012207,L_test: 41.60884094238281\n",
      "Epoch [821/100000], L_train: 31.778629302978516,L_test: 41.59151077270508\n",
      "Epoch [831/100000], L_train: 34.26702117919922,L_test: 41.59675216674805\n",
      "Epoch [841/100000], L_train: 35.530967712402344,L_test: 41.575286865234375\n",
      "Epoch [851/100000], L_train: 33.253746032714844,L_test: 41.56951904296875\n",
      "Epoch [861/100000], L_train: 33.235679626464844,L_test: 41.584774017333984\n",
      "Epoch [871/100000], L_train: 32.63405990600586,L_test: 41.57438278198242\n",
      "Epoch [881/100000], L_train: 33.686485290527344,L_test: 41.58002471923828\n",
      "Epoch [891/100000], L_train: 33.31187438964844,L_test: 41.560909271240234\n",
      "Epoch [901/100000], L_train: 33.81752014160156,L_test: 41.574928283691406\n",
      "Epoch [911/100000], L_train: 35.40972900390625,L_test: 41.60866165161133\n",
      "Epoch [921/100000], L_train: 36.142791748046875,L_test: 41.56741714477539\n",
      "Epoch [931/100000], L_train: 34.34320068359375,L_test: 41.5876579284668\n",
      "Epoch [941/100000], L_train: 36.09125900268555,L_test: 41.555625915527344\n",
      "Epoch [951/100000], L_train: 31.30217170715332,L_test: 41.566036224365234\n",
      "Epoch [961/100000], L_train: 33.45726013183594,L_test: 41.6077995300293\n",
      "Epoch [971/100000], L_train: 35.352134704589844,L_test: 41.592796325683594\n",
      "Epoch [981/100000], L_train: 34.74268341064453,L_test: 41.59334182739258\n",
      "Epoch [991/100000], L_train: 33.01333236694336,L_test: 41.59606170654297\n",
      "Epoch [1001/100000], L_train: 34.528907775878906,L_test: 41.55950927734375\n",
      "Epoch [1011/100000], L_train: 31.803234100341797,L_test: 41.562164306640625\n",
      "Epoch [1021/100000], L_train: 34.44179153442383,L_test: 41.56017303466797\n",
      "Epoch [1031/100000], L_train: 35.496009826660156,L_test: 41.55876541137695\n",
      "Epoch [1041/100000], L_train: 33.42361068725586,L_test: 41.58888244628906\n",
      "Epoch [1051/100000], L_train: 35.88040542602539,L_test: 41.121463775634766\n",
      "Epoch [1061/100000], L_train: 33.07664489746094,L_test: 41.20206069946289\n",
      "Epoch [1071/100000], L_train: 32.9239387512207,L_test: 41.34653854370117\n",
      "Epoch [1081/100000], L_train: 33.68113327026367,L_test: 41.41572189331055\n",
      "Epoch [1091/100000], L_train: 33.433414459228516,L_test: 41.47450256347656\n",
      "Epoch [1101/100000], L_train: 33.409297943115234,L_test: 41.4869270324707\n",
      "Epoch [1111/100000], L_train: 33.76198959350586,L_test: 41.524742126464844\n",
      "Epoch [1121/100000], L_train: 33.598350524902344,L_test: 41.52318572998047\n",
      "Epoch [1131/100000], L_train: 33.807289123535156,L_test: 41.573638916015625\n",
      "Epoch [1141/100000], L_train: 35.30264663696289,L_test: 41.55928039550781\n",
      "Epoch [1151/100000], L_train: 32.43889236450195,L_test: 41.55250549316406\n",
      "Epoch [1161/100000], L_train: 34.68028259277344,L_test: 41.574684143066406\n",
      "Epoch [1171/100000], L_train: 33.54635238647461,L_test: 41.580345153808594\n",
      "Epoch [1181/100000], L_train: 33.70884704589844,L_test: 41.5488395690918\n",
      "Epoch [1191/100000], L_train: 36.80141830444336,L_test: 41.590415954589844\n",
      "Epoch [1201/100000], L_train: 33.308265686035156,L_test: 41.56326675415039\n",
      "Epoch [1211/100000], L_train: 34.06605911254883,L_test: 41.57326889038086\n",
      "Epoch [1221/100000], L_train: 35.439579010009766,L_test: 41.560550689697266\n",
      "Epoch [1231/100000], L_train: 32.88176345825195,L_test: 41.57110595703125\n",
      "Epoch [1241/100000], L_train: 32.2254753112793,L_test: 41.551780700683594\n",
      "Epoch [1251/100000], L_train: 33.734195709228516,L_test: 41.568973541259766\n",
      "Epoch [1261/100000], L_train: 35.39193344116211,L_test: 41.585731506347656\n",
      "Epoch [1271/100000], L_train: 35.32894515991211,L_test: 41.587890625\n",
      "Epoch [1281/100000], L_train: 34.64028549194336,L_test: 41.58598709106445\n",
      "Epoch [1291/100000], L_train: 33.98969650268555,L_test: 41.58111572265625\n",
      "Epoch [1301/100000], L_train: 31.691795349121094,L_test: 41.586517333984375\n",
      "Epoch [1311/100000], L_train: 32.407596588134766,L_test: 41.58755111694336\n",
      "Epoch [1321/100000], L_train: 34.32805633544922,L_test: 41.58491134643555\n",
      "Epoch [1331/100000], L_train: 32.290958404541016,L_test: 41.591854095458984\n",
      "Epoch [1341/100000], L_train: 38.03436279296875,L_test: 41.57618713378906\n",
      "Epoch [1351/100000], L_train: 34.93254089355469,L_test: 41.55754470825195\n",
      "Epoch [1361/100000], L_train: 36.225006103515625,L_test: 41.55902099609375\n",
      "Epoch [1371/100000], L_train: 32.86079025268555,L_test: 41.540977478027344\n",
      "Epoch [1381/100000], L_train: 36.513511657714844,L_test: 41.56662368774414\n",
      "Epoch [1391/100000], L_train: 33.04294967651367,L_test: 41.56462478637695\n",
      "Epoch [1401/100000], L_train: 34.157203674316406,L_test: 41.563148498535156\n",
      "Epoch [1411/100000], L_train: 33.786216735839844,L_test: 41.566776275634766\n",
      "Epoch [1421/100000], L_train: 32.56060028076172,L_test: 41.55813217163086\n",
      "Epoch [1431/100000], L_train: 35.15561294555664,L_test: 41.561641693115234\n",
      "Epoch [1441/100000], L_train: 33.69697952270508,L_test: 41.57435607910156\n",
      "Epoch [1451/100000], L_train: 32.60236358642578,L_test: 41.58203125\n",
      "Epoch [1461/100000], L_train: 33.62193298339844,L_test: 41.60222244262695\n",
      "Epoch [1471/100000], L_train: 33.732547760009766,L_test: 41.58928680419922\n",
      "Epoch [1481/100000], L_train: 34.00889587402344,L_test: 41.559932708740234\n",
      "Epoch [1491/100000], L_train: 34.32599639892578,L_test: 41.56708908081055\n",
      "Epoch [1501/100000], L_train: 33.47694396972656,L_test: 41.578792572021484\n",
      "Epoch [1511/100000], L_train: 31.107078552246094,L_test: 41.570030212402344\n",
      "Epoch [1521/100000], L_train: 34.200538635253906,L_test: 41.58712387084961\n",
      "Epoch [1531/100000], L_train: 33.24777603149414,L_test: 41.56504440307617\n",
      "Epoch [1541/100000], L_train: 33.30104064941406,L_test: 41.56631088256836\n",
      "Epoch [1551/100000], L_train: 36.27595138549805,L_test: 41.55813217163086\n",
      "Epoch [1561/100000], L_train: 31.899192810058594,L_test: 41.57232666015625\n",
      "Epoch [1571/100000], L_train: 33.829994201660156,L_test: 41.58399963378906\n",
      "Epoch [1581/100000], L_train: 31.639196395874023,L_test: 41.561241149902344\n",
      "Epoch [1591/100000], L_train: 33.1756591796875,L_test: 41.55193328857422\n",
      "Epoch [1601/100000], L_train: 34.67601776123047,L_test: 41.572940826416016\n",
      "Epoch [1611/100000], L_train: 185.7330780029297,L_test: 182.04957580566406\n",
      "Epoch [1621/100000], L_train: 184.49623107910156,L_test: 181.88197326660156\n",
      "Epoch [1631/100000], L_train: 193.190673828125,L_test: 181.7603759765625\n",
      "Epoch [1641/100000], L_train: 195.92005920410156,L_test: 187.526611328125\n",
      "Epoch [1651/100000], L_train: 201.09420776367188,L_test: 187.38706970214844\n",
      "Epoch [1661/100000], L_train: 199.0814208984375,L_test: 187.25978088378906\n",
      "Epoch [1671/100000], L_train: 199.666015625,L_test: 187.1414031982422\n",
      "Epoch [1681/100000], L_train: 60.54603958129883,L_test: 63.70807647705078\n",
      "Epoch [1691/100000], L_train: 66.29644775390625,L_test: 83.65144348144531\n",
      "Epoch [1701/100000], L_train: 54.391578674316406,L_test: 62.69008255004883\n",
      "Epoch [1711/100000], L_train: 56.80881118774414,L_test: 67.12482452392578\n",
      "Epoch [1721/100000], L_train: 69.7484359741211,L_test: 64.44853973388672\n",
      "Epoch [1731/100000], L_train: 58.614830017089844,L_test: 62.66940689086914\n",
      "Epoch [1741/100000], L_train: 54.71403884887695,L_test: 69.41481018066406\n",
      "Epoch [1751/100000], L_train: 66.77544403076172,L_test: 65.40547180175781\n",
      "Epoch [1761/100000], L_train: 63.17082977294922,L_test: 73.3452377319336\n",
      "Epoch [1771/100000], L_train: 57.631431579589844,L_test: 67.08921813964844\n",
      "Epoch [1781/100000], L_train: 55.382789611816406,L_test: 64.54582214355469\n",
      "Epoch [1791/100000], L_train: 58.28987503051758,L_test: 66.6226806640625\n",
      "Epoch [1801/100000], L_train: 58.497535705566406,L_test: 62.65277099609375\n",
      "Epoch [1811/100000], L_train: 125.23744201660156,L_test: 141.0109100341797\n",
      "Epoch [1821/100000], L_train: 54.00160598754883,L_test: 62.839115142822266\n",
      "Epoch [1831/100000], L_train: 59.491580963134766,L_test: 62.55755615234375\n",
      "Epoch [1841/100000], L_train: 67.51177215576172,L_test: 63.25201416015625\n",
      "Epoch [1851/100000], L_train: 60.005245208740234,L_test: 63.66788864135742\n",
      "Epoch [1861/100000], L_train: 59.726768493652344,L_test: 63.25355529785156\n",
      "Epoch [1871/100000], L_train: 58.83763122558594,L_test: 66.38218688964844\n",
      "Epoch [1881/100000], L_train: 59.303836822509766,L_test: 65.32583618164062\n",
      "Epoch [1891/100000], L_train: 49.873043060302734,L_test: 62.85239791870117\n",
      "Epoch [1901/100000], L_train: 57.5754508972168,L_test: 62.82029724121094\n",
      "Epoch [1911/100000], L_train: 60.15017318725586,L_test: 63.904048919677734\n",
      "Epoch [1921/100000], L_train: 60.19401168823242,L_test: 64.2284927368164\n",
      "Epoch [1931/100000], L_train: 61.22260665893555,L_test: 80.70560455322266\n",
      "Epoch [1941/100000], L_train: 56.56116485595703,L_test: 64.2061538696289\n",
      "Epoch [1951/100000], L_train: 61.59977340698242,L_test: 63.55250549316406\n",
      "Epoch [1961/100000], L_train: 58.59980773925781,L_test: 62.81539535522461\n",
      "Epoch [1971/100000], L_train: 53.417945861816406,L_test: 66.07303619384766\n",
      "Epoch [1981/100000], L_train: 74.86238861083984,L_test: 80.38520050048828\n",
      "Epoch [1991/100000], L_train: 56.09626007080078,L_test: 66.9212646484375\n",
      "Epoch [2001/100000], L_train: 62.77373123168945,L_test: 62.82194137573242\n",
      "Epoch [2011/100000], L_train: 60.19105911254883,L_test: 70.32928466796875\n",
      "Epoch [2021/100000], L_train: 60.46894836425781,L_test: 62.49046325683594\n",
      "Epoch [2031/100000], L_train: 59.68159103393555,L_test: 62.999393463134766\n",
      "Epoch [2041/100000], L_train: 55.933616638183594,L_test: 66.99593353271484\n",
      "Epoch [2051/100000], L_train: 54.189334869384766,L_test: 65.8731460571289\n",
      "Epoch [2061/100000], L_train: 56.74081802368164,L_test: 70.16625213623047\n",
      "Epoch [2071/100000], L_train: 58.68844223022461,L_test: 62.75178909301758\n",
      "Epoch [2081/100000], L_train: 62.453975677490234,L_test: 62.550235748291016\n",
      "Epoch [2091/100000], L_train: 61.759761810302734,L_test: 63.89471435546875\n",
      "Epoch [2101/100000], L_train: 58.94076919555664,L_test: 62.422611236572266\n",
      "Epoch [2111/100000], L_train: 55.97065353393555,L_test: 65.21521759033203\n",
      "Epoch [2121/100000], L_train: 57.191688537597656,L_test: 62.88382339477539\n",
      "Epoch [2131/100000], L_train: 58.48653793334961,L_test: 66.74063873291016\n",
      "Epoch [2141/100000], L_train: 57.544559478759766,L_test: 63.354976654052734\n",
      "Epoch [2151/100000], L_train: 60.61363983154297,L_test: 65.72319030761719\n",
      "Epoch [2161/100000], L_train: 3101.21875,L_test: 96.91259765625\n",
      "Epoch [2171/100000], L_train: 52.00356674194336,L_test: 67.4505386352539\n",
      "Epoch [2181/100000], L_train: 61.06354904174805,L_test: 64.79551696777344\n",
      "Epoch [2191/100000], L_train: 54.68990707397461,L_test: 70.4312744140625\n",
      "Epoch [2201/100000], L_train: 65.24687194824219,L_test: 76.13357543945312\n",
      "Epoch [2211/100000], L_train: 54.78407669067383,L_test: 64.13166809082031\n",
      "Epoch [2221/100000], L_train: 63.465091705322266,L_test: 65.6060562133789\n",
      "Epoch [2231/100000], L_train: 56.361244201660156,L_test: 62.475433349609375\n",
      "Epoch [2241/100000], L_train: 55.729549407958984,L_test: 63.209716796875\n",
      "Epoch [2251/100000], L_train: 57.805118560791016,L_test: 62.227664947509766\n",
      "Epoch [2261/100000], L_train: 54.4365348815918,L_test: 63.8481559753418\n",
      "Epoch [2271/100000], L_train: 65.562255859375,L_test: 66.21512603759766\n",
      "Epoch [2281/100000], L_train: 57.35194778442383,L_test: 67.12300109863281\n",
      "Epoch [2291/100000], L_train: 60.92987060546875,L_test: 67.15897369384766\n",
      "Epoch [2301/100000], L_train: 58.46162414550781,L_test: 67.4384765625\n",
      "Epoch [2311/100000], L_train: 58.2895393371582,L_test: 63.02369689941406\n",
      "Epoch [2321/100000], L_train: 54.664459228515625,L_test: 66.19448852539062\n",
      "Epoch [2331/100000], L_train: 54.89453125,L_test: 61.87834930419922\n",
      "Epoch [2341/100000], L_train: 54.17636489868164,L_test: 72.16642761230469\n",
      "Epoch [2351/100000], L_train: 54.59446716308594,L_test: 66.09954071044922\n",
      "Epoch [2361/100000], L_train: 57.23994445800781,L_test: 62.58740234375\n",
      "Epoch [2371/100000], L_train: 59.51308059692383,L_test: 76.71630096435547\n",
      "Epoch [2381/100000], L_train: 59.422454833984375,L_test: 63.9731559753418\n",
      "Epoch [2391/100000], L_train: 55.31402587890625,L_test: 62.15586853027344\n",
      "Epoch [2401/100000], L_train: 61.877323150634766,L_test: 66.76294708251953\n",
      "Epoch [2411/100000], L_train: 56.345970153808594,L_test: 65.75446319580078\n",
      "Epoch [2421/100000], L_train: 62.35773849487305,L_test: 71.75192260742188\n",
      "Epoch [2431/100000], L_train: 62.91977310180664,L_test: 62.662235260009766\n",
      "Epoch [2441/100000], L_train: 60.97726821899414,L_test: 62.130672454833984\n",
      "Epoch [2451/100000], L_train: 54.169227600097656,L_test: 63.76087951660156\n",
      "Epoch [2461/100000], L_train: 60.781654357910156,L_test: 63.73577880859375\n",
      "Epoch [2471/100000], L_train: 60.795806884765625,L_test: 66.26069641113281\n",
      "Epoch [2481/100000], L_train: 53.60972595214844,L_test: 64.84131622314453\n",
      "Epoch [2491/100000], L_train: 52.98017501831055,L_test: 61.755279541015625\n",
      "Epoch [2501/100000], L_train: 55.74163818359375,L_test: 64.44833374023438\n",
      "Epoch [2511/100000], L_train: 64.86518096923828,L_test: 61.801673889160156\n",
      "Epoch [2521/100000], L_train: 58.504207611083984,L_test: 79.09736633300781\n",
      "Epoch [2531/100000], L_train: 57.432865142822266,L_test: 61.99791717529297\n",
      "Epoch [2541/100000], L_train: 63.06422424316406,L_test: 67.58622741699219\n",
      "Epoch [2551/100000], L_train: 57.08341598510742,L_test: 66.98434448242188\n",
      "Epoch [2561/100000], L_train: 59.368709564208984,L_test: 75.22090911865234\n",
      "Epoch [2571/100000], L_train: 57.96669387817383,L_test: 62.09622573852539\n",
      "Epoch [2581/100000], L_train: 57.78987503051758,L_test: 68.84551239013672\n",
      "Epoch [2591/100000], L_train: 54.438804626464844,L_test: 66.39166259765625\n",
      "Epoch [2601/100000], L_train: 58.043968200683594,L_test: 68.62584686279297\n",
      "Epoch [2611/100000], L_train: 61.520469665527344,L_test: 66.33108520507812\n",
      "Epoch [2621/100000], L_train: 54.47880935668945,L_test: 71.40705108642578\n",
      "Epoch [2631/100000], L_train: 50.81855392456055,L_test: 62.51818084716797\n",
      "Epoch [2641/100000], L_train: 57.75787353515625,L_test: 67.74762725830078\n",
      "Epoch [2651/100000], L_train: 53.35662078857422,L_test: 71.7977066040039\n",
      "Epoch [2661/100000], L_train: 53.70830154418945,L_test: 62.364864349365234\n",
      "Epoch [2671/100000], L_train: 57.33662414550781,L_test: 62.60517501831055\n",
      "Epoch [2681/100000], L_train: 58.75348663330078,L_test: 63.316593170166016\n",
      "Epoch [2691/100000], L_train: 61.20106887817383,L_test: 63.56173324584961\n",
      "Epoch [2701/100000], L_train: 62.09894943237305,L_test: 62.37737274169922\n",
      "Epoch [2711/100000], L_train: 61.240570068359375,L_test: 66.23776245117188\n",
      "Epoch [2721/100000], L_train: 57.08578872680664,L_test: 63.4614372253418\n",
      "Epoch [2731/100000], L_train: 54.06001281738281,L_test: 67.1660385131836\n",
      "Epoch [2741/100000], L_train: 56.204566955566406,L_test: 61.89244079589844\n",
      "Epoch [2751/100000], L_train: 64.9354476928711,L_test: 66.96356964111328\n",
      "Epoch [2761/100000], L_train: 56.83599853515625,L_test: 62.045223236083984\n",
      "Epoch [2771/100000], L_train: 56.65481948852539,L_test: 63.225704193115234\n",
      "Epoch [2781/100000], L_train: 60.061729431152344,L_test: 63.00987243652344\n",
      "Epoch [2791/100000], L_train: 59.054222106933594,L_test: 62.046539306640625\n",
      "Epoch [2801/100000], L_train: 54.151363372802734,L_test: 63.687294006347656\n",
      "Epoch [2811/100000], L_train: 56.32034683227539,L_test: 61.965084075927734\n",
      "Epoch [2821/100000], L_train: 55.462493896484375,L_test: 68.2567367553711\n",
      "Epoch [2831/100000], L_train: 57.59383010864258,L_test: 63.38022232055664\n",
      "Epoch [2841/100000], L_train: 59.61482620239258,L_test: 64.59107208251953\n",
      "Epoch [2851/100000], L_train: 60.16923904418945,L_test: 61.37086486816406\n",
      "Epoch [2861/100000], L_train: 58.81863021850586,L_test: 67.71321105957031\n",
      "Epoch [2871/100000], L_train: 61.535194396972656,L_test: 61.78180694580078\n",
      "Epoch [2881/100000], L_train: 53.310672760009766,L_test: 64.61051940917969\n",
      "Epoch [2891/100000], L_train: 52.73265075683594,L_test: 61.5478630065918\n",
      "Epoch [2901/100000], L_train: 55.3315544128418,L_test: 61.017333984375\n",
      "Epoch [2911/100000], L_train: 60.66908645629883,L_test: 66.2704849243164\n",
      "Epoch [2921/100000], L_train: 58.56068801879883,L_test: 64.92173767089844\n",
      "Epoch [2931/100000], L_train: 53.89411163330078,L_test: 61.09086608886719\n",
      "Epoch [2941/100000], L_train: 54.6858024597168,L_test: 66.22295379638672\n",
      "Epoch [2951/100000], L_train: 62.621219635009766,L_test: 63.12568664550781\n",
      "Epoch [2961/100000], L_train: 57.390220642089844,L_test: 62.395023345947266\n",
      "Epoch [2971/100000], L_train: 55.14997100830078,L_test: 62.119140625\n",
      "Epoch [2981/100000], L_train: 53.8532829284668,L_test: 66.27214050292969\n",
      "Epoch [2991/100000], L_train: 55.051212310791016,L_test: 67.1234359741211\n",
      "Epoch [3001/100000], L_train: 53.67770004272461,L_test: 65.50386047363281\n",
      "Epoch [3011/100000], L_train: 56.843109130859375,L_test: 61.63252258300781\n",
      "Epoch [3021/100000], L_train: 55.73940658569336,L_test: 60.975914001464844\n",
      "Epoch [3031/100000], L_train: 56.432655334472656,L_test: 64.1136703491211\n",
      "Epoch [3041/100000], L_train: 55.42673110961914,L_test: 60.81163787841797\n",
      "Epoch [3051/100000], L_train: 60.97373962402344,L_test: 63.97220230102539\n",
      "Epoch [3061/100000], L_train: 50.88131332397461,L_test: 65.85639190673828\n",
      "Epoch [3071/100000], L_train: 52.275516510009766,L_test: 64.97773742675781\n",
      "Epoch [3081/100000], L_train: 59.595703125,L_test: 60.94781494140625\n",
      "Epoch [3091/100000], L_train: 54.09690856933594,L_test: 62.675235748291016\n",
      "Epoch [3101/100000], L_train: 54.751895904541016,L_test: 61.986305236816406\n",
      "Epoch [3111/100000], L_train: 51.845787048339844,L_test: 63.748291015625\n",
      "Epoch [3121/100000], L_train: 50.917232513427734,L_test: 62.351654052734375\n",
      "Epoch [3131/100000], L_train: 60.95695114135742,L_test: 62.57680130004883\n",
      "Epoch [3141/100000], L_train: 56.80073547363281,L_test: 62.014286041259766\n",
      "Epoch [3151/100000], L_train: 57.55242156982422,L_test: 61.66712951660156\n",
      "Epoch [3161/100000], L_train: 58.282833099365234,L_test: 65.39095306396484\n",
      "Epoch [3171/100000], L_train: 60.389068603515625,L_test: 73.0360107421875\n",
      "Epoch [3181/100000], L_train: 57.68141174316406,L_test: 61.38821029663086\n",
      "Epoch [3191/100000], L_train: 49.263484954833984,L_test: 62.78306579589844\n",
      "Epoch [3201/100000], L_train: 57.782962799072266,L_test: 60.21520233154297\n",
      "Epoch [3211/100000], L_train: 53.67169952392578,L_test: 67.85363006591797\n",
      "Epoch [3221/100000], L_train: 54.32358932495117,L_test: 61.79387283325195\n",
      "Epoch [3231/100000], L_train: 58.8828010559082,L_test: 64.02171325683594\n",
      "Epoch [3241/100000], L_train: 54.09526062011719,L_test: 61.272132873535156\n",
      "Epoch [3251/100000], L_train: 59.768646240234375,L_test: 61.08027267456055\n",
      "Epoch [3261/100000], L_train: 55.261985778808594,L_test: 70.80182647705078\n",
      "Epoch [3271/100000], L_train: 52.437339782714844,L_test: 61.35833740234375\n",
      "Epoch [3281/100000], L_train: 54.681640625,L_test: 60.275638580322266\n",
      "Epoch [3291/100000], L_train: 56.11827087402344,L_test: 60.1552848815918\n",
      "Epoch [3301/100000], L_train: 56.62551498413086,L_test: 60.469764709472656\n",
      "Epoch [3311/100000], L_train: 57.55828857421875,L_test: 59.19903564453125\n",
      "Epoch [3321/100000], L_train: 53.49189758300781,L_test: 60.211856842041016\n",
      "Epoch [3331/100000], L_train: 55.12751007080078,L_test: 60.77475357055664\n",
      "Epoch [3341/100000], L_train: 47.16297149658203,L_test: 63.28812026977539\n",
      "Epoch [3351/100000], L_train: 52.660404205322266,L_test: 60.00574493408203\n",
      "Epoch [3361/100000], L_train: 47.60232162475586,L_test: 58.98784255981445\n",
      "Epoch [3371/100000], L_train: 52.68909454345703,L_test: 63.59065628051758\n",
      "Epoch [3381/100000], L_train: 49.261287689208984,L_test: 59.977046966552734\n",
      "Epoch [3391/100000], L_train: 45.81060028076172,L_test: 55.03485107421875\n",
      "Epoch [3401/100000], L_train: 53.16361999511719,L_test: 57.762996673583984\n",
      "Epoch [3411/100000], L_train: 53.05942153930664,L_test: 61.317649841308594\n",
      "Epoch [3421/100000], L_train: 51.96628189086914,L_test: 62.23432540893555\n",
      "Epoch [3431/100000], L_train: 52.037376403808594,L_test: 57.58686447143555\n",
      "Epoch [3441/100000], L_train: 49.7973747253418,L_test: 60.95621871948242\n",
      "Epoch [3451/100000], L_train: 44.34768295288086,L_test: 57.157806396484375\n",
      "Epoch [3461/100000], L_train: 51.60379409790039,L_test: 52.638587951660156\n",
      "Epoch [3471/100000], L_train: 44.53232192993164,L_test: 49.924434661865234\n",
      "Epoch [3481/100000], L_train: 64.17984771728516,L_test: 75.51950073242188\n",
      "Epoch [3491/100000], L_train: 46.78307342529297,L_test: 52.66926956176758\n",
      "Epoch [3501/100000], L_train: 45.099117279052734,L_test: 48.10348892211914\n",
      "Epoch [3511/100000], L_train: 38.541324615478516,L_test: 38.78932571411133\n",
      "Epoch [3521/100000], L_train: 33.90815734863281,L_test: 41.40421676635742\n",
      "Epoch [3531/100000], L_train: 31.559541702270508,L_test: 41.77458953857422\n",
      "Epoch [3541/100000], L_train: 34.663551330566406,L_test: 41.744258880615234\n",
      "Epoch [3551/100000], L_train: 31.163171768188477,L_test: 41.62080001831055\n",
      "Epoch [3561/100000], L_train: 33.397552490234375,L_test: 41.59601974487305\n",
      "Epoch [3571/100000], L_train: 34.54092025756836,L_test: 41.577484130859375\n",
      "Epoch [3581/100000], L_train: 32.68130111694336,L_test: 41.589317321777344\n",
      "Epoch [3591/100000], L_train: 34.63070297241211,L_test: 41.57332229614258\n",
      "Epoch [3601/100000], L_train: 37.729984283447266,L_test: 41.589473724365234\n",
      "Epoch [3611/100000], L_train: 33.927433013916016,L_test: 41.57880783081055\n",
      "Epoch [3621/100000], L_train: 34.334999084472656,L_test: 41.573448181152344\n",
      "Epoch [3631/100000], L_train: 34.26072692871094,L_test: 41.601619720458984\n",
      "Epoch [3641/100000], L_train: 33.06818389892578,L_test: 41.037330627441406\n",
      "Epoch [3651/100000], L_train: 31.838886260986328,L_test: 41.19719696044922\n",
      "Epoch [3661/100000], L_train: 33.644134521484375,L_test: 41.35564041137695\n",
      "Epoch [3671/100000], L_train: 34.99932098388672,L_test: 41.430686950683594\n",
      "Epoch [3681/100000], L_train: 34.09960174560547,L_test: 41.47713088989258\n",
      "Epoch [3691/100000], L_train: 32.16366958618164,L_test: 41.300376892089844\n",
      "Epoch [3701/100000], L_train: 36.01710510253906,L_test: 41.40800857543945\n",
      "Epoch [3711/100000], L_train: 34.466487884521484,L_test: 41.4467658996582\n",
      "Epoch [3721/100000], L_train: 33.0664176940918,L_test: 41.48074722290039\n",
      "Epoch [3731/100000], L_train: 34.082401275634766,L_test: 41.51174545288086\n",
      "Epoch [3741/100000], L_train: 34.637901306152344,L_test: 41.537689208984375\n",
      "Epoch [3751/100000], L_train: 34.87397766113281,L_test: 41.55217742919922\n",
      "Epoch [3761/100000], L_train: 33.02153015136719,L_test: 41.549842834472656\n",
      "Epoch [3771/100000], L_train: 32.57078170776367,L_test: 41.568267822265625\n",
      "Epoch [3781/100000], L_train: 30.402517318725586,L_test: 41.563514709472656\n",
      "Epoch [3791/100000], L_train: 34.20698165893555,L_test: 41.585445404052734\n",
      "Epoch [3801/100000], L_train: 34.52839279174805,L_test: 41.59993362426758\n",
      "Epoch [3811/100000], L_train: 34.4749641418457,L_test: 41.59013366699219\n",
      "Epoch [3821/100000], L_train: 34.48353576660156,L_test: 41.576290130615234\n",
      "Epoch [3831/100000], L_train: 31.534168243408203,L_test: 41.58251953125\n",
      "Epoch [3841/100000], L_train: 36.517112731933594,L_test: 41.56889724731445\n",
      "Epoch [3851/100000], L_train: 36.48995590209961,L_test: 41.557899475097656\n",
      "Epoch [3861/100000], L_train: 33.60891342163086,L_test: 41.53554916381836\n",
      "Epoch [3871/100000], L_train: 32.61348342895508,L_test: 41.55348205566406\n",
      "Epoch [3881/100000], L_train: 34.07072067260742,L_test: 41.564205169677734\n",
      "Epoch [3891/100000], L_train: 33.614105224609375,L_test: 41.571205139160156\n",
      "Epoch [3901/100000], L_train: 33.20656967163086,L_test: 41.57666015625\n",
      "Epoch [3911/100000], L_train: 32.27259063720703,L_test: 41.58835220336914\n",
      "Epoch [3921/100000], L_train: 31.796648025512695,L_test: 41.59172821044922\n",
      "Epoch [3931/100000], L_train: 34.46587371826172,L_test: 41.585628509521484\n",
      "Epoch [3941/100000], L_train: 34.657840728759766,L_test: 41.58671569824219\n",
      "Epoch [3951/100000], L_train: 36.26120376586914,L_test: 41.595821380615234\n",
      "Epoch [3961/100000], L_train: 33.4217643737793,L_test: 41.58242416381836\n",
      "Epoch [3971/100000], L_train: 33.16093063354492,L_test: 41.58097457885742\n",
      "Epoch [3981/100000], L_train: 33.480247497558594,L_test: 41.575172424316406\n",
      "Epoch [3991/100000], L_train: 36.15781021118164,L_test: 41.58735656738281\n",
      "Epoch [4001/100000], L_train: 32.08889389038086,L_test: 41.56974411010742\n",
      "Epoch [4011/100000], L_train: 37.47175216674805,L_test: 41.56525802612305\n",
      "Epoch [4021/100000], L_train: 36.60158157348633,L_test: 41.599422454833984\n",
      "Epoch [4031/100000], L_train: 33.82962417602539,L_test: 41.58473205566406\n",
      "Epoch [4041/100000], L_train: 34.248897552490234,L_test: 41.57819747924805\n",
      "Epoch [4051/100000], L_train: 31.904064178466797,L_test: 41.56857681274414\n",
      "Epoch [4061/100000], L_train: 34.817588806152344,L_test: 41.58805465698242\n",
      "Epoch [4071/100000], L_train: 32.73475646972656,L_test: 41.563350677490234\n",
      "Epoch [4081/100000], L_train: 32.9417839050293,L_test: 41.58024978637695\n",
      "Epoch [4091/100000], L_train: 35.0837287902832,L_test: 41.570701599121094\n",
      "Epoch [4101/100000], L_train: 34.93958282470703,L_test: 41.575950622558594\n",
      "Epoch [4111/100000], L_train: 34.77177810668945,L_test: 41.5733642578125\n",
      "Epoch [4121/100000], L_train: 32.00603103637695,L_test: 41.583946228027344\n",
      "Epoch [4131/100000], L_train: 57.888099670410156,L_test: 89.88495635986328\n",
      "Epoch [4141/100000], L_train: 55.01230239868164,L_test: 64.82396697998047\n",
      "Epoch [4151/100000], L_train: 60.733795166015625,L_test: 60.4050407409668\n",
      "Epoch [4161/100000], L_train: 54.16096878051758,L_test: 65.04727172851562\n",
      "Epoch [4171/100000], L_train: 55.12144470214844,L_test: 67.62136840820312\n",
      "Epoch [4181/100000], L_train: 64.34149169921875,L_test: 60.521846771240234\n",
      "Epoch [4191/100000], L_train: 54.960079193115234,L_test: 60.94127655029297\n",
      "Epoch [4201/100000], L_train: 56.87667465209961,L_test: 64.47071075439453\n",
      "Epoch [4211/100000], L_train: 56.87583541870117,L_test: 60.874114990234375\n",
      "Epoch [4221/100000], L_train: 60.003883361816406,L_test: 63.7661247253418\n",
      "Epoch [4231/100000], L_train: 58.64752960205078,L_test: 63.881065368652344\n",
      "Epoch [4241/100000], L_train: 54.860260009765625,L_test: 60.73622512817383\n",
      "Epoch [4251/100000], L_train: 59.27849578857422,L_test: 65.86542510986328\n",
      "Epoch [4261/100000], L_train: 59.248966217041016,L_test: 66.5357437133789\n",
      "Epoch [4271/100000], L_train: 54.380802154541016,L_test: 64.57850646972656\n",
      "Epoch [4281/100000], L_train: 54.557369232177734,L_test: 62.42950439453125\n",
      "Epoch [4291/100000], L_train: 52.37171173095703,L_test: 61.314449310302734\n",
      "Epoch [4301/100000], L_train: 59.300785064697266,L_test: 64.08920288085938\n",
      "Epoch [4311/100000], L_train: 58.989009857177734,L_test: 65.13896179199219\n",
      "Epoch [4321/100000], L_train: 55.35236740112305,L_test: 65.40597534179688\n",
      "Epoch [4331/100000], L_train: 53.887916564941406,L_test: 65.27269744873047\n",
      "Epoch [4341/100000], L_train: 55.54275131225586,L_test: 62.029136657714844\n",
      "Epoch [4351/100000], L_train: 61.29736328125,L_test: 74.54002380371094\n",
      "Epoch [4361/100000], L_train: 59.392860412597656,L_test: 71.04232025146484\n",
      "Epoch [4371/100000], L_train: 55.817909240722656,L_test: 62.78369140625\n",
      "Epoch [4381/100000], L_train: 51.260799407958984,L_test: 62.646121978759766\n",
      "Epoch [4391/100000], L_train: 64.17799377441406,L_test: 59.23524856567383\n",
      "Epoch [4401/100000], L_train: 53.31011962890625,L_test: 62.126949310302734\n",
      "Epoch [4411/100000], L_train: 54.62933349609375,L_test: 61.57155990600586\n",
      "Epoch [4421/100000], L_train: 57.027320861816406,L_test: 62.058349609375\n",
      "Epoch [4431/100000], L_train: 48.80554962158203,L_test: 63.644168853759766\n",
      "Epoch [4441/100000], L_train: 51.076778411865234,L_test: 60.2666130065918\n",
      "Epoch [4451/100000], L_train: 53.28602600097656,L_test: 63.91661834716797\n",
      "Epoch [4461/100000], L_train: 55.85633087158203,L_test: 59.75698471069336\n",
      "Epoch [4471/100000], L_train: 55.07863998413086,L_test: 59.31483840942383\n",
      "Epoch [4481/100000], L_train: 52.68412780761719,L_test: 61.93241882324219\n",
      "Epoch [4491/100000], L_train: 57.62517166137695,L_test: 61.190242767333984\n",
      "Epoch [4501/100000], L_train: 50.18947982788086,L_test: 60.74818801879883\n",
      "Epoch [4511/100000], L_train: 50.057708740234375,L_test: 59.86468505859375\n",
      "Epoch [4521/100000], L_train: 48.824378967285156,L_test: 60.665924072265625\n",
      "Epoch [4531/100000], L_train: 48.17373275756836,L_test: 56.907352447509766\n",
      "Epoch [4541/100000], L_train: 54.670326232910156,L_test: 58.425514221191406\n",
      "Epoch [4551/100000], L_train: 51.03992462158203,L_test: 58.44729232788086\n",
      "Epoch [4561/100000], L_train: 52.79032897949219,L_test: 54.05405044555664\n",
      "Epoch [4571/100000], L_train: 45.28437042236328,L_test: 61.98590850830078\n",
      "Epoch [4581/100000], L_train: 54.4316520690918,L_test: 57.52013397216797\n",
      "Epoch [4591/100000], L_train: 43.366092681884766,L_test: 52.64855194091797\n",
      "Epoch [4601/100000], L_train: 47.53474807739258,L_test: 54.87117385864258\n",
      "Epoch [4611/100000], L_train: 46.15317153930664,L_test: 51.40166091918945\n",
      "Epoch [4621/100000], L_train: 38.80363845825195,L_test: 54.462440490722656\n",
      "Epoch [4631/100000], L_train: 80.4814224243164,L_test: 48.93060302734375\n",
      "Epoch [4641/100000], L_train: 45.35060119628906,L_test: 40.399803161621094\n",
      "Epoch [4651/100000], L_train: 38.05654525756836,L_test: 43.0833854675293\n",
      "Epoch [4661/100000], L_train: 40.80282974243164,L_test: 44.218868255615234\n",
      "Epoch [4671/100000], L_train: 42.4965934753418,L_test: 52.414608001708984\n",
      "Epoch [4681/100000], L_train: 52.232906341552734,L_test: 56.363319396972656\n",
      "Epoch [4691/100000], L_train: 43.78651428222656,L_test: 38.23789596557617\n",
      "Epoch [4701/100000], L_train: 45.52671432495117,L_test: 57.58317184448242\n",
      "Epoch [4711/100000], L_train: 51.10041046142578,L_test: 54.63849639892578\n",
      "Epoch [4721/100000], L_train: 47.53471374511719,L_test: 31.352643966674805\n",
      "Epoch [4731/100000], L_train: 35.26691818237305,L_test: 37.03779220581055\n",
      "Epoch [4741/100000], L_train: 113.44727325439453,L_test: 103.03522491455078\n",
      "Epoch [4751/100000], L_train: 44.823570251464844,L_test: 54.248817443847656\n",
      "Epoch [4761/100000], L_train: 29.853961944580078,L_test: 31.656326293945312\n",
      "Epoch [4771/100000], L_train: 35.713863372802734,L_test: 41.2784309387207\n",
      "Epoch [4781/100000], L_train: 34.186519622802734,L_test: 41.52800750732422\n",
      "Epoch [4791/100000], L_train: 36.42551040649414,L_test: 41.53911209106445\n",
      "Epoch [4801/100000], L_train: 32.3701286315918,L_test: 41.49244689941406\n",
      "Epoch [4811/100000], L_train: 33.29355239868164,L_test: 41.56962585449219\n",
      "Epoch [4821/100000], L_train: 34.039283752441406,L_test: 41.5823860168457\n",
      "Epoch [4831/100000], L_train: 33.72953796386719,L_test: 41.58437728881836\n",
      "Epoch [4841/100000], L_train: 35.19199752807617,L_test: 41.57432556152344\n",
      "Epoch [4851/100000], L_train: 34.288482666015625,L_test: 41.554283142089844\n",
      "Epoch [4861/100000], L_train: 31.587879180908203,L_test: 41.577030181884766\n",
      "Epoch [4871/100000], L_train: 33.32542419433594,L_test: 41.59965896606445\n",
      "Epoch [4881/100000], L_train: 35.044673919677734,L_test: 41.527320861816406\n",
      "Epoch [4891/100000], L_train: 36.10688781738281,L_test: 41.54429626464844\n",
      "Epoch [4901/100000], L_train: 35.21710968017578,L_test: 41.56855773925781\n",
      "Epoch [4911/100000], L_train: 32.94082260131836,L_test: 181.53961181640625\n",
      "Epoch [4921/100000], L_train: 34.15657424926758,L_test: 41.54413604736328\n",
      "Epoch [4931/100000], L_train: 35.82206726074219,L_test: 41.55808639526367\n",
      "Epoch [4941/100000], L_train: 31.856266021728516,L_test: 41.64243698120117\n",
      "Epoch [4951/100000], L_train: 32.634071350097656,L_test: 41.543663024902344\n",
      "Epoch [4961/100000], L_train: 33.11166763305664,L_test: 41.58423614501953\n",
      "Epoch [4971/100000], L_train: 35.36968231201172,L_test: 41.564998626708984\n",
      "Epoch [4981/100000], L_train: 32.37509536743164,L_test: 41.54244613647461\n",
      "Epoch [4991/100000], L_train: 32.284934997558594,L_test: 41.56126403808594\n",
      "Epoch [5001/100000], L_train: 34.41960906982422,L_test: 41.554203033447266\n",
      "Epoch [5011/100000], L_train: 33.919273376464844,L_test: 41.57041931152344\n",
      "Epoch [5021/100000], L_train: 32.52786636352539,L_test: 41.57166290283203\n",
      "Epoch [5031/100000], L_train: 33.27695083618164,L_test: 41.58924102783203\n",
      "Epoch [5041/100000], L_train: 32.956939697265625,L_test: 41.55799865722656\n",
      "Epoch [5051/100000], L_train: 35.23930740356445,L_test: 41.59224319458008\n",
      "Epoch [5061/100000], L_train: 62.94709396362305,L_test: 59.57322692871094\n",
      "Epoch [5071/100000], L_train: 54.89112854003906,L_test: 60.431732177734375\n",
      "Epoch [5081/100000], L_train: 59.09223556518555,L_test: 60.12287521362305\n",
      "Epoch [5091/100000], L_train: 56.47914123535156,L_test: 63.404788970947266\n",
      "Epoch [5101/100000], L_train: 55.04608154296875,L_test: 63.358421325683594\n",
      "Epoch [5111/100000], L_train: 53.104366302490234,L_test: 60.296295166015625\n",
      "Epoch [5121/100000], L_train: 52.218467712402344,L_test: 62.072261810302734\n",
      "Epoch [5131/100000], L_train: 54.264739990234375,L_test: 60.10760498046875\n",
      "Epoch [5141/100000], L_train: 55.005916595458984,L_test: 62.770545959472656\n",
      "Epoch [5151/100000], L_train: 57.92641830444336,L_test: 60.606529235839844\n",
      "Epoch [5161/100000], L_train: 50.71009063720703,L_test: 68.25997924804688\n",
      "Epoch [5171/100000], L_train: 54.23948669433594,L_test: 62.818885803222656\n",
      "Epoch [5181/100000], L_train: 51.70878982543945,L_test: 64.43931579589844\n",
      "Epoch [5191/100000], L_train: 52.59706497192383,L_test: 60.49049377441406\n",
      "Epoch [5201/100000], L_train: 54.0490608215332,L_test: 64.47854614257812\n",
      "Epoch [5211/100000], L_train: 56.253753662109375,L_test: 61.80096435546875\n",
      "Epoch [5221/100000], L_train: 57.070472717285156,L_test: 62.73019790649414\n",
      "Epoch [5231/100000], L_train: 50.128170013427734,L_test: 61.19757080078125\n",
      "Epoch [5241/100000], L_train: 51.64992904663086,L_test: 60.08966064453125\n",
      "Epoch [5251/100000], L_train: 54.70685958862305,L_test: 63.872718811035156\n",
      "Epoch [5261/100000], L_train: 52.81627655029297,L_test: 62.301490783691406\n",
      "Epoch [5271/100000], L_train: 52.17133712768555,L_test: 60.159332275390625\n",
      "Epoch [5281/100000], L_train: 181.26248168945312,L_test: 175.55271911621094\n",
      "Epoch [5291/100000], L_train: 181.803466796875,L_test: 175.34950256347656\n",
      "Epoch [5301/100000], L_train: 186.1129608154297,L_test: 175.1838836669922\n",
      "Epoch [5311/100000], L_train: 190.08546447753906,L_test: 175.0393524169922\n",
      "Epoch [5321/100000], L_train: 185.08322143554688,L_test: 174.90858459472656\n",
      "Epoch [5331/100000], L_train: 185.5366973876953,L_test: 174.78759765625\n",
      "Epoch [5341/100000], L_train: 181.1502685546875,L_test: 174.6740264892578\n",
      "Epoch [5351/100000], L_train: 181.93450927734375,L_test: 174.5662384033203\n",
      "Epoch [5361/100000], L_train: 183.6833953857422,L_test: 174.46316528320312\n",
      "Epoch [5371/100000], L_train: 183.59034729003906,L_test: 174.36387634277344\n",
      "Epoch [5381/100000], L_train: 188.94830322265625,L_test: 174.26788330078125\n",
      "Epoch [5391/100000], L_train: 183.9357452392578,L_test: 174.1746063232422\n",
      "Epoch [5401/100000], L_train: 180.53302001953125,L_test: 174.0836944580078\n",
      "Epoch [5411/100000], L_train: 187.32632446289062,L_test: 173.99481201171875\n",
      "Epoch [5421/100000], L_train: 181.16595458984375,L_test: 173.90771484375\n",
      "Epoch [5431/100000], L_train: 179.9506072998047,L_test: 173.8221893310547\n",
      "Epoch [5441/100000], L_train: 58.88810348510742,L_test: 59.872013092041016\n",
      "Epoch [5451/100000], L_train: 59.22662353515625,L_test: 58.892974853515625\n",
      "Epoch [5461/100000], L_train: 55.25900650024414,L_test: 58.827972412109375\n",
      "Epoch [5471/100000], L_train: 57.05410385131836,L_test: 58.16778564453125\n",
      "Epoch [5481/100000], L_train: 57.68859100341797,L_test: 60.857330322265625\n",
      "Epoch [5491/100000], L_train: 45.232398986816406,L_test: 62.798038482666016\n",
      "Epoch [5501/100000], L_train: 53.715049743652344,L_test: 74.04826354980469\n",
      "Epoch [5511/100000], L_train: 54.08963394165039,L_test: 59.81829833984375\n",
      "Epoch [5521/100000], L_train: 51.876686096191406,L_test: 58.06943893432617\n",
      "Epoch [5531/100000], L_train: 54.71559143066406,L_test: 60.6811408996582\n",
      "Epoch [5541/100000], L_train: 49.78004455566406,L_test: 56.95299530029297\n",
      "Epoch [5551/100000], L_train: 50.502891540527344,L_test: 62.22589874267578\n",
      "Epoch [5561/100000], L_train: 53.21105194091797,L_test: 71.91132354736328\n",
      "Epoch [5571/100000], L_train: 64.52778625488281,L_test: 57.07804489135742\n",
      "Epoch [5581/100000], L_train: 51.704383850097656,L_test: 58.87136459350586\n",
      "Epoch [5591/100000], L_train: 43.431243896484375,L_test: 50.30868911743164\n",
      "Epoch [5601/100000], L_train: 185.99961853027344,L_test: 173.47695922851562\n",
      "Epoch [5611/100000], L_train: 181.2445068359375,L_test: 173.35662841796875\n",
      "Epoch [5621/100000], L_train: 181.37295532226562,L_test: 173.2434539794922\n",
      "Epoch [5631/100000], L_train: 54.48871994018555,L_test: 59.16053009033203\n",
      "Epoch [5641/100000], L_train: 59.225830078125,L_test: 60.0549201965332\n",
      "Epoch [5651/100000], L_train: 56.017372131347656,L_test: 62.883052825927734\n",
      "Epoch [5661/100000], L_train: 58.78439712524414,L_test: 60.718109130859375\n",
      "Epoch [5671/100000], L_train: 56.69733428955078,L_test: 63.58110046386719\n",
      "Epoch [5681/100000], L_train: 56.68661880493164,L_test: 60.45869064331055\n",
      "Epoch [5691/100000], L_train: 58.5075798034668,L_test: 67.15962982177734\n",
      "Epoch [5701/100000], L_train: 51.022865295410156,L_test: 69.36116790771484\n",
      "Epoch [5711/100000], L_train: 57.560298919677734,L_test: 71.57865905761719\n",
      "Epoch [5721/100000], L_train: 53.85508346557617,L_test: 60.857177734375\n",
      "Epoch [5731/100000], L_train: 64.82008361816406,L_test: 68.43771362304688\n",
      "Epoch [5741/100000], L_train: 50.95952224731445,L_test: 62.20786666870117\n",
      "Epoch [5751/100000], L_train: 51.924888610839844,L_test: 62.933101654052734\n",
      "Epoch [5761/100000], L_train: 52.808231353759766,L_test: 59.61579132080078\n",
      "Epoch [5771/100000], L_train: 53.459720611572266,L_test: 69.08186340332031\n",
      "Epoch [5781/100000], L_train: 54.10263442993164,L_test: 60.66809844970703\n",
      "Epoch [5791/100000], L_train: 56.372352600097656,L_test: 59.792991638183594\n",
      "Epoch [5801/100000], L_train: 56.97716522216797,L_test: 68.2245101928711\n",
      "Epoch [5811/100000], L_train: 51.32406997680664,L_test: 61.05242919921875\n",
      "Epoch [5821/100000], L_train: 51.842750549316406,L_test: 62.27208709716797\n",
      "Epoch [5831/100000], L_train: 52.0073127746582,L_test: 59.24644470214844\n",
      "Epoch [5841/100000], L_train: 53.32542419433594,L_test: 59.439781188964844\n",
      "Epoch [5851/100000], L_train: 53.30703353881836,L_test: 62.52480697631836\n",
      "Epoch [5861/100000], L_train: 59.165279388427734,L_test: 68.8543701171875\n",
      "Epoch [5871/100000], L_train: 47.31137466430664,L_test: 67.05231475830078\n",
      "Epoch [5881/100000], L_train: 48.3010139465332,L_test: 67.47428131103516\n",
      "Epoch [5891/100000], L_train: 49.4289665222168,L_test: 61.215274810791016\n",
      "Epoch [5901/100000], L_train: 56.68429183959961,L_test: 59.99982833862305\n",
      "Epoch [5911/100000], L_train: 48.89442443847656,L_test: 60.44891357421875\n",
      "Epoch [5921/100000], L_train: 52.50408935546875,L_test: 55.31212615966797\n",
      "Epoch [5931/100000], L_train: 49.528926849365234,L_test: 56.8955078125\n",
      "Epoch [5941/100000], L_train: 49.06764221191406,L_test: 57.942588806152344\n",
      "Epoch [5951/100000], L_train: 48.87934494018555,L_test: 72.05094146728516\n",
      "Epoch [5961/100000], L_train: 45.98302459716797,L_test: 58.70319366455078\n",
      "Epoch [5971/100000], L_train: 50.27381134033203,L_test: 57.705474853515625\n",
      "Epoch [5981/100000], L_train: 47.39216232299805,L_test: 58.57807922363281\n",
      "Epoch [5991/100000], L_train: 50.91939926147461,L_test: 60.49612808227539\n",
      "Epoch [6001/100000], L_train: 47.67680358886719,L_test: 54.83332061767578\n",
      "Epoch [6011/100000], L_train: 47.20511245727539,L_test: 50.69363784790039\n",
      "Epoch [6021/100000], L_train: 53.261009216308594,L_test: 56.70420455932617\n",
      "Epoch [6031/100000], L_train: 41.959712982177734,L_test: 55.451499938964844\n",
      "Epoch [6041/100000], L_train: 51.98288345336914,L_test: 51.36439514160156\n",
      "Epoch [6051/100000], L_train: 40.63410568237305,L_test: 66.29100036621094\n",
      "Epoch [6061/100000], L_train: 38.00331497192383,L_test: 61.288814544677734\n",
      "Epoch [6071/100000], L_train: 44.54669952392578,L_test: 74.79782104492188\n",
      "Epoch [6081/100000], L_train: 35.15031051635742,L_test: 47.43802261352539\n",
      "Epoch [6091/100000], L_train: 46.56180191040039,L_test: 38.51118087768555\n",
      "Epoch [6101/100000], L_train: 41.33995819091797,L_test: 47.47479248046875\n",
      "Epoch [6111/100000], L_train: 42.297969818115234,L_test: 66.39924621582031\n",
      "Epoch [6121/100000], L_train: 28.775293350219727,L_test: 57.78364562988281\n",
      "Epoch [6131/100000], L_train: 49.06599807739258,L_test: 32.90631103515625\n",
      "Epoch [6141/100000], L_train: 26.60270881652832,L_test: 38.627159118652344\n",
      "Epoch [6151/100000], L_train: 24.83210563659668,L_test: 69.366943359375\n",
      "Epoch [6161/100000], L_train: 77.03702545166016,L_test: 44.200801849365234\n",
      "Epoch [6171/100000], L_train: 30.7379150390625,L_test: 33.19850158691406\n",
      "Epoch [6181/100000], L_train: 32.81316375732422,L_test: 39.44147872924805\n",
      "Epoch [6191/100000], L_train: 23.617883682250977,L_test: 46.82984924316406\n",
      "Epoch [6201/100000], L_train: 29.99073600769043,L_test: 31.18876838684082\n",
      "Epoch [6211/100000], L_train: 25.142410278320312,L_test: 27.623214721679688\n",
      "Epoch [6221/100000], L_train: 79.38605499267578,L_test: 85.03661346435547\n",
      "Epoch [6231/100000], L_train: 33.93608474731445,L_test: 42.08387756347656\n",
      "Epoch [6241/100000], L_train: 23.742565155029297,L_test: 32.061981201171875\n",
      "Epoch [6251/100000], L_train: 24.615379333496094,L_test: 39.609291076660156\n",
      "Epoch [6261/100000], L_train: 63.46260070800781,L_test: 99.41336822509766\n",
      "Epoch [6271/100000], L_train: 30.81449317932129,L_test: 35.96664810180664\n",
      "Epoch [6281/100000], L_train: 36.384971618652344,L_test: 41.460575103759766\n",
      "Epoch [6291/100000], L_train: 34.305755615234375,L_test: 41.557823181152344\n",
      "Epoch [6301/100000], L_train: 33.17801284790039,L_test: 41.561279296875\n",
      "Epoch [6311/100000], L_train: 32.88153076171875,L_test: 41.60693359375\n",
      "Epoch [6321/100000], L_train: 33.73826599121094,L_test: 41.59653091430664\n",
      "Epoch [6331/100000], L_train: 32.19746780395508,L_test: 41.58668518066406\n",
      "Epoch [6341/100000], L_train: 33.20826721191406,L_test: 41.52913284301758\n",
      "Epoch [6351/100000], L_train: 33.51862716674805,L_test: 41.57533264160156\n",
      "Epoch [6361/100000], L_train: 36.37141036987305,L_test: 41.58770751953125\n",
      "Epoch [6371/100000], L_train: 31.591283798217773,L_test: 41.58230209350586\n",
      "Epoch [6381/100000], L_train: 34.67702102661133,L_test: 41.57781219482422\n",
      "Epoch [6391/100000], L_train: 36.14910125732422,L_test: 41.59615707397461\n",
      "Epoch [6401/100000], L_train: 35.2366828918457,L_test: 41.557533264160156\n",
      "Epoch [6411/100000], L_train: 32.18354034423828,L_test: 41.578269958496094\n",
      "Epoch [6421/100000], L_train: 35.75137710571289,L_test: 41.55331802368164\n",
      "Epoch [6431/100000], L_train: 34.92084884643555,L_test: 41.60551834106445\n",
      "Epoch [6441/100000], L_train: 31.404972076416016,L_test: 41.57609939575195\n",
      "Epoch [6451/100000], L_train: 32.75753402709961,L_test: 41.572269439697266\n",
      "Epoch [6461/100000], L_train: 34.258174896240234,L_test: 41.617000579833984\n",
      "Epoch [6471/100000], L_train: 34.38168716430664,L_test: 41.57054138183594\n",
      "Epoch [6481/100000], L_train: 34.07194900512695,L_test: 41.58441162109375\n",
      "Epoch [6491/100000], L_train: 33.83911895751953,L_test: 41.582035064697266\n",
      "Epoch [6501/100000], L_train: 32.965980529785156,L_test: 41.55647659301758\n",
      "Epoch [6511/100000], L_train: 30.28527069091797,L_test: 41.57342529296875\n",
      "Epoch [6521/100000], L_train: 34.169960021972656,L_test: 41.600276947021484\n",
      "Epoch [6531/100000], L_train: 33.38471603393555,L_test: 41.5832633972168\n",
      "Epoch [6541/100000], L_train: 32.43275451660156,L_test: 41.57584762573242\n",
      "Epoch [6551/100000], L_train: 35.85260772705078,L_test: 41.59743118286133\n",
      "Epoch [6561/100000], L_train: 34.84056854248047,L_test: 41.535804748535156\n",
      "Epoch [6571/100000], L_train: 33.33376693725586,L_test: 41.53972244262695\n",
      "Epoch [6581/100000], L_train: 34.650474548339844,L_test: 41.54336166381836\n",
      "Epoch [6591/100000], L_train: 32.23671340942383,L_test: 41.59973907470703\n",
      "Epoch [6601/100000], L_train: 53.28825759887695,L_test: 62.41905212402344\n",
      "Epoch [6611/100000], L_train: 55.81829071044922,L_test: 63.749271392822266\n",
      "Epoch [6621/100000], L_train: 54.6645393371582,L_test: 63.220664978027344\n",
      "Epoch [6631/100000], L_train: 54.30350112915039,L_test: 59.775997161865234\n",
      "Epoch [6641/100000], L_train: 55.481082916259766,L_test: 60.23326110839844\n",
      "Epoch [6651/100000], L_train: 53.67909622192383,L_test: 60.50312805175781\n",
      "Epoch [6661/100000], L_train: 55.36550521850586,L_test: 60.21537780761719\n",
      "Epoch [6671/100000], L_train: 54.0371208190918,L_test: 66.0930404663086\n",
      "Epoch [6681/100000], L_train: 56.43035888671875,L_test: 59.696895599365234\n",
      "Epoch [6691/100000], L_train: 57.14799499511719,L_test: 62.65800857543945\n",
      "Epoch [6701/100000], L_train: 52.819149017333984,L_test: 64.78678894042969\n",
      "Epoch [6711/100000], L_train: 50.28512191772461,L_test: 59.78054428100586\n",
      "Epoch [6721/100000], L_train: 58.000614166259766,L_test: 61.12532424926758\n",
      "Epoch [6731/100000], L_train: 49.926422119140625,L_test: 77.05150604248047\n",
      "Epoch [6741/100000], L_train: 57.28069305419922,L_test: 68.88837432861328\n",
      "Epoch [6751/100000], L_train: 51.618370056152344,L_test: 58.80784606933594\n",
      "Epoch [6761/100000], L_train: 50.90460968017578,L_test: 60.38490295410156\n",
      "Epoch [6771/100000], L_train: 54.02577590942383,L_test: 67.62171936035156\n",
      "Epoch [6781/100000], L_train: 52.47660827636719,L_test: 58.89801788330078\n",
      "Epoch [6791/100000], L_train: 58.33021926879883,L_test: 58.847564697265625\n",
      "Epoch [6801/100000], L_train: 51.32931137084961,L_test: 61.60997772216797\n",
      "Epoch [6811/100000], L_train: 57.26853942871094,L_test: 64.27365112304688\n",
      "Epoch [6821/100000], L_train: 52.855220794677734,L_test: 64.83088684082031\n",
      "Epoch [6831/100000], L_train: 53.22685241699219,L_test: 66.58477783203125\n",
      "Epoch [6841/100000], L_train: 59.60981750488281,L_test: 62.74449157714844\n",
      "Epoch [6851/100000], L_train: 50.498451232910156,L_test: 62.84952926635742\n",
      "Epoch [6861/100000], L_train: 55.12288284301758,L_test: 60.986907958984375\n",
      "Epoch [6871/100000], L_train: 52.09874725341797,L_test: 60.58491516113281\n",
      "Epoch [6881/100000], L_train: 54.20729064941406,L_test: 60.07806396484375\n",
      "Epoch [6891/100000], L_train: 46.63333511352539,L_test: 61.62706756591797\n",
      "Epoch [6901/100000], L_train: 47.00629806518555,L_test: 70.20342254638672\n",
      "Epoch [6911/100000], L_train: 56.718994140625,L_test: 57.03767776489258\n",
      "Epoch [6921/100000], L_train: 51.918216705322266,L_test: 60.575218200683594\n",
      "Epoch [6931/100000], L_train: 52.3168830871582,L_test: 53.38899612426758\n",
      "Epoch [6941/100000], L_train: 45.86879348754883,L_test: 56.33623123168945\n",
      "Epoch [6951/100000], L_train: 54.310020446777344,L_test: 52.610958099365234\n",
      "Epoch [6961/100000], L_train: 40.51858901977539,L_test: 48.11396408081055\n",
      "Epoch [6971/100000], L_train: 52.943138122558594,L_test: 69.1006851196289\n",
      "Epoch [6981/100000], L_train: 46.39836502075195,L_test: 71.18781280517578\n",
      "Epoch [6991/100000], L_train: 58.230960845947266,L_test: 66.193115234375\n",
      "Epoch [7001/100000], L_train: 48.36117935180664,L_test: 54.81263732910156\n",
      "Epoch [7011/100000], L_train: 44.63861846923828,L_test: 46.94076919555664\n",
      "Epoch [7021/100000], L_train: 50.8772087097168,L_test: 57.94390869140625\n",
      "Epoch [7031/100000], L_train: 49.645286560058594,L_test: 48.188758850097656\n",
      "Epoch [7041/100000], L_train: 42.437862396240234,L_test: 50.608848571777344\n",
      "Epoch [7051/100000], L_train: 33.81336975097656,L_test: 80.6380844116211\n",
      "Epoch [7061/100000], L_train: 36.64833450317383,L_test: 52.599735260009766\n",
      "Epoch [7071/100000], L_train: 45.346805572509766,L_test: 55.585655212402344\n",
      "Epoch [7081/100000], L_train: 38.60743713378906,L_test: 47.054237365722656\n",
      "Epoch [7091/100000], L_train: 98.2466812133789,L_test: 41.59311294555664\n",
      "Epoch [7101/100000], L_train: 43.67384338378906,L_test: 44.01557922363281\n",
      "Epoch [7111/100000], L_train: 36.23716354370117,L_test: 61.33840560913086\n",
      "Epoch [7121/100000], L_train: 41.529319763183594,L_test: 33.63154220581055\n",
      "Epoch [7131/100000], L_train: 32.74299240112305,L_test: 56.11838150024414\n",
      "Epoch [7141/100000], L_train: 182.5626983642578,L_test: 169.45938110351562\n",
      "Epoch [7151/100000], L_train: 183.6357879638672,L_test: 169.31895446777344\n",
      "Epoch [7161/100000], L_train: 176.6056671142578,L_test: 169.19097900390625\n",
      "Epoch [7171/100000], L_train: 177.3402862548828,L_test: 169.0720977783203\n",
      "Epoch [7181/100000], L_train: 176.0270538330078,L_test: 168.9601593017578\n",
      "Epoch [7191/100000], L_train: 174.7923126220703,L_test: 168.85366821289062\n",
      "Epoch [7201/100000], L_train: 178.338623046875,L_test: 168.75714111328125\n",
      "Epoch [7211/100000], L_train: 174.950927734375,L_test: 168.66909790039062\n",
      "Epoch [7221/100000], L_train: 173.66433715820312,L_test: 168.57369995117188\n",
      "Epoch [7231/100000], L_train: 182.6414794921875,L_test: 168.48089599609375\n",
      "Epoch [7241/100000], L_train: 177.2076873779297,L_test: 168.39036560058594\n",
      "Epoch [7251/100000], L_train: 176.47972106933594,L_test: 168.30184936523438\n",
      "Epoch [7261/100000], L_train: 171.19790649414062,L_test: 168.21505737304688\n",
      "Epoch [7271/100000], L_train: 175.5100860595703,L_test: 168.1298065185547\n",
      "Epoch [7281/100000], L_train: 175.51792907714844,L_test: 168.04591369628906\n",
      "Epoch [7291/100000], L_train: 177.87594604492188,L_test: 167.96327209472656\n",
      "Epoch [7301/100000], L_train: 176.23451232910156,L_test: 167.88165283203125\n",
      "Epoch [7311/100000], L_train: 177.541748046875,L_test: 167.80105590820312\n",
      "Epoch [7321/100000], L_train: 171.66102600097656,L_test: 167.72129821777344\n",
      "Epoch [7331/100000], L_train: 178.1114959716797,L_test: 167.642333984375\n",
      "Epoch [7341/100000], L_train: 176.15419006347656,L_test: 167.5640869140625\n",
      "Epoch [7351/100000], L_train: 172.83856201171875,L_test: 167.4865264892578\n",
      "Epoch [7361/100000], L_train: 176.23666381835938,L_test: 167.40953063964844\n",
      "Epoch [7371/100000], L_train: 176.98585510253906,L_test: 167.33306884765625\n",
      "Epoch [7381/100000], L_train: 171.9270477294922,L_test: 167.2587890625\n",
      "Epoch [7391/100000], L_train: 170.82940673828125,L_test: 167.18414306640625\n",
      "Epoch [7401/100000], L_train: 96.28206634521484,L_test: 167.2517852783203\n",
      "Epoch [7411/100000], L_train: 177.01817321777344,L_test: 167.0409698486328\n",
      "Epoch [7421/100000], L_train: 177.15484619140625,L_test: 166.96653747558594\n",
      "Epoch [7431/100000], L_train: 176.2560272216797,L_test: 166.89244079589844\n",
      "Epoch [7441/100000], L_train: 175.44772338867188,L_test: 166.8186492919922\n",
      "Epoch [7451/100000], L_train: 177.7683868408203,L_test: 166.7467041015625\n",
      "Epoch [7461/100000], L_train: 180.53749084472656,L_test: 166.67364501953125\n",
      "Epoch [7471/100000], L_train: 173.06590270996094,L_test: 166.60060119628906\n",
      "Epoch [7481/100000], L_train: 178.9976806640625,L_test: 166.52980041503906\n",
      "Epoch [7491/100000], L_train: 177.08143615722656,L_test: 166.4592742919922\n",
      "Epoch [7501/100000], L_train: 169.61309814453125,L_test: 166.38682556152344\n",
      "Epoch [7511/100000], L_train: 171.37315368652344,L_test: 166.31455993652344\n",
      "Epoch [7521/100000], L_train: 174.87962341308594,L_test: 166.24246215820312\n",
      "Epoch [7531/100000], L_train: 176.4003448486328,L_test: 166.17050170898438\n",
      "Epoch [7541/100000], L_train: 172.4386749267578,L_test: 166.09864807128906\n",
      "Epoch [7551/100000], L_train: 170.86598205566406,L_test: 166.03038024902344\n",
      "Epoch [7561/100000], L_train: 175.4906005859375,L_test: 165.95909118652344\n",
      "Epoch [7571/100000], L_train: 173.47401428222656,L_test: 165.88754272460938\n",
      "Epoch [7581/100000], L_train: 179.4778289794922,L_test: 165.81619262695312\n",
      "Epoch [7591/100000], L_train: 177.7763671875,L_test: 165.7479705810547\n",
      "Epoch [7601/100000], L_train: 174.632080078125,L_test: 165.67742919921875\n",
      "Epoch [7611/100000], L_train: 175.53611755371094,L_test: 165.606201171875\n",
      "Epoch [7621/100000], L_train: 176.5862579345703,L_test: 165.53517150878906\n",
      "Epoch [7631/100000], L_train: 173.3077392578125,L_test: 165.4641571044922\n",
      "Epoch [7641/100000], L_train: 173.94696044921875,L_test: 165.39309692382812\n",
      "Epoch [7651/100000], L_train: 175.51455688476562,L_test: 165.322265625\n",
      "Epoch [7661/100000], L_train: 176.058837890625,L_test: 165.25149536132812\n",
      "Epoch [7671/100000], L_train: 174.963134765625,L_test: 165.18075561523438\n",
      "Epoch [7681/100000], L_train: 174.5231475830078,L_test: 165.10997009277344\n",
      "Epoch [7691/100000], L_train: 172.54588317871094,L_test: 165.03924560546875\n",
      "Epoch [7701/100000], L_train: 173.36618041992188,L_test: 58.299503326416016\n",
      "Epoch [7711/100000], L_train: 176.17071533203125,L_test: 164.90191650390625\n",
      "Epoch [7721/100000], L_train: 176.4015655517578,L_test: 164.83143615722656\n",
      "Epoch [7731/100000], L_train: 173.17909240722656,L_test: 164.761474609375\n",
      "Epoch [7741/100000], L_train: 179.31163024902344,L_test: 164.69239807128906\n",
      "Epoch [7751/100000], L_train: 171.84571838378906,L_test: 164.62191772460938\n",
      "Epoch [7761/100000], L_train: 177.4282684326172,L_test: 164.55145263671875\n",
      "Epoch [7771/100000], L_train: 169.2047882080078,L_test: 164.48094177246094\n",
      "Epoch [7781/100000], L_train: 170.4923858642578,L_test: 164.4104461669922\n",
      "Epoch [7791/100000], L_train: 176.46485900878906,L_test: 164.34011840820312\n",
      "Epoch [7801/100000], L_train: 175.84913635253906,L_test: 164.26988220214844\n",
      "Epoch [7811/100000], L_train: 172.00108337402344,L_test: 164.19964599609375\n",
      "Epoch [7821/100000], L_train: 173.63523864746094,L_test: 164.12940979003906\n",
      "Epoch [7831/100000], L_train: 173.58258056640625,L_test: 164.05918884277344\n",
      "Epoch [7841/100000], L_train: 174.13412475585938,L_test: 163.99240112304688\n",
      "Epoch [7851/100000], L_train: 177.4962615966797,L_test: 163.9224395751953\n",
      "Epoch [7861/100000], L_train: 177.9883270263672,L_test: 163.85223388671875\n",
      "Epoch [7871/100000], L_train: 173.570068359375,L_test: 163.781982421875\n",
      "Epoch [7881/100000], L_train: 175.5778350830078,L_test: 163.71176147460938\n",
      "Epoch [7891/100000], L_train: 175.1049041748047,L_test: 163.6415252685547\n",
      "Epoch [7901/100000], L_train: 169.9749298095703,L_test: 163.5712890625\n",
      "Epoch [7911/100000], L_train: 168.29238891601562,L_test: 163.50106811523438\n",
      "Epoch [7921/100000], L_train: 172.3634490966797,L_test: 163.43084716796875\n",
      "Epoch [7931/100000], L_train: 173.96705627441406,L_test: 163.36062622070312\n",
      "Epoch [7941/100000], L_train: 169.41969299316406,L_test: 163.29039001464844\n",
      "Epoch [7951/100000], L_train: 174.7767333984375,L_test: 163.2218780517578\n",
      "Epoch [7961/100000], L_train: 170.57444763183594,L_test: 163.1520538330078\n",
      "Epoch [7971/100000], L_train: 168.15426635742188,L_test: 163.08367919921875\n",
      "Epoch [7981/100000], L_train: 178.85482788085938,L_test: 163.01376342773438\n",
      "Epoch [7991/100000], L_train: 164.0393524169922,L_test: 162.94366455078125\n",
      "Epoch [8001/100000], L_train: 171.77854919433594,L_test: 162.8737335205078\n",
      "Epoch [8011/100000], L_train: 175.098388671875,L_test: 162.80372619628906\n",
      "Epoch [8021/100000], L_train: 173.74159240722656,L_test: 162.73379516601562\n",
      "Epoch [8031/100000], L_train: 172.2656707763672,L_test: 162.66688537597656\n",
      "Epoch [8041/100000], L_train: 165.8109893798828,L_test: 162.59674072265625\n",
      "Epoch [8051/100000], L_train: 173.03240966796875,L_test: 162.5267791748047\n",
      "Epoch [8061/100000], L_train: 171.0090789794922,L_test: 162.4568328857422\n",
      "Epoch [8071/100000], L_train: 172.9328155517578,L_test: 162.38685607910156\n",
      "Epoch [8081/100000], L_train: 169.60256958007812,L_test: 162.31800842285156\n",
      "Epoch [8091/100000], L_train: 168.792724609375,L_test: 162.25527954101562\n",
      "Epoch [8101/100000], L_train: 167.65371704101562,L_test: 162.18508911132812\n",
      "Epoch [8111/100000], L_train: 171.3795166015625,L_test: 162.1148681640625\n",
      "Epoch [8121/100000], L_train: 169.99264526367188,L_test: 162.04464721679688\n",
      "Epoch [8131/100000], L_train: 169.820556640625,L_test: 161.97442626953125\n",
      "Epoch [8141/100000], L_train: 172.58030700683594,L_test: 161.90419006347656\n",
      "Epoch [8151/100000], L_train: 170.13673400878906,L_test: 161.833984375\n",
      "Epoch [8161/100000], L_train: 171.32907104492188,L_test: 161.76402282714844\n",
      "Epoch [8171/100000], L_train: 173.47288513183594,L_test: 161.6940460205078\n",
      "Epoch [8181/100000], L_train: 173.02587890625,L_test: 161.62408447265625\n",
      "Epoch [8191/100000], L_train: 169.33905029296875,L_test: 161.5541229248047\n",
      "Epoch [8201/100000], L_train: 169.9876251220703,L_test: 161.4841766357422\n",
      "Epoch [8211/100000], L_train: 167.21067810058594,L_test: 161.41419982910156\n",
      "Epoch [8221/100000], L_train: 169.52590942382812,L_test: 161.34423828125\n",
      "Epoch [8231/100000], L_train: 172.89276123046875,L_test: 161.27430725097656\n",
      "Epoch [8241/100000], L_train: 166.89215087890625,L_test: 161.20431518554688\n",
      "Epoch [8251/100000], L_train: 175.7945098876953,L_test: 161.13436889648438\n",
      "Epoch [8261/100000], L_train: 167.73826599121094,L_test: 161.0644073486328\n",
      "Epoch [8271/100000], L_train: 170.6947021484375,L_test: 160.9944305419922\n",
      "Epoch [8281/100000], L_train: 170.5602569580078,L_test: 160.92449951171875\n",
      "Epoch [8291/100000], L_train: 170.05740356445312,L_test: 160.85450744628906\n",
      "Epoch [8301/100000], L_train: 171.58880615234375,L_test: 160.78456115722656\n",
      "Epoch [8311/100000], L_train: 173.85939025878906,L_test: 160.71458435058594\n",
      "Epoch [8321/100000], L_train: 164.93601989746094,L_test: 160.64463806152344\n",
      "Epoch [8331/100000], L_train: 170.14138793945312,L_test: 160.57467651367188\n",
      "Epoch [8341/100000], L_train: 166.2738800048828,L_test: 160.50469970703125\n",
      "Epoch [8351/100000], L_train: 169.95654296875,L_test: 160.43475341796875\n",
      "Epoch [8361/100000], L_train: 166.60238647460938,L_test: 160.36477661132812\n",
      "Epoch [8371/100000], L_train: 170.40240478515625,L_test: 160.29481506347656\n",
      "Epoch [8381/100000], L_train: 169.15121459960938,L_test: 160.224853515625\n",
      "Epoch [8391/100000], L_train: 168.10687255859375,L_test: 160.15489196777344\n",
      "Epoch [8401/100000], L_train: 170.56922912597656,L_test: 160.08494567871094\n",
      "Epoch [8411/100000], L_train: 170.73208618164062,L_test: 160.0149688720703\n",
      "Epoch [8421/100000], L_train: 171.6736602783203,L_test: 159.9450225830078\n",
      "Epoch [8431/100000], L_train: 170.611572265625,L_test: 159.8750457763672\n",
      "Epoch [8441/100000], L_train: 165.32772827148438,L_test: 159.8050994873047\n",
      "Epoch [8451/100000], L_train: 164.83982849121094,L_test: 159.73513793945312\n",
      "Epoch [8461/100000], L_train: 169.39212036132812,L_test: 159.66514587402344\n",
      "Epoch [8471/100000], L_train: 166.75274658203125,L_test: 159.59521484375\n",
      "Epoch [8481/100000], L_train: 173.1002655029297,L_test: 159.52523803710938\n",
      "Epoch [8491/100000], L_train: 167.06353759765625,L_test: 159.4552764892578\n",
      "Epoch [8501/100000], L_train: 172.14617919921875,L_test: 159.3872833251953\n",
      "Epoch [8511/100000], L_train: 168.82167053222656,L_test: 159.31732177734375\n",
      "Epoch [8521/100000], L_train: 172.24081420898438,L_test: 159.2473602294922\n",
      "Epoch [8531/100000], L_train: 169.0839385986328,L_test: 159.17738342285156\n",
      "Epoch [8541/100000], L_train: 170.1570587158203,L_test: 159.10743713378906\n",
      "Epoch [8551/100000], L_train: 167.94114685058594,L_test: 159.03749084472656\n",
      "Epoch [8561/100000], L_train: 172.39854431152344,L_test: 158.96749877929688\n",
      "Epoch [8571/100000], L_train: 164.7954864501953,L_test: 158.89755249023438\n",
      "Epoch [8581/100000], L_train: 168.11289978027344,L_test: 158.82760620117188\n",
      "Epoch [8591/100000], L_train: 168.51036071777344,L_test: 158.75762939453125\n",
      "Epoch [8601/100000], L_train: 166.48675537109375,L_test: 158.68768310546875\n",
      "Epoch [8611/100000], L_train: 166.916259765625,L_test: 158.61769104003906\n",
      "Epoch [8621/100000], L_train: 166.95758056640625,L_test: 158.5495147705078\n",
      "Epoch [8631/100000], L_train: 167.98057556152344,L_test: 158.479736328125\n",
      "Epoch [8641/100000], L_train: 169.99932861328125,L_test: 158.40977478027344\n",
      "Epoch [8651/100000], L_train: 163.00473022460938,L_test: 158.33981323242188\n",
      "Epoch [8661/100000], L_train: 168.87478637695312,L_test: 158.26983642578125\n",
      "Epoch [8671/100000], L_train: 167.06846618652344,L_test: 158.1999053955078\n",
      "Epoch [8681/100000], L_train: 169.89111328125,L_test: 158.12989807128906\n",
      "Epoch [8691/100000], L_train: 165.41766357421875,L_test: 158.05996704101562\n",
      "Epoch [8701/100000], L_train: 168.6959991455078,L_test: 157.99002075195312\n",
      "Epoch [8711/100000], L_train: 162.24671936035156,L_test: 157.92002868652344\n",
      "Epoch [8721/100000], L_train: 170.95516967773438,L_test: 157.85009765625\n",
      "Epoch [8731/100000], L_train: 168.69754028320312,L_test: 157.78012084960938\n",
      "Epoch [8741/100000], L_train: 166.2115478515625,L_test: 157.7101593017578\n",
      "Epoch [8751/100000], L_train: 167.41961669921875,L_test: 157.64019775390625\n",
      "Epoch [8761/100000], L_train: 169.62889099121094,L_test: 157.5702362060547\n",
      "Epoch [8771/100000], L_train: 168.76319885253906,L_test: 157.5002899169922\n",
      "Epoch [8781/100000], L_train: 162.4280242919922,L_test: 157.4302978515625\n",
      "Epoch [8791/100000], L_train: 162.123291015625,L_test: 157.3603515625\n",
      "Epoch [8801/100000], L_train: 171.3116455078125,L_test: 157.29039001464844\n",
      "Epoch [8811/100000], L_train: 157.98536682128906,L_test: 157.2204132080078\n",
      "Epoch [8821/100000], L_train: 169.63099670410156,L_test: 157.1504669189453\n",
      "Epoch [8831/100000], L_train: 168.41940307617188,L_test: 157.08050537109375\n",
      "Epoch [8841/100000], L_train: 164.38812255859375,L_test: 157.0105438232422\n",
      "Epoch [8851/100000], L_train: 169.29376220703125,L_test: 156.94058227539062\n",
      "Epoch [8861/100000], L_train: 169.79873657226562,L_test: 156.87062072753906\n",
      "Epoch [8871/100000], L_train: 163.75335693359375,L_test: 156.80067443847656\n",
      "Epoch [8881/100000], L_train: 163.93429565429688,L_test: 156.73068237304688\n",
      "Epoch [8891/100000], L_train: 167.0211181640625,L_test: 156.66073608398438\n",
      "Epoch [8901/100000], L_train: 163.14476013183594,L_test: 156.59255981445312\n",
      "Epoch [8911/100000], L_train: 162.4576416015625,L_test: 156.52256774902344\n",
      "Epoch [8921/100000], L_train: 169.19692993164062,L_test: 156.45263671875\n",
      "Epoch [8931/100000], L_train: 167.48770141601562,L_test: 156.38265991210938\n",
      "Epoch [8941/100000], L_train: 169.76242065429688,L_test: 156.3126983642578\n",
      "Epoch [8951/100000], L_train: 169.635009765625,L_test: 156.24273681640625\n",
      "Epoch [8961/100000], L_train: 167.93165588378906,L_test: 156.1727752685547\n",
      "Epoch [8971/100000], L_train: 165.08413696289062,L_test: 156.1028289794922\n",
      "Epoch [8981/100000], L_train: 163.28076171875,L_test: 156.0328369140625\n",
      "Epoch [8991/100000], L_train: 162.5139923095703,L_test: 155.962890625\n",
      "Epoch [9001/100000], L_train: 166.64984130859375,L_test: 155.89292907714844\n",
      "Epoch [9011/100000], L_train: 166.51112365722656,L_test: 155.8229522705078\n",
      "Epoch [9021/100000], L_train: 161.269775390625,L_test: 155.7530059814453\n",
      "Epoch [9031/100000], L_train: 168.59359741210938,L_test: 155.68304443359375\n",
      "Epoch [9041/100000], L_train: 167.1027374267578,L_test: 155.6130828857422\n",
      "Epoch [9051/100000], L_train: 167.17274475097656,L_test: 155.54312133789062\n",
      "Epoch [9061/100000], L_train: 161.0196075439453,L_test: 155.47315979003906\n",
      "Epoch [9071/100000], L_train: 162.092529296875,L_test: 155.40321350097656\n",
      "Epoch [9081/100000], L_train: 165.69717407226562,L_test: 155.33322143554688\n",
      "Epoch [9091/100000], L_train: 164.92822265625,L_test: 155.26327514648438\n",
      "Epoch [9101/100000], L_train: 160.0521240234375,L_test: 155.1933135986328\n",
      "Epoch [9111/100000], L_train: 164.45277404785156,L_test: 155.12335205078125\n",
      "Epoch [9121/100000], L_train: 164.65919494628906,L_test: 155.05340576171875\n",
      "Epoch [9131/100000], L_train: 167.63650512695312,L_test: 154.98341369628906\n",
      "Epoch [9141/100000], L_train: 166.1722869873047,L_test: 154.91346740722656\n",
      "Epoch [9151/100000], L_train: 163.9822235107422,L_test: 154.843505859375\n",
      "Epoch [9161/100000], L_train: 163.6129150390625,L_test: 154.77354431152344\n",
      "Epoch [9171/100000], L_train: 159.2260284423828,L_test: 154.70358276367188\n",
      "Epoch [9181/100000], L_train: 161.48138427734375,L_test: 154.63360595703125\n",
      "Epoch [9191/100000], L_train: 159.1017303466797,L_test: 154.5636749267578\n",
      "Epoch [9201/100000], L_train: 165.37667846679688,L_test: 154.49368286132812\n",
      "Epoch [9211/100000], L_train: 168.8173828125,L_test: 154.42373657226562\n",
      "Epoch [9221/100000], L_train: 161.86317443847656,L_test: 154.35379028320312\n",
      "Epoch [9231/100000], L_train: 164.66494750976562,L_test: 154.28379821777344\n",
      "Epoch [9241/100000], L_train: 165.0690460205078,L_test: 154.21385192871094\n",
      "Epoch [9251/100000], L_train: 163.28402709960938,L_test: 154.1438751220703\n",
      "Epoch [9261/100000], L_train: 165.75155639648438,L_test: 154.0739288330078\n",
      "Epoch [9271/100000], L_train: 161.03219604492188,L_test: 154.00396728515625\n",
      "Epoch [9281/100000], L_train: 161.80819702148438,L_test: 153.9340057373047\n",
      "Epoch [9291/100000], L_train: 158.8738250732422,L_test: 153.8640594482422\n",
      "Epoch [9301/100000], L_train: 159.6684112548828,L_test: 153.79405212402344\n",
      "Epoch [9311/100000], L_train: 162.18548583984375,L_test: 153.72412109375\n",
      "Epoch [9321/100000], L_train: 164.94285583496094,L_test: 153.65415954589844\n",
      "Epoch [9331/100000], L_train: 165.57638549804688,L_test: 153.5841827392578\n",
      "Epoch [9341/100000], L_train: 166.0769500732422,L_test: 153.5142364501953\n",
      "Epoch [9351/100000], L_train: 161.62908935546875,L_test: 153.44427490234375\n",
      "Epoch [9361/100000], L_train: 161.12315368652344,L_test: 153.3743133544922\n",
      "Epoch [9371/100000], L_train: 162.8269500732422,L_test: 153.30433654785156\n",
      "Epoch [9381/100000], L_train: 163.73944091796875,L_test: 153.23439025878906\n",
      "Epoch [9391/100000], L_train: 160.83262634277344,L_test: 153.16444396972656\n",
      "Epoch [9401/100000], L_train: 161.41940307617188,L_test: 153.09445190429688\n",
      "Epoch [9411/100000], L_train: 160.75787353515625,L_test: 153.02450561523438\n",
      "Epoch [9421/100000], L_train: 159.95896911621094,L_test: 152.95455932617188\n",
      "Epoch [9431/100000], L_train: 163.26707458496094,L_test: 152.88458251953125\n",
      "Epoch [9441/100000], L_train: 164.00634765625,L_test: 152.81463623046875\n",
      "Epoch [9451/100000], L_train: 167.83551025390625,L_test: 152.74464416503906\n",
      "Epoch [9461/100000], L_train: 167.0105438232422,L_test: 152.67469787597656\n",
      "Epoch [9471/100000], L_train: 162.04566955566406,L_test: 152.604736328125\n",
      "Epoch [9481/100000], L_train: 158.7796173095703,L_test: 152.53477478027344\n",
      "Epoch [9491/100000], L_train: 158.1017608642578,L_test: 152.46481323242188\n",
      "Epoch [9501/100000], L_train: 166.22720336914062,L_test: 152.39483642578125\n",
      "Epoch [9511/100000], L_train: 166.8103485107422,L_test: 152.3249053955078\n",
      "Epoch [9521/100000], L_train: 160.7484893798828,L_test: 152.25489807128906\n",
      "Epoch [9531/100000], L_train: 158.8640594482422,L_test: 152.18496704101562\n",
      "Epoch [9541/100000], L_train: 157.3329315185547,L_test: 152.11502075195312\n",
      "Epoch [9551/100000], L_train: 164.63453674316406,L_test: 152.04502868652344\n",
      "Epoch [9561/100000], L_train: 159.39663696289062,L_test: 151.97508239746094\n",
      "Epoch [9571/100000], L_train: 161.5986328125,L_test: 151.90512084960938\n",
      "Epoch [9581/100000], L_train: 165.90658569335938,L_test: 151.8351593017578\n",
      "Epoch [9591/100000], L_train: 158.36553955078125,L_test: 151.76519775390625\n",
      "Epoch [9601/100000], L_train: 160.9691619873047,L_test: 151.6952362060547\n",
      "Epoch [9611/100000], L_train: 159.60752868652344,L_test: 151.6252899169922\n",
      "Epoch [9621/100000], L_train: 161.212158203125,L_test: 151.5552978515625\n",
      "Epoch [9631/100000], L_train: 162.56900024414062,L_test: 151.4853515625\n",
      "Epoch [9641/100000], L_train: 159.60450744628906,L_test: 151.41539001464844\n",
      "Epoch [9651/100000], L_train: 157.8364715576172,L_test: 151.3454132080078\n",
      "Epoch [9661/100000], L_train: 159.58270263671875,L_test: 151.2754669189453\n",
      "Epoch [9671/100000], L_train: 157.48309326171875,L_test: 151.20550537109375\n",
      "Epoch [9681/100000], L_train: 155.12559509277344,L_test: 151.1355438232422\n",
      "Epoch [9691/100000], L_train: 157.57382202148438,L_test: 151.06558227539062\n",
      "Epoch [9701/100000], L_train: 158.9852752685547,L_test: 150.99562072753906\n",
      "Epoch [9711/100000], L_train: 161.42324829101562,L_test: 150.92567443847656\n",
      "Epoch [9721/100000], L_train: 161.83692932128906,L_test: 150.85568237304688\n",
      "Epoch [9731/100000], L_train: 159.56959533691406,L_test: 150.78573608398438\n",
      "Epoch [9741/100000], L_train: 154.53201293945312,L_test: 150.7157745361328\n",
      "Epoch [9751/100000], L_train: 161.93882751464844,L_test: 150.64581298828125\n",
      "Epoch [9761/100000], L_train: 159.6794891357422,L_test: 150.57586669921875\n",
      "Epoch [9771/100000], L_train: 159.98727416992188,L_test: 150.50587463378906\n",
      "Epoch [9781/100000], L_train: 158.45330810546875,L_test: 150.43592834472656\n",
      "Epoch [9791/100000], L_train: 161.02511596679688,L_test: 150.365966796875\n",
      "Epoch [9801/100000], L_train: 158.05821228027344,L_test: 150.29600524902344\n",
      "Epoch [9811/100000], L_train: 158.90689086914062,L_test: 150.22604370117188\n",
      "Epoch [9821/100000], L_train: 156.84861755371094,L_test: 150.15606689453125\n",
      "Epoch [9831/100000], L_train: 160.79417419433594,L_test: 150.0861358642578\n",
      "Epoch [9841/100000], L_train: 157.45846557617188,L_test: 150.01614379882812\n",
      "Epoch [9851/100000], L_train: 160.56146240234375,L_test: 149.94619750976562\n",
      "Epoch [9861/100000], L_train: 156.69515991210938,L_test: 149.87625122070312\n",
      "Epoch [9871/100000], L_train: 160.01560974121094,L_test: 149.80625915527344\n",
      "Epoch [9881/100000], L_train: 156.35401916503906,L_test: 149.73631286621094\n",
      "Epoch [9891/100000], L_train: 156.1814422607422,L_test: 149.6663360595703\n",
      "Epoch [9901/100000], L_train: 157.45904541015625,L_test: 149.5963897705078\n",
      "Epoch [9911/100000], L_train: 154.57566833496094,L_test: 149.52642822265625\n",
      "Epoch [9921/100000], L_train: 154.8590850830078,L_test: 149.4564666748047\n",
      "Epoch [9931/100000], L_train: 154.65435791015625,L_test: 149.3865203857422\n",
      "Epoch [9941/100000], L_train: 161.66845703125,L_test: 149.31651306152344\n",
      "Epoch [9951/100000], L_train: 153.10061645507812,L_test: 149.24658203125\n",
      "Epoch [9961/100000], L_train: 159.24986267089844,L_test: 149.17662048339844\n",
      "Epoch [9971/100000], L_train: 153.83425903320312,L_test: 149.1066436767578\n",
      "Epoch [9981/100000], L_train: 157.76295471191406,L_test: 149.0366973876953\n",
      "Epoch [9991/100000], L_train: 158.0609588623047,L_test: 148.96673583984375\n",
      "Epoch [10001/100000], L_train: 155.4951171875,L_test: 148.8967742919922\n",
      "Epoch [10011/100000], L_train: 156.48727416992188,L_test: 148.82679748535156\n",
      "Epoch [10021/100000], L_train: 162.9490509033203,L_test: 148.75685119628906\n",
      "Epoch [10031/100000], L_train: 160.3866729736328,L_test: 148.68690490722656\n",
      "Epoch [10041/100000], L_train: 158.62091064453125,L_test: 148.61691284179688\n",
      "Epoch [10051/100000], L_train: 157.7334747314453,L_test: 148.54696655273438\n",
      "Epoch [10061/100000], L_train: 157.09507751464844,L_test: 148.47702026367188\n",
      "Epoch [10071/100000], L_train: 155.35533142089844,L_test: 148.40704345703125\n",
      "Epoch [10081/100000], L_train: 157.72857666015625,L_test: 148.33709716796875\n",
      "Epoch [10091/100000], L_train: 149.35467529296875,L_test: 148.26710510253906\n",
      "Epoch [10101/100000], L_train: 159.60736083984375,L_test: 148.19715881347656\n",
      "Epoch [10111/100000], L_train: 156.4461669921875,L_test: 148.127197265625\n",
      "Epoch [10121/100000], L_train: 152.57525634765625,L_test: 148.05723571777344\n",
      "Epoch [10131/100000], L_train: 159.23907470703125,L_test: 147.98727416992188\n",
      "Epoch [10141/100000], L_train: 154.14520263671875,L_test: 147.91729736328125\n",
      "Epoch [10151/100000], L_train: 158.74232482910156,L_test: 147.8473663330078\n",
      "Epoch [10161/100000], L_train: 156.86314392089844,L_test: 147.77735900878906\n",
      "Epoch [10171/100000], L_train: 158.96832275390625,L_test: 147.70742797851562\n",
      "Epoch [10181/100000], L_train: 160.01405334472656,L_test: 147.63748168945312\n",
      "Epoch [10191/100000], L_train: 159.48861694335938,L_test: 147.56748962402344\n",
      "Epoch [10201/100000], L_train: 153.90835571289062,L_test: 147.49754333496094\n",
      "Epoch [10211/100000], L_train: 156.8732452392578,L_test: 147.42758178710938\n",
      "Epoch [10221/100000], L_train: 157.2244873046875,L_test: 147.3576202392578\n",
      "Epoch [10231/100000], L_train: 452.4060363769531,L_test: 56.913780212402344\n",
      "Epoch [10241/100000], L_train: 151.53855895996094,L_test: 147.2205047607422\n",
      "Epoch [10251/100000], L_train: 153.5216522216797,L_test: 147.15054321289062\n",
      "Epoch [10261/100000], L_train: 159.4689483642578,L_test: 147.08058166503906\n",
      "Epoch [10271/100000], L_train: 158.84727478027344,L_test: 147.01063537597656\n",
      "Epoch [10281/100000], L_train: 159.5215606689453,L_test: 146.94064331054688\n",
      "Epoch [10291/100000], L_train: 155.49713134765625,L_test: 146.87069702148438\n",
      "Epoch [10301/100000], L_train: 152.73458862304688,L_test: 146.8007354736328\n",
      "Epoch [10311/100000], L_train: 154.4792022705078,L_test: 146.73077392578125\n",
      "Epoch [10321/100000], L_train: 162.56314086914062,L_test: 146.66082763671875\n",
      "Epoch [10331/100000], L_train: 154.37362670898438,L_test: 146.59083557128906\n",
      "Epoch [10341/100000], L_train: 153.8655242919922,L_test: 146.52088928222656\n",
      "Epoch [10351/100000], L_train: 156.86764526367188,L_test: 146.450927734375\n",
      "Epoch [10361/100000], L_train: 150.46253967285156,L_test: 146.38096618652344\n",
      "Epoch [10371/100000], L_train: 157.6448516845703,L_test: 146.31100463867188\n",
      "Epoch [10381/100000], L_train: 155.90245056152344,L_test: 146.24102783203125\n",
      "Epoch [10391/100000], L_train: 154.12461853027344,L_test: 146.1710968017578\n",
      "Epoch [10401/100000], L_train: 149.53021240234375,L_test: 146.10110473632812\n",
      "Epoch [10411/100000], L_train: 159.2585906982422,L_test: 146.03115844726562\n",
      "Epoch [10421/100000], L_train: 157.89613342285156,L_test: 145.96121215820312\n",
      "Epoch [10431/100000], L_train: 153.84715270996094,L_test: 145.89122009277344\n",
      "Epoch [10441/100000], L_train: 151.5159149169922,L_test: 145.82127380371094\n",
      "Epoch [10451/100000], L_train: 156.89930725097656,L_test: 145.7512969970703\n",
      "Epoch [10461/100000], L_train: 155.298095703125,L_test: 145.6813507080078\n",
      "Epoch [10471/100000], L_train: 157.6182861328125,L_test: 145.61138916015625\n",
      "Epoch [10481/100000], L_train: 158.4459228515625,L_test: 145.5414276123047\n",
      "Epoch [10491/100000], L_train: 153.03262329101562,L_test: 145.4714813232422\n",
      "Epoch [10501/100000], L_train: 156.68800354003906,L_test: 145.40147399902344\n",
      "Epoch [10511/100000], L_train: 155.6085662841797,L_test: 145.33154296875\n",
      "Epoch [10521/100000], L_train: 154.9553680419922,L_test: 145.26158142089844\n",
      "Epoch [10531/100000], L_train: 159.58895874023438,L_test: 145.1916046142578\n",
      "Epoch [10541/100000], L_train: 148.10401916503906,L_test: 145.1216583251953\n",
      "Epoch [10551/100000], L_train: 158.07850646972656,L_test: 145.05169677734375\n",
      "Epoch [10561/100000], L_train: 148.3730926513672,L_test: 144.9817352294922\n",
      "Epoch [10571/100000], L_train: 153.4903106689453,L_test: 144.91175842285156\n",
      "Epoch [10581/100000], L_train: 154.620849609375,L_test: 144.84181213378906\n",
      "Epoch [10591/100000], L_train: 156.42869567871094,L_test: 144.77186584472656\n",
      "Epoch [10601/100000], L_train: 147.11402893066406,L_test: 144.70187377929688\n",
      "Epoch [10611/100000], L_train: 157.68414306640625,L_test: 144.63192749023438\n",
      "Epoch [10621/100000], L_train: 161.62843322753906,L_test: 144.56198120117188\n",
      "Epoch [10631/100000], L_train: 155.4865264892578,L_test: 144.49200439453125\n",
      "Epoch [10641/100000], L_train: 156.4464874267578,L_test: 144.42205810546875\n",
      "Epoch [10651/100000], L_train: 152.6004638671875,L_test: 144.35206604003906\n",
      "Epoch [10661/100000], L_train: 152.4228057861328,L_test: 144.28211975097656\n",
      "Epoch [10671/100000], L_train: 154.1007537841797,L_test: 144.212158203125\n",
      "Epoch [10681/100000], L_train: 151.8000030517578,L_test: 144.14219665527344\n",
      "Epoch [10691/100000], L_train: 152.30859375,L_test: 144.07223510742188\n",
      "Epoch [10701/100000], L_train: 155.0705108642578,L_test: 144.00225830078125\n",
      "Epoch [10711/100000], L_train: 149.82598876953125,L_test: 143.9323272705078\n",
      "Epoch [10721/100000], L_train: 158.05001831054688,L_test: 143.86231994628906\n",
      "Epoch [10731/100000], L_train: 153.36988830566406,L_test: 143.79238891601562\n",
      "Epoch [10741/100000], L_train: 151.58921813964844,L_test: 143.72244262695312\n",
      "Epoch [10751/100000], L_train: 152.00209045410156,L_test: 143.65245056152344\n",
      "Epoch [10761/100000], L_train: 153.84237670898438,L_test: 143.58250427246094\n",
      "Epoch [10771/100000], L_train: 152.1253662109375,L_test: 143.51254272460938\n",
      "Epoch [10781/100000], L_train: 149.24510192871094,L_test: 143.4425811767578\n",
      "Epoch [10791/100000], L_train: 154.75465393066406,L_test: 143.37261962890625\n",
      "Epoch [10801/100000], L_train: 150.53285217285156,L_test: 143.3026580810547\n",
      "Epoch [10811/100000], L_train: 157.0130615234375,L_test: 143.2327117919922\n",
      "Epoch [10821/100000], L_train: 146.61636352539062,L_test: 143.1627197265625\n",
      "Epoch [10831/100000], L_train: 154.4333953857422,L_test: 143.0927734375\n",
      "Epoch [10841/100000], L_train: 152.30255126953125,L_test: 143.02281188964844\n",
      "Epoch [10851/100000], L_train: 153.5877685546875,L_test: 142.9528350830078\n",
      "Epoch [10861/100000], L_train: 152.3234405517578,L_test: 142.8828887939453\n",
      "Epoch [10871/100000], L_train: 156.25845336914062,L_test: 142.81292724609375\n",
      "Epoch [10881/100000], L_train: 147.88307189941406,L_test: 142.7429656982422\n",
      "Epoch [10891/100000], L_train: 147.04833984375,L_test: 142.67300415039062\n",
      "Epoch [10901/100000], L_train: 154.28952026367188,L_test: 142.60304260253906\n",
      "Epoch [10911/100000], L_train: 154.1124267578125,L_test: 142.53309631347656\n",
      "Epoch [10921/100000], L_train: 152.9119110107422,L_test: 142.46310424804688\n",
      "Epoch [10931/100000], L_train: 156.1842041015625,L_test: 142.39317321777344\n",
      "Epoch [10941/100000], L_train: 149.67111206054688,L_test: 142.3231964111328\n",
      "Epoch [10951/100000], L_train: 151.0420684814453,L_test: 142.25323486328125\n",
      "Epoch [10961/100000], L_train: 150.99322509765625,L_test: 142.18328857421875\n",
      "Epoch [10971/100000], L_train: 153.33660888671875,L_test: 142.11329650878906\n",
      "Epoch [10981/100000], L_train: 150.17234802246094,L_test: 142.04335021972656\n",
      "Epoch [10991/100000], L_train: 149.3148193359375,L_test: 141.973388671875\n",
      "Epoch [11001/100000], L_train: 149.16737365722656,L_test: 141.90342712402344\n",
      "Epoch [11011/100000], L_train: 149.7185516357422,L_test: 141.83348083496094\n",
      "Epoch [11021/100000], L_train: 57.037086486816406,L_test: 54.31364822387695\n",
      "Epoch [11031/100000], L_train: 50.42666244506836,L_test: 54.616737365722656\n",
      "Epoch [11041/100000], L_train: 55.1657829284668,L_test: 68.86572265625\n",
      "Epoch [11051/100000], L_train: 57.41394805908203,L_test: 58.634368896484375\n",
      "Epoch [11061/100000], L_train: 50.47799301147461,L_test: 62.622764587402344\n",
      "Epoch [11071/100000], L_train: 50.424861907958984,L_test: 56.39459228515625\n",
      "Epoch [11081/100000], L_train: 51.38982009887695,L_test: 60.50994873046875\n",
      "Epoch [11091/100000], L_train: 47.193580627441406,L_test: 59.51335525512695\n",
      "Epoch [11101/100000], L_train: 49.24003982543945,L_test: 67.85665130615234\n",
      "Epoch [11111/100000], L_train: 50.96059799194336,L_test: 58.44208908081055\n",
      "Epoch [11121/100000], L_train: 50.47517776489258,L_test: 56.31278991699219\n",
      "Epoch [11131/100000], L_train: 51.26580047607422,L_test: 61.103389739990234\n",
      "Epoch [11141/100000], L_train: 54.171382904052734,L_test: 56.48912048339844\n",
      "Epoch [11151/100000], L_train: 49.996917724609375,L_test: 60.68585968017578\n",
      "Epoch [11161/100000], L_train: 51.75261688232422,L_test: 56.0693473815918\n",
      "Epoch [11171/100000], L_train: 49.62852096557617,L_test: 60.750694274902344\n",
      "Epoch [11181/100000], L_train: 48.98828125,L_test: 58.724849700927734\n",
      "Epoch [11191/100000], L_train: 53.40963363647461,L_test: 58.660308837890625\n",
      "Epoch [11201/100000], L_train: 50.06898498535156,L_test: 56.73284149169922\n",
      "Epoch [11211/100000], L_train: 47.26402282714844,L_test: 61.60208511352539\n",
      "Epoch [11221/100000], L_train: 52.13660430908203,L_test: 64.6717300415039\n",
      "Epoch [11231/100000], L_train: 46.90681838989258,L_test: 55.02801513671875\n",
      "Epoch [11241/100000], L_train: 53.80033874511719,L_test: 55.360313415527344\n",
      "Epoch [11251/100000], L_train: 54.97491455078125,L_test: 66.00932312011719\n",
      "Epoch [11261/100000], L_train: 48.1574592590332,L_test: 61.78459930419922\n",
      "Epoch [11271/100000], L_train: 54.15412521362305,L_test: 54.75689697265625\n",
      "Epoch [11281/100000], L_train: 51.79693603515625,L_test: 54.82589340209961\n",
      "Epoch [11291/100000], L_train: 51.76995849609375,L_test: 56.75606918334961\n",
      "Epoch [11301/100000], L_train: 48.63193130493164,L_test: 55.0478515625\n",
      "Epoch [11311/100000], L_train: 53.25124740600586,L_test: 61.05024337768555\n",
      "Epoch [11321/100000], L_train: 54.446537017822266,L_test: 62.91958236694336\n",
      "Epoch [11331/100000], L_train: 51.66880416870117,L_test: 61.25077819824219\n",
      "Epoch [11341/100000], L_train: 44.71896743774414,L_test: 62.832767486572266\n",
      "Epoch [11351/100000], L_train: 50.72539138793945,L_test: 59.47541427612305\n",
      "Epoch [11361/100000], L_train: 53.06278991699219,L_test: 56.69203186035156\n",
      "Epoch [11371/100000], L_train: 48.47522735595703,L_test: 54.843746185302734\n",
      "Epoch [11381/100000], L_train: 53.29655838012695,L_test: 58.60523986816406\n",
      "Epoch [11391/100000], L_train: 51.13395309448242,L_test: 60.33608627319336\n",
      "Epoch [11401/100000], L_train: 53.428619384765625,L_test: 61.018882751464844\n",
      "Epoch [11411/100000], L_train: 51.61101531982422,L_test: 62.67555236816406\n",
      "Epoch [11421/100000], L_train: 52.179141998291016,L_test: 64.2061996459961\n",
      "Epoch [11431/100000], L_train: 51.99880599975586,L_test: 63.032203674316406\n",
      "Epoch [11441/100000], L_train: 45.69075393676758,L_test: 56.743709564208984\n",
      "Epoch [11451/100000], L_train: 53.34768295288086,L_test: 58.29179763793945\n",
      "Epoch [11461/100000], L_train: 46.70623779296875,L_test: 59.37465286254883\n",
      "Epoch [11471/100000], L_train: 49.12044143676758,L_test: 60.711891174316406\n",
      "Epoch [11481/100000], L_train: 51.62179946899414,L_test: 56.29008865356445\n",
      "Epoch [11491/100000], L_train: 52.55402374267578,L_test: 56.67567825317383\n",
      "Epoch [11501/100000], L_train: 52.81009292602539,L_test: 58.58362579345703\n",
      "Epoch [11511/100000], L_train: 54.879154205322266,L_test: 57.9653434753418\n",
      "Epoch [11521/100000], L_train: 47.96995544433594,L_test: 57.269309997558594\n",
      "Epoch [11531/100000], L_train: 49.727840423583984,L_test: 59.875919342041016\n",
      "Epoch [11541/100000], L_train: 50.47718811035156,L_test: 64.23323822021484\n",
      "Epoch [11551/100000], L_train: 52.0075569152832,L_test: 58.46975326538086\n",
      "Epoch [11561/100000], L_train: 48.84238815307617,L_test: 57.898433685302734\n",
      "Epoch [11571/100000], L_train: 47.52761459350586,L_test: 58.649532318115234\n",
      "Epoch [11581/100000], L_train: 48.93098068237305,L_test: 54.12545394897461\n",
      "Epoch [11591/100000], L_train: 48.37970733642578,L_test: 56.79684066772461\n",
      "Epoch [11601/100000], L_train: 51.101890563964844,L_test: 58.94224166870117\n",
      "Epoch [11611/100000], L_train: 48.582481384277344,L_test: 53.62820816040039\n",
      "Epoch [11621/100000], L_train: 50.119224548339844,L_test: 60.60111999511719\n",
      "Epoch [11631/100000], L_train: 47.101295471191406,L_test: 55.3647346496582\n",
      "Epoch [11641/100000], L_train: 54.687477111816406,L_test: 54.04113006591797\n",
      "Epoch [11651/100000], L_train: 49.907657623291016,L_test: 55.62091827392578\n",
      "Epoch [11661/100000], L_train: 49.86318588256836,L_test: 51.799713134765625\n",
      "Epoch [11671/100000], L_train: 47.7651252746582,L_test: 55.74681091308594\n",
      "Epoch [11681/100000], L_train: 52.00160217285156,L_test: 52.61982345581055\n",
      "Epoch [11691/100000], L_train: 50.82072830200195,L_test: 57.52753829956055\n",
      "Epoch [11701/100000], L_train: 48.95515060424805,L_test: 52.10466766357422\n",
      "Epoch [11711/100000], L_train: 48.08702087402344,L_test: 48.521244049072266\n",
      "Epoch [11721/100000], L_train: 66.56059265136719,L_test: 55.656436920166016\n",
      "Epoch [11731/100000], L_train: 47.062313079833984,L_test: 49.09504318237305\n",
      "Epoch [11741/100000], L_train: 43.826908111572266,L_test: 47.78477096557617\n",
      "Epoch [11751/100000], L_train: 45.42962646484375,L_test: 55.592586517333984\n",
      "Epoch [11761/100000], L_train: 40.75090789794922,L_test: 53.13398361206055\n",
      "Epoch [11771/100000], L_train: 85.82183074951172,L_test: 61.46246337890625\n",
      "Epoch [11781/100000], L_train: 38.052452087402344,L_test: 40.04169845581055\n",
      "Epoch [11791/100000], L_train: 38.352783203125,L_test: 42.93550491333008\n",
      "Epoch [11801/100000], L_train: 36.81474304199219,L_test: 36.61536407470703\n",
      "Epoch [11811/100000], L_train: 42.94057083129883,L_test: 44.224388122558594\n",
      "Epoch [11821/100000], L_train: 28.561838150024414,L_test: 38.46171188354492\n",
      "Epoch [11831/100000], L_train: 47.900203704833984,L_test: 33.870140075683594\n",
      "Epoch [11841/100000], L_train: 29.54911231994629,L_test: 37.684879302978516\n",
      "Epoch [11851/100000], L_train: 48.40993881225586,L_test: 42.5603141784668\n",
      "Epoch [11861/100000], L_train: 79.60922241210938,L_test: 28.852880477905273\n",
      "Epoch [11871/100000], L_train: 129.43783569335938,L_test: 115.16434478759766\n",
      "Epoch [11881/100000], L_train: 83.49651336669922,L_test: 76.21182250976562\n",
      "Epoch [11891/100000], L_train: 51.596527099609375,L_test: 44.48590850830078\n",
      "Epoch [11901/100000], L_train: 36.618934631347656,L_test: 38.833213806152344\n",
      "Epoch [11911/100000], L_train: 35.83319091796875,L_test: 40.03084182739258\n",
      "Epoch [11921/100000], L_train: 36.78079605102539,L_test: 40.69697570800781\n",
      "Epoch [11931/100000], L_train: 32.03440856933594,L_test: 40.96916198730469\n",
      "Epoch [11941/100000], L_train: 34.01447677612305,L_test: 41.16868591308594\n",
      "Epoch [11951/100000], L_train: 34.71181106567383,L_test: 41.2779655456543\n",
      "Epoch [11961/100000], L_train: 34.5048713684082,L_test: 41.37117004394531\n",
      "Epoch [11971/100000], L_train: 33.70851135253906,L_test: 41.405826568603516\n",
      "Epoch [11981/100000], L_train: 34.13962173461914,L_test: 41.43613052368164\n",
      "Epoch [11991/100000], L_train: 33.76704025268555,L_test: 41.467533111572266\n",
      "Epoch [12001/100000], L_train: 44.41269302368164,L_test: 71.96894073486328\n",
      "Epoch [12011/100000], L_train: 50.937591552734375,L_test: 56.4633674621582\n",
      "Epoch [12021/100000], L_train: 53.55095291137695,L_test: 54.581661224365234\n",
      "Epoch [12031/100000], L_train: 51.74884033203125,L_test: 56.829402923583984\n",
      "Epoch [12041/100000], L_train: 48.93931579589844,L_test: 65.2746353149414\n",
      "Epoch [12051/100000], L_train: 59.15275192260742,L_test: 62.398311614990234\n",
      "Epoch [12061/100000], L_train: 47.57665252685547,L_test: 63.36745834350586\n",
      "Epoch [12071/100000], L_train: 52.19113540649414,L_test: 57.47838592529297\n",
      "Epoch [12081/100000], L_train: 53.996463775634766,L_test: 58.425498962402344\n",
      "Epoch [12091/100000], L_train: 44.40866470336914,L_test: 58.15605545043945\n",
      "Epoch [12101/100000], L_train: 52.890384674072266,L_test: 62.80569839477539\n",
      "Epoch [12111/100000], L_train: 45.987770080566406,L_test: 62.29132080078125\n",
      "Epoch [12121/100000], L_train: 47.11851119995117,L_test: 58.79350280761719\n",
      "Epoch [12131/100000], L_train: 55.96936798095703,L_test: 58.47135543823242\n",
      "Epoch [12141/100000], L_train: 53.64714813232422,L_test: 56.12495040893555\n",
      "Epoch [12151/100000], L_train: 53.65179443359375,L_test: 63.038482666015625\n",
      "Epoch [12161/100000], L_train: 48.045127868652344,L_test: 54.04920196533203\n",
      "Epoch [12171/100000], L_train: 47.173797607421875,L_test: 56.684547424316406\n",
      "Epoch [12181/100000], L_train: 51.225257873535156,L_test: 57.42908477783203\n",
      "Epoch [12191/100000], L_train: 52.641998291015625,L_test: 55.131500244140625\n",
      "Epoch [12201/100000], L_train: 53.03620147705078,L_test: 62.6677360534668\n",
      "Epoch [12211/100000], L_train: 49.62371063232422,L_test: 57.65027618408203\n",
      "Epoch [12221/100000], L_train: 47.128421783447266,L_test: 58.71710205078125\n",
      "Epoch [12231/100000], L_train: 46.991004943847656,L_test: 57.234615325927734\n",
      "Epoch [12241/100000], L_train: 49.48073959350586,L_test: 57.69398498535156\n",
      "Epoch [12251/100000], L_train: 51.359580993652344,L_test: 54.629058837890625\n",
      "Epoch [12261/100000], L_train: 45.51267623901367,L_test: 58.828121185302734\n",
      "Epoch [12271/100000], L_train: 46.087825775146484,L_test: 55.96887969970703\n",
      "Epoch [12281/100000], L_train: 47.8545036315918,L_test: 54.1689453125\n",
      "Epoch [12291/100000], L_train: 47.80190658569336,L_test: 52.63494110107422\n",
      "Epoch [12301/100000], L_train: 50.29404067993164,L_test: 54.23834228515625\n",
      "Epoch [12311/100000], L_train: 45.271324157714844,L_test: 52.625431060791016\n",
      "Epoch [12321/100000], L_train: 49.082218170166016,L_test: 70.36644744873047\n",
      "Epoch [12331/100000], L_train: 49.139888763427734,L_test: 53.643714904785156\n",
      "Epoch [12341/100000], L_train: 49.39812088012695,L_test: 51.665687561035156\n",
      "Epoch [12351/100000], L_train: 1425.01611328125,L_test: 41.56492233276367\n",
      "Epoch [12361/100000], L_train: 33.42148208618164,L_test: 41.56819534301758\n",
      "Epoch [12371/100000], L_train: 33.34298324584961,L_test: 41.60643005371094\n",
      "Epoch [12381/100000], L_train: 34.677772521972656,L_test: 41.52544021606445\n",
      "Epoch [12391/100000], L_train: 32.208824157714844,L_test: 41.583003997802734\n",
      "Epoch [12401/100000], L_train: 33.40101623535156,L_test: 41.5573844909668\n",
      "Epoch [12411/100000], L_train: 33.09320831298828,L_test: 41.592777252197266\n",
      "Epoch [12421/100000], L_train: 38.223670959472656,L_test: 41.58815002441406\n",
      "Epoch [12431/100000], L_train: 31.6437931060791,L_test: 41.5730094909668\n",
      "Epoch [12441/100000], L_train: 33.49381637573242,L_test: 41.59523010253906\n",
      "Epoch [12451/100000], L_train: 33.74660873413086,L_test: 41.630897521972656\n",
      "Epoch [12461/100000], L_train: 34.36562728881836,L_test: 41.61299133300781\n",
      "Epoch [12471/100000], L_train: 30.617868423461914,L_test: 41.59089279174805\n",
      "Epoch [12481/100000], L_train: 33.212615966796875,L_test: 41.54487609863281\n",
      "Epoch [12491/100000], L_train: 32.20347595214844,L_test: 41.557640075683594\n",
      "Epoch [12501/100000], L_train: 150.39886474609375,L_test: 137.64732360839844\n",
      "Epoch [12511/100000], L_train: 143.29417419433594,L_test: 137.44004821777344\n",
      "Epoch [12521/100000], L_train: 149.63809204101562,L_test: 137.27261352539062\n",
      "Epoch [12531/100000], L_train: 141.82281494140625,L_test: 137.1269073486328\n",
      "Epoch [12541/100000], L_train: 148.03712463378906,L_test: 136.99534606933594\n",
      "Epoch [12551/100000], L_train: 146.03184509277344,L_test: 136.87380981445312\n",
      "Epoch [12561/100000], L_train: 142.12693786621094,L_test: 136.75978088378906\n",
      "Epoch [12571/100000], L_train: 141.4635009765625,L_test: 136.6516571044922\n",
      "Epoch [12581/100000], L_train: 146.9395294189453,L_test: 136.54827880859375\n",
      "Epoch [12591/100000], L_train: 150.19833374023438,L_test: 136.44879150390625\n",
      "Epoch [12601/100000], L_train: 146.0925750732422,L_test: 136.3525848388672\n",
      "Epoch [12611/100000], L_train: 144.289306640625,L_test: 136.25912475585938\n",
      "Epoch [12621/100000], L_train: 148.36212158203125,L_test: 136.16806030273438\n",
      "Epoch [12631/100000], L_train: 140.0751495361328,L_test: 136.07907104492188\n",
      "Epoch [12641/100000], L_train: 143.888427734375,L_test: 135.9918670654297\n",
      "Epoch [12651/100000], L_train: 146.56158447265625,L_test: 135.90623474121094\n",
      "Epoch [12661/100000], L_train: 48.278160095214844,L_test: 410.9910888671875\n",
      "Epoch [12671/100000], L_train: 145.30831909179688,L_test: 135.74264526367188\n",
      "Epoch [12681/100000], L_train: 146.7322235107422,L_test: 135.66070556640625\n",
      "Epoch [12691/100000], L_train: 145.71734619140625,L_test: 135.5797882080078\n",
      "Epoch [12701/100000], L_train: 144.50381469726562,L_test: 135.499755859375\n",
      "Epoch [12711/100000], L_train: 146.91940307617188,L_test: 135.42051696777344\n",
      "Epoch [12721/100000], L_train: 146.71473693847656,L_test: 135.3420867919922\n",
      "Epoch [12731/100000], L_train: 148.4047088623047,L_test: 135.26431274414062\n",
      "Epoch [12741/100000], L_train: 142.46084594726562,L_test: 135.18711853027344\n",
      "Epoch [12751/100000], L_train: 144.15675354003906,L_test: 135.11050415039062\n",
      "Epoch [12761/100000], L_train: 145.81544494628906,L_test: 135.0343780517578\n",
      "Epoch [12771/100000], L_train: 143.90733337402344,L_test: 134.95870971679688\n",
      "Epoch [12781/100000], L_train: 143.98153686523438,L_test: 134.88340759277344\n",
      "Epoch [12791/100000], L_train: 145.15093994140625,L_test: 134.80865478515625\n",
      "Epoch [12801/100000], L_train: 142.3166961669922,L_test: 134.73419189453125\n",
      "Epoch [12811/100000], L_train: 141.21597290039062,L_test: 134.6599578857422\n",
      "Epoch [12821/100000], L_train: 141.65953063964844,L_test: 134.5862274169922\n",
      "Epoch [12831/100000], L_train: 142.1154327392578,L_test: 134.51254272460938\n",
      "Epoch [12841/100000], L_train: 146.16737365722656,L_test: 134.4393768310547\n",
      "Epoch [12851/100000], L_train: 140.74603271484375,L_test: 134.36619567871094\n",
      "Epoch [12861/100000], L_train: 138.6915283203125,L_test: 134.29344177246094\n",
      "Epoch [12871/100000], L_train: 141.06260681152344,L_test: 134.22080993652344\n",
      "Epoch [12881/100000], L_train: 142.6573944091797,L_test: 134.14817810058594\n",
      "Epoch [12891/100000], L_train: 152.21392822265625,L_test: 134.0760955810547\n",
      "Epoch [12901/100000], L_train: 145.5945281982422,L_test: 134.00399780273438\n",
      "Epoch [12911/100000], L_train: 146.50294494628906,L_test: 133.931884765625\n",
      "Epoch [12921/100000], L_train: 141.14308166503906,L_test: 133.8600616455078\n",
      "Epoch [12931/100000], L_train: 123.88579559326172,L_test: 63.66511154174805\n",
      "Epoch [12941/100000], L_train: 49.33766174316406,L_test: 56.72239685058594\n",
      "Epoch [12951/100000], L_train: 57.743492126464844,L_test: 53.83118438720703\n",
      "Epoch [12961/100000], L_train: 48.57775115966797,L_test: 56.956180572509766\n",
      "Epoch [12971/100000], L_train: 48.604515075683594,L_test: 58.4111213684082\n",
      "Epoch [12981/100000], L_train: 49.57370376586914,L_test: 59.097877502441406\n",
      "Epoch [12991/100000], L_train: 50.663700103759766,L_test: 54.28617858886719\n",
      "Epoch [13001/100000], L_train: 50.532958984375,L_test: 58.49922180175781\n",
      "Epoch [13011/100000], L_train: 51.00645065307617,L_test: 60.08019256591797\n",
      "Epoch [13021/100000], L_train: 45.72679138183594,L_test: 61.34388732910156\n",
      "Epoch [13031/100000], L_train: 50.40618896484375,L_test: 61.832008361816406\n",
      "Epoch [13041/100000], L_train: 49.03535079956055,L_test: 55.495872497558594\n",
      "Epoch [13051/100000], L_train: 53.11119842529297,L_test: 55.053138732910156\n",
      "Epoch [13061/100000], L_train: 45.61973571777344,L_test: 58.90787887573242\n",
      "Epoch [13071/100000], L_train: 49.189701080322266,L_test: 56.09019470214844\n",
      "Epoch [13081/100000], L_train: 46.87306213378906,L_test: 58.52239227294922\n",
      "Epoch [13091/100000], L_train: 54.701263427734375,L_test: 57.68869400024414\n",
      "Epoch [13101/100000], L_train: 45.14950942993164,L_test: 61.098934173583984\n",
      "Epoch [13111/100000], L_train: 53.89549255371094,L_test: 58.8195686340332\n",
      "Epoch [13121/100000], L_train: 44.14450454711914,L_test: 55.532962799072266\n",
      "Epoch [13131/100000], L_train: 142.0183563232422,L_test: 133.45147705078125\n",
      "Epoch [13141/100000], L_train: 146.49911499023438,L_test: 133.33116149902344\n",
      "Epoch [13151/100000], L_train: 140.6268768310547,L_test: 133.218017578125\n",
      "Epoch [13161/100000], L_train: 140.88084411621094,L_test: 133.11056518554688\n",
      "Epoch [13171/100000], L_train: 144.53379821777344,L_test: 133.00775146484375\n",
      "Epoch [13181/100000], L_train: 141.88330078125,L_test: 132.90875244140625\n",
      "Epoch [13191/100000], L_train: 143.040283203125,L_test: 132.81295776367188\n",
      "Epoch [13201/100000], L_train: 142.8333740234375,L_test: 132.71983337402344\n",
      "Epoch [13211/100000], L_train: 143.02682495117188,L_test: 132.62905883789062\n",
      "Epoch [13221/100000], L_train: 142.8600616455078,L_test: 132.540283203125\n",
      "Epoch [13231/100000], L_train: 138.3900909423828,L_test: 132.4533233642578\n",
      "Epoch [13241/100000], L_train: 141.48805236816406,L_test: 132.36788940429688\n",
      "Epoch [13251/100000], L_train: 142.33897399902344,L_test: 132.28387451171875\n",
      "Epoch [13261/100000], L_train: 139.4845733642578,L_test: 132.20106506347656\n",
      "Epoch [13271/100000], L_train: 142.46405029296875,L_test: 132.1193389892578\n",
      "Epoch [13281/100000], L_train: 142.28770446777344,L_test: 132.03858947753906\n",
      "Epoch [13291/100000], L_train: 144.52842712402344,L_test: 131.958740234375\n",
      "Epoch [13301/100000], L_train: 139.93548583984375,L_test: 131.87969970703125\n",
      "Epoch [13311/100000], L_train: 135.45144653320312,L_test: 131.80142211914062\n",
      "Epoch [13321/100000], L_train: 142.946044921875,L_test: 131.72373962402344\n",
      "Epoch [13331/100000], L_train: 139.2968292236328,L_test: 131.6466827392578\n",
      "Epoch [13341/100000], L_train: 141.39208984375,L_test: 131.57015991210938\n",
      "Epoch [13351/100000], L_train: 143.09974670410156,L_test: 131.49415588378906\n",
      "Epoch [13361/100000], L_train: 142.87017822265625,L_test: 131.41860961914062\n",
      "Epoch [13371/100000], L_train: 142.13710021972656,L_test: 131.3434295654297\n",
      "Epoch [13381/100000], L_train: 141.4337921142578,L_test: 131.26866149902344\n",
      "Epoch [13391/100000], L_train: 138.8710479736328,L_test: 131.19432067871094\n",
      "Epoch [13401/100000], L_train: 142.57676696777344,L_test: 131.1201629638672\n",
      "Epoch [13411/100000], L_train: 138.26194763183594,L_test: 131.04644775390625\n",
      "Epoch [13421/100000], L_train: 141.26040649414062,L_test: 130.97286987304688\n",
      "Epoch [13431/100000], L_train: 139.87144470214844,L_test: 130.8997039794922\n",
      "Epoch [13441/100000], L_train: 134.40403747558594,L_test: 130.82655334472656\n",
      "Epoch [13451/100000], L_train: 135.80564880371094,L_test: 130.75389099121094\n",
      "Epoch [13461/100000], L_train: 140.73545837402344,L_test: 130.68125915527344\n",
      "Epoch [13471/100000], L_train: 139.12351989746094,L_test: 130.60874938964844\n",
      "Epoch [13481/100000], L_train: 147.10997009277344,L_test: 130.53663635253906\n",
      "Epoch [13491/100000], L_train: 142.42713928222656,L_test: 130.4645538330078\n",
      "Epoch [13501/100000], L_train: 141.76174926757812,L_test: 130.39247131347656\n",
      "Epoch [13511/100000], L_train: 142.3493194580078,L_test: 130.32073974609375\n",
      "Epoch [13521/100000], L_train: 138.31748962402344,L_test: 130.2491912841797\n",
      "Epoch [13531/100000], L_train: 140.68260192871094,L_test: 130.17762756347656\n",
      "Epoch [13541/100000], L_train: 138.93699645996094,L_test: 130.10606384277344\n",
      "Epoch [13551/100000], L_train: 139.14073181152344,L_test: 130.03448486328125\n",
      "Epoch [13561/100000], L_train: 134.88514709472656,L_test: 129.96343994140625\n",
      "Epoch [13571/100000], L_train: 136.16383361816406,L_test: 129.89242553710938\n",
      "Epoch [13581/100000], L_train: 136.26605224609375,L_test: 129.82139587402344\n",
      "Epoch [13591/100000], L_train: 140.58473205566406,L_test: 129.75035095214844\n",
      "Epoch [13601/100000], L_train: 140.19085693359375,L_test: 129.67930603027344\n",
      "Epoch [13611/100000], L_train: 68.07996368408203,L_test: 60.518253326416016\n",
      "Epoch [13621/100000], L_train: 50.24159622192383,L_test: 58.308685302734375\n",
      "Epoch [13631/100000], L_train: 57.9187126159668,L_test: 52.683406829833984\n",
      "Epoch [13641/100000], L_train: 139.98794555664062,L_test: 129.58932495117188\n",
      "Epoch [13651/100000], L_train: 131.0002899169922,L_test: 129.51416015625\n",
      "Epoch [13661/100000], L_train: 138.9507598876953,L_test: 129.43711853027344\n",
      "Epoch [13671/100000], L_train: 142.8703155517578,L_test: 129.36065673828125\n",
      "Epoch [13681/100000], L_train: 140.67530822753906,L_test: 129.28468322753906\n",
      "Epoch [13691/100000], L_train: 142.1182403564453,L_test: 129.2091827392578\n"
     ]
    }
   ],
   "source": [
    "# continue optimize\n",
    "# start from last training result\n",
    "epochs=10000\n",
    "LR = 1e-15\n",
    "Weight_decay=0.001\n",
    "value_net.train()\n",
    "\n",
    "run_count = 0 \n",
    "\n",
    "while True:\n",
    "    for epoch in range(epochs):\n",
    "        l_train, l_test = optimize_model()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], L_train: {l_train}, L_test: {l_test}')\n",
    "    \n",
    "    run_count += 1\n",
    "\n",
    "    # Check the stopping condition\n",
    "    if l_test <= 10 or run_count > 100:\n",
    "        print(\"Stopping the loop.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#save last line results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(value_net\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_parameters.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#save last line results\n",
    "torch.save(value_net.state_dict(),'model_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "loaded_model = NN(n_observations).to(device)\n",
    "value_net.load_state_dict(torch.load('model_parameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to predict\n",
    "windowed_data, predict_prices = slide_windows(pd.read_csv(predict_data_file))\n",
    "# Convert to PyTorch tensors\n",
    "state_new = torch.tensor(windowed_data, dtype=torch.float32).to(device)\n",
    "real_price = torch.tensor(predict_prices, dtype=torch.float32).to(device)\n",
    "\n",
    "#load the trained parameter to the model\n",
    "# loaded_model = NN(n_observations).to(device)\n",
    "# loaded_model.load_state_dict(torch.load('model_parameters.pth')) \n",
    "# value_net=loaded_model \n",
    "\n",
    "# predict price\n",
    "value_net.eval()  \n",
    "with torch.no_grad():\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    predict_price=loaded_model(state_new).squeeze() #predict price\n",
    "    l_test=criterion(real_price,predict_price) #calculate difference\n",
    "print(predict_price)\n",
    "print(l_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(real_price, color='black', label='Real Stock Price')\n",
    "plt.plot(predict_price, color='green', label='Predicted Stock Price in NN')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(l_test, color='red', label='Prediction Error (%)')\n",
    "plt.title('Prediction Error')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Error (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(real_price, predict_prices)\n",
    "\n",
    "print(f\"Average NN Prediction Error (%): {np.mean(l_test)}\")\n",
    "print(f\"Variance of NN Prediction Error (%): {np.var(l_test)}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
